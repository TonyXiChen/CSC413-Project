{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20d33e46-58e1-4b92-bcaf-172acdf1b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from textwrap import fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62d55421-4413-41de-aa74-2b9dfb88309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_parameter_finetune(base_model=\"runwayml/stable-diffusion-v1-5\",\n",
    "               output_dir=os.getcwd() + '/Full_Parameter_Finetune', dataset=\"dataset.parquet\",\n",
    "               num_train_epochs=3, checkpointing_steps=100, max_train_steps=1000,\n",
    "               learning_rate=1e-04, max_grad_norm=2, precision=\"bf16\"):\n",
    "\n",
    "    def parquet_to_imagefolder(output_dir, dataset):\n",
    "        imagefolder_dir = os.path.join(output_dir, os.path.splitext(dataset)[0])\n",
    "        metadata_csv_path = os.path.join(imagefolder_dir, 'metadata.csv')\n",
    "        \n",
    "        os.makedirs(imagefolder_dir, exist_ok=True)\n",
    "        \n",
    "        df = pd.read_parquet(dataset)\n",
    "        \n",
    "        metadata = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            image_data = row['image']['bytes']\n",
    "            text = row['text']\n",
    "        \n",
    "            image_name = f'{index}.jpg'\n",
    "            image_path = os.path.join(imagefolder_dir, image_name)\n",
    "        \n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "            image.save(image_path)\n",
    "        \n",
    "            metadata.append({'file_name': image_name, 'text': text})\n",
    "        \n",
    "        \n",
    "        metadata_df = pd.DataFrame(metadata)\n",
    "        metadata_df.to_csv(metadata_csv_path, index=False)\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    def run_command_in_notebook(command):\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    \n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if output == '' and process.poll() is not None:\n",
    "                break\n",
    "            if output:\n",
    "                print(output.strip())\n",
    "        \n",
    "        return_code = process.poll()\n",
    "        if return_code:\n",
    "            print(f\"Command exited with error code {return_code}\")\n",
    "    \n",
    "    command = [\n",
    "        'accelerate',\n",
    "        'launch',\n",
    "        \"--mixed_precision=\" + str(precision),\n",
    "        'train_text_to_image.py',\n",
    "        '--pretrained_model_name_or_path=' + str(base_model),\n",
    "        '--train_data_dir=' + os.path.join(output_dir, os.path.splitext(dataset)[0]),\n",
    "        '--use_ema',\n",
    "        '--resolution=512',\n",
    "        '--train_batch_size=8',\n",
    "        '--center_crop',\n",
    "        '--num_train_epochs=' + str(num_train_epochs),\n",
    "        '--checkpointing_steps=' + str(checkpointing_steps),\n",
    "        '--max_train_steps=' + str(max_train_steps),\n",
    "        '--learning_rate=' + str(learning_rate),\n",
    "        '--max_grad_norm=' + str(max_grad_norm),\n",
    "        '--lr_scheduler=constant',\n",
    "        '--lr_warmup_steps=0',\n",
    "        '--seed=413',\n",
    "        '--output_dir=' + str(output_dir),\n",
    "        '--report_to=wandb',\n",
    "    ]\n",
    "\n",
    "    parquet_to_imagefolder(output_dir, dataset)\n",
    "    \n",
    "    run_command_in_notebook(command)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71482f70-228d-4703-b132-7ea6a6660e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/17/2024 03:54:55 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n",
      "{'rescale_betas_zero_snr', 'prediction_type', 'clip_sample_range', 'sample_max_value', 'variance_type', 'thresholding', 'timestep_spacing', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "{'scaling_factor', 'latents_std', 'latents_mean', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 32 examples [00:00, ? examples/s]\n",
      "wandb: Currently logged in as: tonyxichen. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.15.4\n",
      "wandb: Run data is saved locally in X:\\Jupyter\\csc413_project\\wandb\\run-20240417_035505-q9m70n6e\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run ruby-fog-40\n",
      "wandb:  View project at https://wandb.ai/tonyxichen/text2image-fine-tune\n",
      "wandb:  View run at https://wandb.ai/tonyxichen/text2image-fine-tune/runs/q9m70n6e\n",
      "04/17/2024 03:55:05 - INFO - __main__ - ***** Running training *****\n",
      "04/17/2024 03:55:05 - INFO - __main__ -   Num examples = 32\n",
      "04/17/2024 03:55:05 - INFO - __main__ -   Num Epochs = 250\n",
      "04/17/2024 03:55:05 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
      "04/17/2024 03:55:05 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "04/17/2024 03:55:05 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "04/17/2024 03:55:05 - INFO - __main__ -   Total optimization steps = 1000\n",
      "\n",
      "Steps:   0%|          | 0/1000 [00:00<?, ?it/s]C:\\Users\\Tony\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\diffusers\\models\\attention_processor.py:1276: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "hidden_states = F.scaled_dot_product_attention(\n",
      "C:\\Users\\Tony\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "warnings.warn(\n",
      "\n",
      "Steps:   0%|          | 1/1000 [00:01<32:31,  1.95s/it]\n",
      "Steps:   0%|          | 1/1000 [00:01<32:31,  1.95s/it, lr=0.0001, step_loss=0.0436]\n",
      "Steps:   0%|          | 2/1000 [00:13<2:01:46,  7.32s/it, lr=0.0001, step_loss=0.0436]\n",
      "Steps:   0%|          | 2/1000 [00:13<2:01:46,  7.32s/it, lr=0.0001, step_loss=0.0336]\n",
      "Steps:   0%|          | 3/1000 [00:20<2:05:22,  7.55s/it, lr=0.0001, step_loss=0.0336]\n",
      "Steps:   0%|          | 3/1000 [00:20<2:05:22,  7.55s/it, lr=0.0001, step_loss=0.0571]\n",
      "Steps:   0%|          | 4/1000 [00:28<2:08:19,  7.73s/it, lr=0.0001, step_loss=0.0571]\n",
      "Steps:   0%|          | 4/1000 [00:28<2:08:19,  7.73s/it, lr=0.0001, step_loss=0.102]\n",
      "Steps:   0%|          | 5/1000 [00:36<2:09:03,  7.78s/it, lr=0.0001, step_loss=0.102]\n",
      "Steps:   0%|          | 5/1000 [00:36<2:09:03,  7.78s/it, lr=0.0001, step_loss=0.11]\n",
      "Steps:   1%|          | 6/1000 [00:44<2:10:51,  7.90s/it, lr=0.0001, step_loss=0.11]\n",
      "Steps:   1%|          | 6/1000 [00:44<2:10:51,  7.90s/it, lr=0.0001, step_loss=0.0363]\n",
      "Steps:   1%|          | 7/1000 [00:52<2:09:45,  7.84s/it, lr=0.0001, step_loss=0.0363]\n",
      "Steps:   1%|          | 7/1000 [00:52<2:09:45,  7.84s/it, lr=0.0001, step_loss=0.1]\n",
      "Steps:   1%|          | 8/1000 [01:00<2:10:37,  7.90s/it, lr=0.0001, step_loss=0.1]\n",
      "Steps:   1%|          | 8/1000 [01:00<2:10:37,  7.90s/it, lr=0.0001, step_loss=0.0669]\n",
      "Steps:   1%|          | 9/1000 [01:08<2:10:13,  7.88s/it, lr=0.0001, step_loss=0.0669]\n",
      "Steps:   1%|          | 9/1000 [01:08<2:10:13,  7.88s/it, lr=0.0001, step_loss=0.0428]\n",
      "Steps:   1%|          | 10/1000 [01:16<2:11:17,  7.96s/it, lr=0.0001, step_loss=0.0428]\n",
      "Steps:   1%|          | 10/1000 [01:16<2:11:17,  7.96s/it, lr=0.0001, step_loss=0.0326]\n",
      "Steps:   1%|          | 11/1000 [01:24<2:10:02,  7.89s/it, lr=0.0001, step_loss=0.0326]\n",
      "Steps:   1%|          | 11/1000 [01:24<2:10:02,  7.89s/it, lr=0.0001, step_loss=0.0625]\n",
      "Steps:   1%|          | 12/1000 [01:32<2:10:37,  7.93s/it, lr=0.0001, step_loss=0.0625]\n",
      "Steps:   1%|          | 12/1000 [01:32<2:10:37,  7.93s/it, lr=0.0001, step_loss=0.0413]\n",
      "Steps:   1%|▏         | 13/1000 [01:40<2:09:58,  7.90s/it, lr=0.0001, step_loss=0.0413]\n",
      "Steps:   1%|▏         | 13/1000 [01:40<2:09:58,  7.90s/it, lr=0.0001, step_loss=0.036]\n",
      "Steps:   1%|▏         | 14/1000 [01:48<2:11:01,  7.97s/it, lr=0.0001, step_loss=0.036]\n",
      "Steps:   1%|▏         | 14/1000 [01:48<2:11:01,  7.97s/it, lr=0.0001, step_loss=0.0416]\n",
      "Steps:   2%|▏         | 15/1000 [01:56<2:10:05,  7.92s/it, lr=0.0001, step_loss=0.0416]\n",
      "Steps:   2%|▏         | 15/1000 [01:56<2:10:05,  7.92s/it, lr=0.0001, step_loss=0.0657]\n",
      "Steps:   2%|▏         | 16/1000 [02:04<2:10:29,  7.96s/it, lr=0.0001, step_loss=0.0657]\n",
      "Steps:   2%|▏         | 16/1000 [02:04<2:10:29,  7.96s/it, lr=0.0001, step_loss=0.0438]\n",
      "Steps:   2%|▏         | 17/1000 [02:12<2:09:52,  7.93s/it, lr=0.0001, step_loss=0.0438]\n",
      "Steps:   2%|▏         | 17/1000 [02:12<2:09:52,  7.93s/it, lr=0.0001, step_loss=0.0541]\n",
      "Steps:   2%|▏         | 18/1000 [02:20<2:10:43,  7.99s/it, lr=0.0001, step_loss=0.0541]\n",
      "Steps:   2%|▏         | 18/1000 [02:20<2:10:43,  7.99s/it, lr=0.0001, step_loss=0.0737]\n",
      "Steps:   2%|▏         | 19/1000 [02:27<2:09:19,  7.91s/it, lr=0.0001, step_loss=0.0737]\n",
      "Steps:   2%|▏         | 19/1000 [02:27<2:09:19,  7.91s/it, lr=0.0001, step_loss=0.0342]\n",
      "Steps:   2%|▏         | 20/1000 [02:35<2:09:48,  7.95s/it, lr=0.0001, step_loss=0.0342]\n",
      "Steps:   2%|▏         | 20/1000 [02:35<2:09:48,  7.95s/it, lr=0.0001, step_loss=0.0419]\n",
      "Steps:   2%|▏         | 21/1000 [02:43<2:09:02,  7.91s/it, lr=0.0001, step_loss=0.0419]\n",
      "Steps:   2%|▏         | 21/1000 [02:43<2:09:02,  7.91s/it, lr=0.0001, step_loss=0.0323]\n",
      "Steps:   2%|▏         | 22/1000 [02:51<2:10:10,  7.99s/it, lr=0.0001, step_loss=0.0323]\n",
      "Steps:   2%|▏         | 22/1000 [02:51<2:10:10,  7.99s/it, lr=0.0001, step_loss=0.0471]\n",
      "Steps:   2%|▏         | 23/1000 [02:59<2:08:48,  7.91s/it, lr=0.0001, step_loss=0.0471]\n",
      "Steps:   2%|▏         | 23/1000 [02:59<2:08:48,  7.91s/it, lr=0.0001, step_loss=0.0288]\n",
      "Steps:   2%|▏         | 24/1000 [03:07<2:09:18,  7.95s/it, lr=0.0001, step_loss=0.0288]\n",
      "Steps:   2%|▏         | 24/1000 [03:07<2:09:18,  7.95s/it, lr=0.0001, step_loss=0.0949]\n",
      "Steps:   2%|▎         | 25/1000 [03:15<2:08:37,  7.91s/it, lr=0.0001, step_loss=0.0949]\n",
      "Steps:   2%|▎         | 25/1000 [03:15<2:08:37,  7.91s/it, lr=0.0001, step_loss=0.0876]\n",
      "Steps:   3%|▎         | 26/1000 [03:23<2:09:39,  7.99s/it, lr=0.0001, step_loss=0.0876]\n",
      "Steps:   3%|▎         | 26/1000 [03:23<2:09:39,  7.99s/it, lr=0.0001, step_loss=0.0566]\n",
      "Steps:   3%|▎         | 27/1000 [03:31<2:08:08,  7.90s/it, lr=0.0001, step_loss=0.0566]\n",
      "Steps:   3%|▎         | 27/1000 [03:31<2:08:08,  7.90s/it, lr=0.0001, step_loss=0.0967]\n",
      "Steps:   3%|▎         | 28/1000 [03:39<2:08:42,  7.95s/it, lr=0.0001, step_loss=0.0967]\n",
      "Steps:   3%|▎         | 28/1000 [03:39<2:08:42,  7.95s/it, lr=0.0001, step_loss=0.0377]\n",
      "Steps:   3%|▎         | 29/1000 [03:47<2:07:56,  7.91s/it, lr=0.0001, step_loss=0.0377]\n",
      "Steps:   3%|▎         | 29/1000 [03:47<2:07:56,  7.91s/it, lr=0.0001, step_loss=0.0336]\n",
      "Steps:   3%|▎         | 30/1000 [03:55<2:09:10,  7.99s/it, lr=0.0001, step_loss=0.0336]\n",
      "Steps:   3%|▎         | 30/1000 [03:55<2:09:10,  7.99s/it, lr=0.0001, step_loss=0.0739]\n",
      "Steps:   3%|▎         | 31/1000 [04:03<2:07:56,  7.92s/it, lr=0.0001, step_loss=0.0739]\n",
      "Steps:   3%|▎         | 31/1000 [04:03<2:07:56,  7.92s/it, lr=0.0001, step_loss=0.0459]\n",
      "Steps:   3%|▎         | 32/1000 [04:11<2:08:25,  7.96s/it, lr=0.0001, step_loss=0.0459]\n",
      "Steps:   3%|▎         | 32/1000 [04:11<2:08:25,  7.96s/it, lr=0.0001, step_loss=0.0457]\n",
      "Steps:   3%|▎         | 33/1000 [04:19<2:07:48,  7.93s/it, lr=0.0001, step_loss=0.0457]\n",
      "Steps:   3%|▎         | 33/1000 [04:19<2:07:48,  7.93s/it, lr=0.0001, step_loss=0.0418]\n",
      "Steps:   3%|▎         | 34/1000 [04:27<2:08:41,  7.99s/it, lr=0.0001, step_loss=0.0418]\n",
      "Steps:   3%|▎         | 34/1000 [04:27<2:08:41,  7.99s/it, lr=0.0001, step_loss=0.0415]\n",
      "Steps:   4%|▎         | 35/1000 [04:34<2:07:18,  7.92s/it, lr=0.0001, step_loss=0.0415]\n",
      "Steps:   4%|▎         | 35/1000 [04:34<2:07:18,  7.92s/it, lr=0.0001, step_loss=0.0452]\n",
      "Steps:   4%|▎         | 36/1000 [04:42<2:07:43,  7.95s/it, lr=0.0001, step_loss=0.0452]\n",
      "Steps:   4%|▎         | 36/1000 [04:42<2:07:43,  7.95s/it, lr=0.0001, step_loss=0.0611]\n",
      "Steps:   4%|▎         | 37/1000 [04:50<2:07:05,  7.92s/it, lr=0.0001, step_loss=0.0611]\n",
      "Steps:   4%|▎         | 37/1000 [04:50<2:07:05,  7.92s/it, lr=0.0001, step_loss=0.0604]\n",
      "Steps:   4%|▍         | 38/1000 [04:58<2:07:57,  7.98s/it, lr=0.0001, step_loss=0.0604]\n",
      "Steps:   4%|▍         | 38/1000 [04:58<2:07:57,  7.98s/it, lr=0.0001, step_loss=0.0511]\n",
      "Steps:   4%|▍         | 39/1000 [05:06<2:06:42,  7.91s/it, lr=0.0001, step_loss=0.0511]\n",
      "Steps:   4%|▍         | 39/1000 [05:06<2:06:42,  7.91s/it, lr=0.0001, step_loss=0.0482]\n",
      "Steps:   4%|▍         | 40/1000 [05:14<2:07:13,  7.95s/it, lr=0.0001, step_loss=0.0482]\n",
      "Steps:   4%|▍         | 40/1000 [05:14<2:07:13,  7.95s/it, lr=0.0001, step_loss=0.102]\n",
      "Steps:   4%|▍         | 41/1000 [05:22<2:06:25,  7.91s/it, lr=0.0001, step_loss=0.102]\n",
      "Steps:   4%|▍         | 41/1000 [05:22<2:06:25,  7.91s/it, lr=0.0001, step_loss=0.0415]\n",
      "Steps:   4%|▍         | 42/1000 [05:30<2:07:28,  7.98s/it, lr=0.0001, step_loss=0.0415]\n",
      "Steps:   4%|▍         | 42/1000 [05:30<2:07:28,  7.98s/it, lr=0.0001, step_loss=0.0361]\n",
      "Steps:   4%|▍         | 43/1000 [05:38<2:06:13,  7.91s/it, lr=0.0001, step_loss=0.0361]\n",
      "Steps:   4%|▍         | 43/1000 [05:38<2:06:13,  7.91s/it, lr=0.0001, step_loss=0.0237]\n",
      "Steps:   4%|▍         | 44/1000 [05:46<2:06:43,  7.95s/it, lr=0.0001, step_loss=0.0237]\n",
      "Steps:   4%|▍         | 44/1000 [05:46<2:06:43,  7.95s/it, lr=0.0001, step_loss=0.0669]\n",
      "Steps:   4%|▍         | 45/1000 [05:54<2:06:13,  7.93s/it, lr=0.0001, step_loss=0.0669]\n",
      "Steps:   4%|▍         | 45/1000 [05:54<2:06:13,  7.93s/it, lr=0.0001, step_loss=0.0675]\n",
      "Steps:   5%|▍         | 46/1000 [06:02<2:07:05,  7.99s/it, lr=0.0001, step_loss=0.0675]\n",
      "Steps:   5%|▍         | 46/1000 [06:02<2:07:05,  7.99s/it, lr=0.0001, step_loss=0.0468]\n",
      "Steps:   5%|▍         | 47/1000 [06:10<2:05:43,  7.92s/it, lr=0.0001, step_loss=0.0468]\n",
      "Steps:   5%|▍         | 47/1000 [06:10<2:05:43,  7.92s/it, lr=0.0001, step_loss=0.0202]\n",
      "Steps:   5%|▍         | 48/1000 [06:18<2:06:08,  7.95s/it, lr=0.0001, step_loss=0.0202]\n",
      "Steps:   5%|▍         | 48/1000 [06:18<2:06:08,  7.95s/it, lr=0.0001, step_loss=0.0322]\n",
      "Steps:   5%|▍         | 49/1000 [06:26<2:05:30,  7.92s/it, lr=0.0001, step_loss=0.0322]\n",
      "Steps:   5%|▍         | 49/1000 [06:26<2:05:30,  7.92s/it, lr=0.0001, step_loss=0.0419]\n",
      "Steps:   5%|▌         | 50/1000 [06:34<2:06:25,  7.99s/it, lr=0.0001, step_loss=0.0419]\n",
      "Steps:   5%|▌         | 50/1000 [06:34<2:06:25,  7.99s/it, lr=0.0001, step_loss=0.068]\n",
      "Steps:   5%|▌         | 51/1000 [06:42<2:05:10,  7.91s/it, lr=0.0001, step_loss=0.068]\n",
      "Steps:   5%|▌         | 51/1000 [06:42<2:05:10,  7.91s/it, lr=0.0001, step_loss=0.0611]\n",
      "Steps:   5%|▌         | 52/1000 [06:50<2:05:36,  7.95s/it, lr=0.0001, step_loss=0.0611]\n",
      "Steps:   5%|▌         | 52/1000 [06:50<2:05:36,  7.95s/it, lr=0.0001, step_loss=0.185]\n",
      "Steps:   5%|▌         | 53/1000 [06:57<2:04:53,  7.91s/it, lr=0.0001, step_loss=0.185]\n",
      "Steps:   5%|▌         | 53/1000 [06:57<2:04:53,  7.91s/it, lr=0.0001, step_loss=0.0572]\n",
      "Steps:   5%|▌         | 54/1000 [07:06<2:05:50,  7.98s/it, lr=0.0001, step_loss=0.0572]\n",
      "Steps:   5%|▌         | 54/1000 [07:06<2:05:50,  7.98s/it, lr=0.0001, step_loss=0.0879]\n",
      "Steps:   6%|▌         | 55/1000 [07:13<2:04:36,  7.91s/it, lr=0.0001, step_loss=0.0879]\n",
      "Steps:   6%|▌         | 55/1000 [07:13<2:04:36,  7.91s/it, lr=0.0001, step_loss=0.052]\n",
      "Steps:   6%|▌         | 56/1000 [07:21<2:05:02,  7.95s/it, lr=0.0001, step_loss=0.052]\n",
      "Steps:   6%|▌         | 56/1000 [07:21<2:05:02,  7.95s/it, lr=0.0001, step_loss=0.0391]\n",
      "Steps:   6%|▌         | 57/1000 [07:29<2:04:28,  7.92s/it, lr=0.0001, step_loss=0.0391]\n",
      "Steps:   6%|▌         | 57/1000 [07:29<2:04:28,  7.92s/it, lr=0.0001, step_loss=0.0479]\n",
      "Steps:   6%|▌         | 58/1000 [07:37<2:05:15,  7.98s/it, lr=0.0001, step_loss=0.0479]\n",
      "Steps:   6%|▌         | 58/1000 [07:37<2:05:15,  7.98s/it, lr=0.0001, step_loss=0.0261]\n",
      "Steps:   6%|▌         | 59/1000 [07:45<2:04:07,  7.91s/it, lr=0.0001, step_loss=0.0261]\n",
      "Steps:   6%|▌         | 59/1000 [07:45<2:04:07,  7.91s/it, lr=0.0001, step_loss=0.0492]\n",
      "Steps:   6%|▌         | 60/1000 [07:53<2:04:32,  7.95s/it, lr=0.0001, step_loss=0.0492]\n",
      "Steps:   6%|▌         | 60/1000 [07:53<2:04:32,  7.95s/it, lr=0.0001, step_loss=0.0259]\n",
      "Steps:   6%|▌         | 61/1000 [08:01<2:04:10,  7.93s/it, lr=0.0001, step_loss=0.0259]\n",
      "Steps:   6%|▌         | 61/1000 [08:01<2:04:10,  7.93s/it, lr=0.0001, step_loss=0.0512]\n",
      "Steps:   6%|▌         | 62/1000 [08:09<2:04:58,  7.99s/it, lr=0.0001, step_loss=0.0512]\n",
      "Steps:   6%|▌         | 62/1000 [08:09<2:04:58,  7.99s/it, lr=0.0001, step_loss=0.0765]\n",
      "Steps:   6%|▋         | 63/1000 [08:17<2:03:45,  7.92s/it, lr=0.0001, step_loss=0.0765]\n",
      "Steps:   6%|▋         | 63/1000 [08:17<2:03:45,  7.92s/it, lr=0.0001, step_loss=0.0343]\n",
      "Steps:   6%|▋         | 64/1000 [08:25<2:04:09,  7.96s/it, lr=0.0001, step_loss=0.0343]\n",
      "Steps:   6%|▋         | 64/1000 [08:25<2:04:09,  7.96s/it, lr=0.0001, step_loss=0.0469]\n",
      "Steps:   6%|▋         | 65/1000 [08:33<2:03:22,  7.92s/it, lr=0.0001, step_loss=0.0469]\n",
      "Steps:   6%|▋         | 65/1000 [08:33<2:03:22,  7.92s/it, lr=0.0001, step_loss=0.0364]\n",
      "Steps:   7%|▋         | 66/1000 [08:41<2:04:30,  8.00s/it, lr=0.0001, step_loss=0.0364]\n",
      "Steps:   7%|▋         | 66/1000 [08:41<2:04:30,  8.00s/it, lr=0.0001, step_loss=0.0417]\n",
      "Steps:   7%|▋         | 67/1000 [08:49<2:03:04,  7.91s/it, lr=0.0001, step_loss=0.0417]\n",
      "Steps:   7%|▋         | 67/1000 [08:49<2:03:04,  7.91s/it, lr=0.0001, step_loss=0.0582]\n",
      "Steps:   7%|▋         | 68/1000 [08:57<2:03:33,  7.95s/it, lr=0.0001, step_loss=0.0582]\n",
      "Steps:   7%|▋         | 68/1000 [08:57<2:03:33,  7.95s/it, lr=0.0001, step_loss=0.0384]\n",
      "Steps:   7%|▋         | 69/1000 [09:05<2:03:00,  7.93s/it, lr=0.0001, step_loss=0.0384]\n",
      "Steps:   7%|▋         | 69/1000 [09:05<2:03:00,  7.93s/it, lr=0.0001, step_loss=0.0294]\n",
      "Steps:   7%|▋         | 70/1000 [09:13<2:03:46,  7.99s/it, lr=0.0001, step_loss=0.0294]\n",
      "Steps:   7%|▋         | 70/1000 [09:13<2:03:46,  7.99s/it, lr=0.0001, step_loss=0.0263]\n",
      "Steps:   7%|▋         | 71/1000 [09:20<2:02:28,  7.91s/it, lr=0.0001, step_loss=0.0263]\n",
      "Steps:   7%|▋         | 71/1000 [09:20<2:02:28,  7.91s/it, lr=0.0001, step_loss=0.066]\n",
      "Steps:   7%|▋         | 72/1000 [09:28<2:02:58,  7.95s/it, lr=0.0001, step_loss=0.066]\n",
      "Steps:   7%|▋         | 72/1000 [09:28<2:02:58,  7.95s/it, lr=0.0001, step_loss=0.0252]\n",
      "Steps:   7%|▋         | 73/1000 [09:36<2:02:12,  7.91s/it, lr=0.0001, step_loss=0.0252]\n",
      "Steps:   7%|▋         | 73/1000 [09:36<2:02:12,  7.91s/it, lr=0.0001, step_loss=0.0207]\n",
      "Steps:   7%|▋         | 74/1000 [09:44<2:03:14,  7.99s/it, lr=0.0001, step_loss=0.0207]\n",
      "Steps:   7%|▋         | 74/1000 [09:44<2:03:14,  7.99s/it, lr=0.0001, step_loss=0.065]\n",
      "Steps:   8%|▊         | 75/1000 [09:52<2:01:59,  7.91s/it, lr=0.0001, step_loss=0.065]\n",
      "Steps:   8%|▊         | 75/1000 [09:52<2:01:59,  7.91s/it, lr=0.0001, step_loss=0.0759]\n",
      "Steps:   8%|▊         | 76/1000 [10:00<2:02:32,  7.96s/it, lr=0.0001, step_loss=0.0759]\n",
      "Steps:   8%|▊         | 76/1000 [10:00<2:02:32,  7.96s/it, lr=0.0001, step_loss=0.0218]\n",
      "Steps:   8%|▊         | 77/1000 [10:08<2:01:40,  7.91s/it, lr=0.0001, step_loss=0.0218]\n",
      "Steps:   8%|▊         | 77/1000 [10:08<2:01:40,  7.91s/it, lr=0.0001, step_loss=0.0813]\n",
      "Steps:   8%|▊         | 78/1000 [10:16<2:02:45,  7.99s/it, lr=0.0001, step_loss=0.0813]\n",
      "Steps:   8%|▊         | 78/1000 [10:16<2:02:45,  7.99s/it, lr=0.0001, step_loss=0.0125]\n",
      "Steps:   8%|▊         | 79/1000 [10:24<2:01:27,  7.91s/it, lr=0.0001, step_loss=0.0125]\n",
      "Steps:   8%|▊         | 79/1000 [10:24<2:01:27,  7.91s/it, lr=0.0001, step_loss=0.0375]\n",
      "Steps:   8%|▊         | 80/1000 [10:32<2:01:51,  7.95s/it, lr=0.0001, step_loss=0.0375]\n",
      "Steps:   8%|▊         | 80/1000 [10:32<2:01:51,  7.95s/it, lr=0.0001, step_loss=0.04]\n",
      "Steps:   8%|▊         | 81/1000 [10:40<2:01:11,  7.91s/it, lr=0.0001, step_loss=0.04]\n",
      "Steps:   8%|▊         | 81/1000 [10:40<2:01:11,  7.91s/it, lr=0.0001, step_loss=0.0606]\n",
      "Steps:   8%|▊         | 82/1000 [10:48<2:02:10,  7.99s/it, lr=0.0001, step_loss=0.0606]\n",
      "Steps:   8%|▊         | 82/1000 [10:48<2:02:10,  7.99s/it, lr=0.0001, step_loss=0.0509]\n",
      "Steps:   8%|▊         | 83/1000 [10:56<2:00:57,  7.91s/it, lr=0.0001, step_loss=0.0509]\n",
      "Steps:   8%|▊         | 83/1000 [10:56<2:00:57,  7.91s/it, lr=0.0001, step_loss=0.0396]\n",
      "Steps:   8%|▊         | 84/1000 [11:04<2:01:21,  7.95s/it, lr=0.0001, step_loss=0.0396]\n",
      "Steps:   8%|▊         | 84/1000 [11:04<2:01:21,  7.95s/it, lr=0.0001, step_loss=0.0414]\n",
      "Steps:   8%|▊         | 85/1000 [11:12<2:00:44,  7.92s/it, lr=0.0001, step_loss=0.0414]\n",
      "Steps:   8%|▊         | 85/1000 [11:12<2:00:44,  7.92s/it, lr=0.0001, step_loss=0.057]\n",
      "Steps:   9%|▊         | 86/1000 [11:20<2:01:38,  7.98s/it, lr=0.0001, step_loss=0.057]\n",
      "Steps:   9%|▊         | 86/1000 [11:20<2:01:38,  7.98s/it, lr=0.0001, step_loss=0.0466]\n",
      "Steps:   9%|▊         | 87/1000 [11:27<2:00:25,  7.91s/it, lr=0.0001, step_loss=0.0466]\n",
      "Steps:   9%|▊         | 87/1000 [11:27<2:00:25,  7.91s/it, lr=0.0001, step_loss=0.0444]\n",
      "Steps:   9%|▉         | 88/1000 [11:36<2:00:54,  7.95s/it, lr=0.0001, step_loss=0.0444]\n",
      "Steps:   9%|▉         | 88/1000 [11:36<2:00:54,  7.95s/it, lr=0.0001, step_loss=0.0221]\n",
      "Steps:   9%|▉         | 89/1000 [11:43<2:00:07,  7.91s/it, lr=0.0001, step_loss=0.0221]\n",
      "Steps:   9%|▉         | 89/1000 [11:43<2:00:07,  7.91s/it, lr=0.0001, step_loss=0.0764]\n",
      "Steps:   9%|▉         | 90/1000 [11:51<2:01:01,  7.98s/it, lr=0.0001, step_loss=0.0764]\n",
      "Steps:   9%|▉         | 90/1000 [11:51<2:01:01,  7.98s/it, lr=0.0001, step_loss=0.0519]\n",
      "Steps:   9%|▉         | 91/1000 [11:59<2:00:03,  7.93s/it, lr=0.0001, step_loss=0.0519]\n",
      "Steps:   9%|▉         | 91/1000 [11:59<2:00:03,  7.93s/it, lr=0.0001, step_loss=0.0304]\n",
      "Steps:   9%|▉         | 92/1000 [12:07<2:00:20,  7.95s/it, lr=0.0001, step_loss=0.0304]\n",
      "Steps:   9%|▉         | 92/1000 [12:07<2:00:20,  7.95s/it, lr=0.0001, step_loss=0.0395]\n",
      "Steps:   9%|▉         | 93/1000 [12:15<1:59:38,  7.92s/it, lr=0.0001, step_loss=0.0395]\n",
      "Steps:   9%|▉         | 93/1000 [12:15<1:59:38,  7.92s/it, lr=0.0001, step_loss=0.108]\n",
      "Steps:   9%|▉         | 94/1000 [12:23<2:00:40,  7.99s/it, lr=0.0001, step_loss=0.108]\n",
      "Steps:   9%|▉         | 94/1000 [12:23<2:00:40,  7.99s/it, lr=0.0001, step_loss=0.103]\n",
      "Steps:  10%|▉         | 95/1000 [12:31<1:59:27,  7.92s/it, lr=0.0001, step_loss=0.103]\n",
      "Steps:  10%|▉         | 95/1000 [12:31<1:59:27,  7.92s/it, lr=0.0001, step_loss=0.0259]\n",
      "Steps:  10%|▉         | 96/1000 [12:39<1:59:53,  7.96s/it, lr=0.0001, step_loss=0.0259]\n",
      "Steps:  10%|▉         | 96/1000 [12:39<1:59:53,  7.96s/it, lr=0.0001, step_loss=0.0352]\n",
      "Steps:  10%|▉         | 97/1000 [12:47<1:59:06,  7.91s/it, lr=0.0001, step_loss=0.0352]\n",
      "Steps:  10%|▉         | 97/1000 [12:47<1:59:06,  7.91s/it, lr=0.0001, step_loss=0.0161]\n",
      "Steps:  10%|▉         | 98/1000 [12:55<2:00:08,  7.99s/it, lr=0.0001, step_loss=0.0161]\n",
      "Steps:  10%|▉         | 98/1000 [12:55<2:00:08,  7.99s/it, lr=0.0001, step_loss=0.076]\n",
      "Steps:  10%|▉         | 99/1000 [13:03<1:58:50,  7.91s/it, lr=0.0001, step_loss=0.076]\n",
      "Steps:  10%|▉         | 99/1000 [13:03<1:58:50,  7.91s/it, lr=0.0001, step_loss=0.0665]\n",
      "Steps:  10%|█         | 100/1000 [13:11<1:59:13,  7.95s/it, lr=0.0001, step_loss=0.0665]04/17/2024 04:08:17 - INFO - accelerate.accelerator - Saving current state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-100\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-100\\unet_ema\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-100/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-100\\unet\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-100/unet/diffusion_pytorch_model.safetensors\n",
      "04/17/2024 04:08:56 - INFO - accelerate.checkpointing - Optimizer state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-100\\optimizer.bin\n",
      "04/17/2024 04:08:56 - INFO - accelerate.checkpointing - Scheduler state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-100\\scheduler.bin\n",
      "04/17/2024 04:08:56 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-100\\sampler.bin\n",
      "04/17/2024 04:08:56 - INFO - accelerate.checkpointing - Random states saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-100\\random_states_0.pkl\n",
      "04/17/2024 04:08:56 - INFO - __main__ - Saved state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-100\n",
      "\n",
      "Steps:  10%|█         | 100/1000 [13:50<1:59:13,  7.95s/it, lr=0.0001, step_loss=0.0719]\n",
      "Steps:  10%|█         | 101/1000 [13:59<4:58:43, 19.94s/it, lr=0.0001, step_loss=0.0719]\n",
      "Steps:  10%|█         | 101/1000 [13:59<4:58:43, 19.94s/it, lr=0.0001, step_loss=0.0442]\n",
      "Steps:  10%|█         | 102/1000 [14:07<4:08:00, 16.57s/it, lr=0.0001, step_loss=0.0442]\n",
      "Steps:  10%|█         | 102/1000 [14:07<4:08:00, 16.57s/it, lr=0.0001, step_loss=0.0463]\n",
      "Steps:  10%|█         | 103/1000 [14:16<3:30:33, 14.08s/it, lr=0.0001, step_loss=0.0463]\n",
      "Steps:  10%|█         | 103/1000 [14:16<3:30:33, 14.08s/it, lr=0.0001, step_loss=0.0415]\n",
      "Steps:  10%|█         | 104/1000 [14:24<3:05:43, 12.44s/it, lr=0.0001, step_loss=0.0415]\n",
      "Steps:  10%|█         | 104/1000 [14:24<3:05:43, 12.44s/it, lr=0.0001, step_loss=0.0695]\n",
      "Steps:  10%|█         | 105/1000 [14:33<2:47:24, 11.22s/it, lr=0.0001, step_loss=0.0695]\n",
      "Steps:  10%|█         | 105/1000 [14:33<2:47:24, 11.22s/it, lr=0.0001, step_loss=0.0507]\n",
      "Steps:  11%|█         | 106/1000 [14:41<2:35:53, 10.46s/it, lr=0.0001, step_loss=0.0507]\n",
      "Steps:  11%|█         | 106/1000 [14:41<2:35:53, 10.46s/it, lr=0.0001, step_loss=0.0355]\n",
      "Steps:  11%|█         | 107/1000 [14:50<2:26:02,  9.81s/it, lr=0.0001, step_loss=0.0355]\n",
      "Steps:  11%|█         | 107/1000 [14:50<2:26:02,  9.81s/it, lr=0.0001, step_loss=0.0586]\n",
      "Steps:  11%|█         | 108/1000 [14:58<2:20:30,  9.45s/it, lr=0.0001, step_loss=0.0586]\n",
      "Steps:  11%|█         | 108/1000 [14:58<2:20:30,  9.45s/it, lr=0.0001, step_loss=0.0427]\n",
      "Steps:  11%|█         | 109/1000 [15:07<2:15:33,  9.13s/it, lr=0.0001, step_loss=0.0427]\n",
      "Steps:  11%|█         | 109/1000 [15:07<2:15:33,  9.13s/it, lr=0.0001, step_loss=0.0287]\n",
      "Steps:  11%|█         | 110/1000 [15:15<2:13:35,  9.01s/it, lr=0.0001, step_loss=0.0287]\n",
      "Steps:  11%|█         | 110/1000 [15:15<2:13:35,  9.01s/it, lr=0.0001, step_loss=0.0443]\n",
      "Steps:  11%|█         | 111/1000 [15:24<2:10:08,  8.78s/it, lr=0.0001, step_loss=0.0443]\n",
      "Steps:  11%|█         | 111/1000 [15:24<2:10:08,  8.78s/it, lr=0.0001, step_loss=0.0424]\n",
      "Steps:  11%|█         | 112/1000 [15:32<2:09:12,  8.73s/it, lr=0.0001, step_loss=0.0424]\n",
      "Steps:  11%|█         | 112/1000 [15:32<2:09:12,  8.73s/it, lr=0.0001, step_loss=0.0274]\n",
      "Steps:  11%|█▏        | 113/1000 [15:41<2:07:34,  8.63s/it, lr=0.0001, step_loss=0.0274]\n",
      "Steps:  11%|█▏        | 113/1000 [15:41<2:07:34,  8.63s/it, lr=0.0001, step_loss=0.0355]\n",
      "Steps:  11%|█▏        | 114/1000 [15:49<2:07:49,  8.66s/it, lr=0.0001, step_loss=0.0355]\n",
      "Steps:  11%|█▏        | 114/1000 [15:49<2:07:49,  8.66s/it, lr=0.0001, step_loss=0.0265]\n",
      "Steps:  12%|█▏        | 115/1000 [15:58<2:06:13,  8.56s/it, lr=0.0001, step_loss=0.0265]\n",
      "Steps:  12%|█▏        | 115/1000 [15:58<2:06:13,  8.56s/it, lr=0.0001, step_loss=0.0504]\n",
      "Steps:  12%|█▏        | 116/1000 [16:06<2:06:14,  8.57s/it, lr=0.0001, step_loss=0.0504]\n",
      "Steps:  12%|█▏        | 116/1000 [16:06<2:06:14,  8.57s/it, lr=0.0001, step_loss=0.0518]\n",
      "Steps:  12%|█▏        | 117/1000 [16:15<2:05:21,  8.52s/it, lr=0.0001, step_loss=0.0518]\n",
      "Steps:  12%|█▏        | 117/1000 [16:15<2:05:21,  8.52s/it, lr=0.0001, step_loss=0.0219]\n",
      "Steps:  12%|█▏        | 118/1000 [16:23<2:06:03,  8.58s/it, lr=0.0001, step_loss=0.0219]\n",
      "Steps:  12%|█▏        | 118/1000 [16:23<2:06:03,  8.58s/it, lr=0.0001, step_loss=0.0238]\n",
      "Steps:  12%|█▏        | 119/1000 [16:32<2:04:41,  8.49s/it, lr=0.0001, step_loss=0.0238]\n",
      "Steps:  12%|█▏        | 119/1000 [16:32<2:04:41,  8.49s/it, lr=0.0001, step_loss=0.0135]\n",
      "Steps:  12%|█▏        | 120/1000 [16:40<2:04:55,  8.52s/it, lr=0.0001, step_loss=0.0135]\n",
      "Steps:  12%|█▏        | 120/1000 [16:40<2:04:55,  8.52s/it, lr=0.0001, step_loss=0.0403]\n",
      "Steps:  12%|█▏        | 121/1000 [16:49<2:04:03,  8.47s/it, lr=0.0001, step_loss=0.0403]\n",
      "Steps:  12%|█▏        | 121/1000 [16:49<2:04:03,  8.47s/it, lr=0.0001, step_loss=0.0416]\n",
      "Steps:  12%|█▏        | 122/1000 [16:57<2:05:06,  8.55s/it, lr=0.0001, step_loss=0.0416]\n",
      "Steps:  12%|█▏        | 122/1000 [16:57<2:05:06,  8.55s/it, lr=0.0001, step_loss=0.0312]\n",
      "Steps:  12%|█▏        | 123/1000 [17:06<2:03:51,  8.47s/it, lr=0.0001, step_loss=0.0312]\n",
      "Steps:  12%|█▏        | 123/1000 [17:06<2:03:51,  8.47s/it, lr=0.0001, step_loss=0.0357]\n",
      "Steps:  12%|█▏        | 124/1000 [17:14<2:04:14,  8.51s/it, lr=0.0001, step_loss=0.0357]\n",
      "Steps:  12%|█▏        | 124/1000 [17:14<2:04:14,  8.51s/it, lr=0.0001, step_loss=0.0329]\n",
      "Steps:  12%|█▎        | 125/1000 [17:23<2:03:34,  8.47s/it, lr=0.0001, step_loss=0.0329]\n",
      "Steps:  12%|█▎        | 125/1000 [17:23<2:03:34,  8.47s/it, lr=0.0001, step_loss=0.0359]\n",
      "Steps:  13%|█▎        | 126/1000 [17:31<2:04:22,  8.54s/it, lr=0.0001, step_loss=0.0359]\n",
      "Steps:  13%|█▎        | 126/1000 [17:31<2:04:22,  8.54s/it, lr=0.0001, step_loss=0.0534]\n",
      "Steps:  13%|█▎        | 127/1000 [17:40<2:03:10,  8.47s/it, lr=0.0001, step_loss=0.0534]\n",
      "Steps:  13%|█▎        | 127/1000 [17:40<2:03:10,  8.47s/it, lr=0.0001, step_loss=0.0162]\n",
      "Steps:  13%|█▎        | 128/1000 [17:48<2:03:31,  8.50s/it, lr=0.0001, step_loss=0.0162]\n",
      "Steps:  13%|█▎        | 128/1000 [17:48<2:03:31,  8.50s/it, lr=0.0001, step_loss=0.0957]\n",
      "Steps:  13%|█▎        | 129/1000 [17:57<2:02:54,  8.47s/it, lr=0.0001, step_loss=0.0957]\n",
      "Steps:  13%|█▎        | 129/1000 [17:57<2:02:54,  8.47s/it, lr=0.0001, step_loss=0.0433]\n",
      "Steps:  13%|█▎        | 130/1000 [18:05<2:03:55,  8.55s/it, lr=0.0001, step_loss=0.0433]\n",
      "Steps:  13%|█▎        | 130/1000 [18:05<2:03:55,  8.55s/it, lr=0.0001, step_loss=0.0316]\n",
      "Steps:  13%|█▎        | 131/1000 [18:14<2:02:42,  8.47s/it, lr=0.0001, step_loss=0.0316]\n",
      "Steps:  13%|█▎        | 131/1000 [18:14<2:02:42,  8.47s/it, lr=0.0001, step_loss=0.0348]\n",
      "Steps:  13%|█▎        | 132/1000 [18:22<2:03:06,  8.51s/it, lr=0.0001, step_loss=0.0348]\n",
      "Steps:  13%|█▎        | 132/1000 [18:22<2:03:06,  8.51s/it, lr=0.0001, step_loss=0.0201]\n",
      "Steps:  13%|█▎        | 133/1000 [18:31<2:02:30,  8.48s/it, lr=0.0001, step_loss=0.0201]\n",
      "Steps:  13%|█▎        | 133/1000 [18:31<2:02:30,  8.48s/it, lr=0.0001, step_loss=0.0648]\n",
      "Steps:  13%|█▎        | 134/1000 [18:39<2:03:20,  8.55s/it, lr=0.0001, step_loss=0.0648]\n",
      "Steps:  13%|█▎        | 134/1000 [18:39<2:03:20,  8.55s/it, lr=0.0001, step_loss=0.0301]\n",
      "Steps:  14%|█▎        | 135/1000 [18:48<2:02:03,  8.47s/it, lr=0.0001, step_loss=0.0301]\n",
      "Steps:  14%|█▎        | 135/1000 [18:48<2:02:03,  8.47s/it, lr=0.0001, step_loss=0.0584]\n",
      "Steps:  14%|█▎        | 136/1000 [18:56<2:02:34,  8.51s/it, lr=0.0001, step_loss=0.0584]\n",
      "Steps:  14%|█▎        | 136/1000 [18:56<2:02:34,  8.51s/it, lr=0.0001, step_loss=0.0684]\n",
      "Steps:  14%|█▎        | 137/1000 [19:05<2:01:58,  8.48s/it, lr=0.0001, step_loss=0.0684]\n",
      "Steps:  14%|█▎        | 137/1000 [19:05<2:01:58,  8.48s/it, lr=0.0001, step_loss=0.0278]\n",
      "Steps:  14%|█▍        | 138/1000 [19:13<2:02:48,  8.55s/it, lr=0.0001, step_loss=0.0278]\n",
      "Steps:  14%|█▍        | 138/1000 [19:13<2:02:48,  8.55s/it, lr=0.0001, step_loss=0.0349]\n",
      "Steps:  14%|█▍        | 139/1000 [19:22<2:01:35,  8.47s/it, lr=0.0001, step_loss=0.0349]\n",
      "Steps:  14%|█▍        | 139/1000 [19:22<2:01:35,  8.47s/it, lr=0.0001, step_loss=0.0277]\n",
      "Steps:  14%|█▍        | 140/1000 [19:30<2:01:58,  8.51s/it, lr=0.0001, step_loss=0.0277]\n",
      "Steps:  14%|█▍        | 140/1000 [19:30<2:01:58,  8.51s/it, lr=0.0001, step_loss=0.0536]\n",
      "Steps:  14%|█▍        | 141/1000 [19:39<2:01:16,  8.47s/it, lr=0.0001, step_loss=0.0536]\n",
      "Steps:  14%|█▍        | 141/1000 [19:39<2:01:16,  8.47s/it, lr=0.0001, step_loss=0.0547]\n",
      "Steps:  14%|█▍        | 142/1000 [19:47<2:02:10,  8.54s/it, lr=0.0001, step_loss=0.0547]\n",
      "Steps:  14%|█▍        | 142/1000 [19:47<2:02:10,  8.54s/it, lr=0.0001, step_loss=0.0291]\n",
      "Steps:  14%|█▍        | 143/1000 [19:56<2:00:58,  8.47s/it, lr=0.0001, step_loss=0.0291]\n",
      "Steps:  14%|█▍        | 143/1000 [19:56<2:00:58,  8.47s/it, lr=0.0001, step_loss=0.0278]\n",
      "Steps:  14%|█▍        | 144/1000 [20:04<2:01:25,  8.51s/it, lr=0.0001, step_loss=0.0278]\n",
      "Steps:  14%|█▍        | 144/1000 [20:04<2:01:25,  8.51s/it, lr=0.0001, step_loss=0.0352]\n",
      "Steps:  14%|█▍        | 145/1000 [20:13<2:00:46,  8.48s/it, lr=0.0001, step_loss=0.0352]\n",
      "Steps:  14%|█▍        | 145/1000 [20:13<2:00:46,  8.48s/it, lr=0.0001, step_loss=0.0528]\n",
      "Steps:  15%|█▍        | 146/1000 [20:22<2:04:36,  8.75s/it, lr=0.0001, step_loss=0.0528]\n",
      "Steps:  15%|█▍        | 146/1000 [20:22<2:04:36,  8.75s/it, lr=0.0001, step_loss=0.0636]\n",
      "Steps:  15%|█▍        | 147/1000 [20:31<2:04:22,  8.75s/it, lr=0.0001, step_loss=0.0636]\n",
      "Steps:  15%|█▍        | 147/1000 [20:31<2:04:22,  8.75s/it, lr=0.0001, step_loss=0.0225]\n",
      "Steps:  15%|█▍        | 148/1000 [20:40<2:05:17,  8.82s/it, lr=0.0001, step_loss=0.0225]\n",
      "Steps:  15%|█▍        | 148/1000 [20:40<2:05:17,  8.82s/it, lr=0.0001, step_loss=0.0318]\n",
      "Steps:  15%|█▍        | 149/1000 [20:49<2:05:14,  8.83s/it, lr=0.0001, step_loss=0.0318]\n",
      "Steps:  15%|█▍        | 149/1000 [20:49<2:05:14,  8.83s/it, lr=0.0001, step_loss=0.0363]\n",
      "Steps:  15%|█▌        | 150/1000 [20:58<2:06:20,  8.92s/it, lr=0.0001, step_loss=0.0363]\n",
      "Steps:  15%|█▌        | 150/1000 [20:58<2:06:20,  8.92s/it, lr=0.0001, step_loss=0.0429]\n",
      "Steps:  15%|█▌        | 151/1000 [21:07<2:05:24,  8.86s/it, lr=0.0001, step_loss=0.0429]\n",
      "Steps:  15%|█▌        | 151/1000 [21:07<2:05:24,  8.86s/it, lr=0.0001, step_loss=0.0111]\n",
      "Steps:  15%|█▌        | 152/1000 [21:16<2:05:54,  8.91s/it, lr=0.0001, step_loss=0.0111]\n",
      "Steps:  15%|█▌        | 152/1000 [21:16<2:05:54,  8.91s/it, lr=0.0001, step_loss=0.0234]\n",
      "Steps:  15%|█▌        | 153/1000 [21:24<2:05:17,  8.88s/it, lr=0.0001, step_loss=0.0234]\n",
      "Steps:  15%|█▌        | 153/1000 [21:24<2:05:17,  8.88s/it, lr=0.0001, step_loss=0.0389]\n",
      "Steps:  15%|█▌        | 154/1000 [21:33<2:06:11,  8.95s/it, lr=0.0001, step_loss=0.0389]\n",
      "Steps:  15%|█▌        | 154/1000 [21:33<2:06:11,  8.95s/it, lr=0.0001, step_loss=0.0385]\n",
      "Steps:  16%|█▌        | 155/1000 [21:42<2:05:12,  8.89s/it, lr=0.0001, step_loss=0.0385]\n",
      "Steps:  16%|█▌        | 155/1000 [21:42<2:05:12,  8.89s/it, lr=0.0001, step_loss=0.0587]\n",
      "Steps:  16%|█▌        | 156/1000 [21:51<2:05:36,  8.93s/it, lr=0.0001, step_loss=0.0587]\n",
      "Steps:  16%|█▌        | 156/1000 [21:51<2:05:36,  8.93s/it, lr=0.0001, step_loss=0.0353]\n",
      "Steps:  16%|█▌        | 157/1000 [22:00<2:05:08,  8.91s/it, lr=0.0001, step_loss=0.0353]\n",
      "Steps:  16%|█▌        | 157/1000 [22:00<2:05:08,  8.91s/it, lr=0.0001, step_loss=0.0345]\n",
      "Steps:  16%|█▌        | 158/1000 [22:09<2:05:54,  8.97s/it, lr=0.0001, step_loss=0.0345]\n",
      "Steps:  16%|█▌        | 158/1000 [22:09<2:05:54,  8.97s/it, lr=0.0001, step_loss=0.0232]\n",
      "Steps:  16%|█▌        | 159/1000 [22:18<2:04:49,  8.91s/it, lr=0.0001, step_loss=0.0232]\n",
      "Steps:  16%|█▌        | 159/1000 [22:18<2:04:49,  8.91s/it, lr=0.0001, step_loss=0.0235]\n",
      "Steps:  16%|█▌        | 160/1000 [22:27<2:05:08,  8.94s/it, lr=0.0001, step_loss=0.0235]\n",
      "Steps:  16%|█▌        | 160/1000 [22:27<2:05:08,  8.94s/it, lr=0.0001, step_loss=0.0338]\n",
      "Steps:  16%|█▌        | 161/1000 [22:36<2:04:39,  8.91s/it, lr=0.0001, step_loss=0.0338]\n",
      "Steps:  16%|█▌        | 161/1000 [22:36<2:04:39,  8.91s/it, lr=0.0001, step_loss=0.046]\n",
      "Steps:  16%|█▌        | 162/1000 [22:45<2:05:22,  8.98s/it, lr=0.0001, step_loss=0.046]\n",
      "Steps:  16%|█▌        | 162/1000 [22:45<2:05:22,  8.98s/it, lr=0.0001, step_loss=0.0191]\n",
      "Steps:  16%|█▋        | 163/1000 [22:54<2:03:58,  8.89s/it, lr=0.0001, step_loss=0.0191]\n",
      "Steps:  16%|█▋        | 163/1000 [22:54<2:03:58,  8.89s/it, lr=0.0001, step_loss=0.022]\n",
      "Steps:  16%|█▋        | 164/1000 [23:03<2:04:25,  8.93s/it, lr=0.0001, step_loss=0.022]\n",
      "Steps:  16%|█▋        | 164/1000 [23:03<2:04:25,  8.93s/it, lr=0.0001, step_loss=0.0693]\n",
      "Steps:  16%|█▋        | 165/1000 [23:11<2:03:54,  8.90s/it, lr=0.0001, step_loss=0.0693]\n",
      "Steps:  16%|█▋        | 165/1000 [23:12<2:03:54,  8.90s/it, lr=0.0001, step_loss=0.0406]\n",
      "Steps:  17%|█▋        | 166/1000 [23:21<2:04:37,  8.97s/it, lr=0.0001, step_loss=0.0406]\n",
      "Steps:  17%|█▋        | 166/1000 [23:21<2:04:37,  8.97s/it, lr=0.0001, step_loss=0.0213]\n",
      "Steps:  17%|█▋        | 167/1000 [23:29<2:03:30,  8.90s/it, lr=0.0001, step_loss=0.0213]\n",
      "Steps:  17%|█▋        | 167/1000 [23:29<2:03:30,  8.90s/it, lr=0.0001, step_loss=0.0222]\n",
      "Steps:  17%|█▋        | 168/1000 [23:38<2:03:47,  8.93s/it, lr=0.0001, step_loss=0.0222]\n",
      "Steps:  17%|█▋        | 168/1000 [23:38<2:03:47,  8.93s/it, lr=0.0001, step_loss=0.0345]\n",
      "Steps:  17%|█▋        | 169/1000 [23:47<2:03:08,  8.89s/it, lr=0.0001, step_loss=0.0345]\n",
      "Steps:  17%|█▋        | 169/1000 [23:47<2:03:08,  8.89s/it, lr=0.0001, step_loss=0.0148]\n",
      "Steps:  17%|█▋        | 170/1000 [23:56<2:04:03,  8.97s/it, lr=0.0001, step_loss=0.0148]\n",
      "Steps:  17%|█▋        | 170/1000 [23:56<2:04:03,  8.97s/it, lr=0.0001, step_loss=0.118]\n",
      "Steps:  17%|█▋        | 171/1000 [24:05<2:02:56,  8.90s/it, lr=0.0001, step_loss=0.118]\n",
      "Steps:  17%|█▋        | 171/1000 [24:05<2:02:56,  8.90s/it, lr=0.0001, step_loss=0.0412]\n",
      "Steps:  17%|█▋        | 172/1000 [24:14<2:03:20,  8.94s/it, lr=0.0001, step_loss=0.0412]\n",
      "Steps:  17%|█▋        | 172/1000 [24:14<2:03:20,  8.94s/it, lr=0.0001, step_loss=0.0284]\n",
      "Steps:  17%|█▋        | 173/1000 [24:23<2:02:40,  8.90s/it, lr=0.0001, step_loss=0.0284]\n",
      "Steps:  17%|█▋        | 173/1000 [24:23<2:02:40,  8.90s/it, lr=0.0001, step_loss=0.0324]\n",
      "Steps:  17%|█▋        | 174/1000 [24:32<2:03:27,  8.97s/it, lr=0.0001, step_loss=0.0324]\n",
      "Steps:  17%|█▋        | 174/1000 [24:32<2:03:27,  8.97s/it, lr=0.0001, step_loss=0.0504]\n",
      "Steps:  18%|█▊        | 175/1000 [24:41<2:02:20,  8.90s/it, lr=0.0001, step_loss=0.0504]\n",
      "Steps:  18%|█▊        | 175/1000 [24:41<2:02:20,  8.90s/it, lr=0.0001, step_loss=0.0307]\n",
      "Steps:  18%|█▊        | 176/1000 [24:50<2:02:40,  8.93s/it, lr=0.0001, step_loss=0.0307]\n",
      "Steps:  18%|█▊        | 176/1000 [24:50<2:02:40,  8.93s/it, lr=0.0001, step_loss=0.0173]\n",
      "Steps:  18%|█▊        | 177/1000 [24:59<2:01:58,  8.89s/it, lr=0.0001, step_loss=0.0173]\n",
      "Steps:  18%|█▊        | 177/1000 [24:59<2:01:58,  8.89s/it, lr=0.0001, step_loss=0.0299]\n",
      "Steps:  18%|█▊        | 178/1000 [25:08<2:02:46,  8.96s/it, lr=0.0001, step_loss=0.0299]\n",
      "Steps:  18%|█▊        | 178/1000 [25:08<2:02:46,  8.96s/it, lr=0.0001, step_loss=0.0152]\n",
      "Steps:  18%|█▊        | 179/1000 [25:16<2:01:41,  8.89s/it, lr=0.0001, step_loss=0.0152]\n",
      "Steps:  18%|█▊        | 179/1000 [25:16<2:01:41,  8.89s/it, lr=0.0001, step_loss=0.0206]\n",
      "Steps:  18%|█▊        | 180/1000 [25:25<2:02:02,  8.93s/it, lr=0.0001, step_loss=0.0206]\n",
      "Steps:  18%|█▊        | 180/1000 [25:25<2:02:02,  8.93s/it, lr=0.0001, step_loss=0.0301]\n",
      "Steps:  18%|█▊        | 181/1000 [25:34<2:01:28,  8.90s/it, lr=0.0001, step_loss=0.0301]\n",
      "Steps:  18%|█▊        | 181/1000 [25:34<2:01:28,  8.90s/it, lr=0.0001, step_loss=0.0722]\n",
      "Steps:  18%|█▊        | 182/1000 [25:43<2:02:15,  8.97s/it, lr=0.0001, step_loss=0.0722]\n",
      "Steps:  18%|█▊        | 182/1000 [25:43<2:02:15,  8.97s/it, lr=0.0001, step_loss=0.0162]\n",
      "Steps:  18%|█▊        | 183/1000 [25:52<2:01:01,  8.89s/it, lr=0.0001, step_loss=0.0162]\n",
      "Steps:  18%|█▊        | 183/1000 [25:52<2:01:01,  8.89s/it, lr=0.0001, step_loss=0.0344]\n",
      "Steps:  18%|█▊        | 184/1000 [26:01<2:01:35,  8.94s/it, lr=0.0001, step_loss=0.0344]\n",
      "Steps:  18%|█▊        | 184/1000 [26:01<2:01:35,  8.94s/it, lr=0.0001, step_loss=0.0179]\n",
      "Steps:  18%|█▊        | 185/1000 [26:10<2:00:55,  8.90s/it, lr=0.0001, step_loss=0.0179]\n",
      "Steps:  18%|█▊        | 185/1000 [26:10<2:00:55,  8.90s/it, lr=0.0001, step_loss=0.03]\n",
      "Steps:  19%|█▊        | 186/1000 [26:19<2:01:40,  8.97s/it, lr=0.0001, step_loss=0.03]\n",
      "Steps:  19%|█▊        | 186/1000 [26:19<2:01:40,  8.97s/it, lr=0.0001, step_loss=0.0269]\n",
      "Steps:  19%|█▊        | 187/1000 [26:28<2:00:34,  8.90s/it, lr=0.0001, step_loss=0.0269]\n",
      "Steps:  19%|█▊        | 187/1000 [26:28<2:00:34,  8.90s/it, lr=0.0001, step_loss=0.039]\n",
      "Steps:  19%|█▉        | 188/1000 [26:37<2:00:51,  8.93s/it, lr=0.0001, step_loss=0.039]\n",
      "Steps:  19%|█▉        | 188/1000 [26:37<2:00:51,  8.93s/it, lr=0.0001, step_loss=0.0268]\n",
      "Steps:  19%|█▉        | 189/1000 [26:46<2:00:16,  8.90s/it, lr=0.0001, step_loss=0.0268]\n",
      "Steps:  19%|█▉        | 189/1000 [26:46<2:00:16,  8.90s/it, lr=0.0001, step_loss=0.0381]\n",
      "Steps:  19%|█▉        | 190/1000 [26:55<2:01:02,  8.97s/it, lr=0.0001, step_loss=0.0381]\n",
      "Steps:  19%|█▉        | 190/1000 [26:55<2:01:02,  8.97s/it, lr=0.0001, step_loss=0.0425]\n",
      "Steps:  19%|█▉        | 191/1000 [27:03<1:59:53,  8.89s/it, lr=0.0001, step_loss=0.0425]\n",
      "Steps:  19%|█▉        | 191/1000 [27:03<1:59:53,  8.89s/it, lr=0.0001, step_loss=0.03]\n",
      "Steps:  19%|█▉        | 192/1000 [27:12<2:00:14,  8.93s/it, lr=0.0001, step_loss=0.03]\n",
      "Steps:  19%|█▉        | 192/1000 [27:12<2:00:14,  8.93s/it, lr=0.0001, step_loss=0.0318]\n",
      "Steps:  19%|█▉        | 193/1000 [27:21<1:59:33,  8.89s/it, lr=0.0001, step_loss=0.0318]\n",
      "Steps:  19%|█▉        | 193/1000 [27:21<1:59:33,  8.89s/it, lr=0.0001, step_loss=0.0122]\n",
      "Steps:  19%|█▉        | 194/1000 [27:30<2:00:18,  8.96s/it, lr=0.0001, step_loss=0.0122]\n",
      "Steps:  19%|█▉        | 194/1000 [27:30<2:00:18,  8.96s/it, lr=0.0001, step_loss=0.0279]\n",
      "Steps:  20%|█▉        | 195/1000 [27:39<1:59:19,  8.89s/it, lr=0.0001, step_loss=0.0279]\n",
      "Steps:  20%|█▉        | 195/1000 [27:39<1:59:19,  8.89s/it, lr=0.0001, step_loss=0.0161]\n",
      "Steps:  20%|█▉        | 196/1000 [27:48<1:59:39,  8.93s/it, lr=0.0001, step_loss=0.0161]\n",
      "Steps:  20%|█▉        | 196/1000 [27:48<1:59:39,  8.93s/it, lr=0.0001, step_loss=0.0228]\n",
      "Steps:  20%|█▉        | 197/1000 [27:57<1:59:06,  8.90s/it, lr=0.0001, step_loss=0.0228]\n",
      "Steps:  20%|█▉        | 197/1000 [27:57<1:59:06,  8.90s/it, lr=0.0001, step_loss=0.0264]\n",
      "Steps:  20%|█▉        | 198/1000 [28:06<1:59:59,  8.98s/it, lr=0.0001, step_loss=0.0264]\n",
      "Steps:  20%|█▉        | 198/1000 [28:06<1:59:59,  8.98s/it, lr=0.0001, step_loss=0.0653]\n",
      "Steps:  20%|█▉        | 199/1000 [28:15<1:58:46,  8.90s/it, lr=0.0001, step_loss=0.0653]\n",
      "Steps:  20%|█▉        | 199/1000 [28:15<1:58:46,  8.90s/it, lr=0.0001, step_loss=0.0355]\n",
      "Steps:  20%|██        | 200/1000 [28:24<1:59:04,  8.93s/it, lr=0.0001, step_loss=0.0355]04/17/2024 04:23:30 - INFO - accelerate.accelerator - Saving current state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-200\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-200\\unet_ema\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-200/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-200\\unet\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-200/unet/diffusion_pytorch_model.safetensors\n",
      "04/17/2024 04:24:06 - INFO - accelerate.checkpointing - Optimizer state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-200\\optimizer.bin\n",
      "04/17/2024 04:24:06 - INFO - accelerate.checkpointing - Scheduler state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-200\\scheduler.bin\n",
      "04/17/2024 04:24:06 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-200\\sampler.bin\n",
      "04/17/2024 04:24:06 - INFO - accelerate.checkpointing - Random states saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-200\\random_states_0.pkl\n",
      "04/17/2024 04:24:06 - INFO - __main__ - Saved state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-200\n",
      "\n",
      "Steps:  20%|██        | 200/1000 [29:00<1:59:04,  8.93s/it, lr=0.0001, step_loss=0.0154]\n",
      "Steps:  20%|██        | 201/1000 [29:09<4:25:27, 19.93s/it, lr=0.0001, step_loss=0.0154]\n",
      "Steps:  20%|██        | 201/1000 [29:09<4:25:27, 19.93s/it, lr=0.0001, step_loss=0.0233]\n",
      "Steps:  20%|██        | 202/1000 [29:19<3:41:56, 16.69s/it, lr=0.0001, step_loss=0.0233]\n",
      "Steps:  20%|██        | 202/1000 [29:19<3:41:56, 16.69s/it, lr=0.0001, step_loss=0.0638]\n",
      "Steps:  20%|██        | 203/1000 [29:27<3:09:53, 14.30s/it, lr=0.0001, step_loss=0.0638]\n",
      "Steps:  20%|██        | 203/1000 [29:27<3:09:53, 14.30s/it, lr=0.0001, step_loss=0.0189]\n",
      "Steps:  20%|██        | 204/1000 [29:36<2:48:38, 12.71s/it, lr=0.0001, step_loss=0.0189]\n",
      "Steps:  20%|██        | 204/1000 [29:36<2:48:38, 12.71s/it, lr=0.0001, step_loss=0.0274]\n",
      "Steps:  20%|██        | 205/1000 [29:45<2:32:52, 11.54s/it, lr=0.0001, step_loss=0.0274]\n",
      "Steps:  20%|██        | 205/1000 [29:45<2:32:52, 11.54s/it, lr=0.0001, step_loss=0.0387]\n",
      "Steps:  21%|██        | 206/1000 [29:54<2:23:13, 10.82s/it, lr=0.0001, step_loss=0.0387]\n",
      "Steps:  21%|██        | 206/1000 [29:54<2:23:13, 10.82s/it, lr=0.0001, step_loss=0.0279]\n",
      "Steps:  21%|██        | 207/1000 [30:03<2:14:57, 10.21s/it, lr=0.0001, step_loss=0.0279]\n",
      "Steps:  21%|██        | 207/1000 [30:03<2:14:57, 10.21s/it, lr=0.0001, step_loss=0.0237]\n",
      "Steps:  21%|██        | 208/1000 [30:12<2:09:59,  9.85s/it, lr=0.0001, step_loss=0.0237]\n",
      "Steps:  21%|██        | 208/1000 [30:12<2:09:59,  9.85s/it, lr=0.0001, step_loss=0.0344]\n",
      "Steps:  21%|██        | 209/1000 [30:21<2:05:47,  9.54s/it, lr=0.0001, step_loss=0.0344]\n",
      "Steps:  21%|██        | 209/1000 [30:21<2:05:47,  9.54s/it, lr=0.0001, step_loss=0.0182]\n",
      "Steps:  21%|██        | 210/1000 [30:30<2:03:51,  9.41s/it, lr=0.0001, step_loss=0.0182]\n",
      "Steps:  21%|██        | 210/1000 [30:30<2:03:51,  9.41s/it, lr=0.0001, step_loss=0.0219]\n",
      "Steps:  21%|██        | 211/1000 [30:39<2:01:03,  9.21s/it, lr=0.0001, step_loss=0.0219]\n",
      "Steps:  21%|██        | 211/1000 [30:39<2:01:03,  9.21s/it, lr=0.0001, step_loss=0.0156]\n",
      "Steps:  21%|██        | 212/1000 [30:48<2:00:04,  9.14s/it, lr=0.0001, step_loss=0.0156]\n",
      "Steps:  21%|██        | 212/1000 [30:48<2:00:04,  9.14s/it, lr=0.0001, step_loss=0.016]\n",
      "Steps:  21%|██▏       | 213/1000 [30:57<1:58:45,  9.05s/it, lr=0.0001, step_loss=0.016]\n",
      "Steps:  21%|██▏       | 213/1000 [30:57<1:58:45,  9.05s/it, lr=0.0001, step_loss=0.0664]\n",
      "Steps:  21%|██▏       | 214/1000 [31:06<1:58:49,  9.07s/it, lr=0.0001, step_loss=0.0664]\n",
      "Steps:  21%|██▏       | 214/1000 [31:06<1:58:49,  9.07s/it, lr=0.0001, step_loss=0.0292]\n",
      "Steps:  22%|██▏       | 215/1000 [31:14<1:57:17,  8.97s/it, lr=0.0001, step_loss=0.0292]\n",
      "Steps:  22%|██▏       | 215/1000 [31:14<1:57:17,  8.97s/it, lr=0.0001, step_loss=0.0164]\n",
      "Steps:  22%|██▏       | 216/1000 [31:23<1:57:18,  8.98s/it, lr=0.0001, step_loss=0.0164]\n",
      "Steps:  22%|██▏       | 216/1000 [31:23<1:57:18,  8.98s/it, lr=0.0001, step_loss=0.0288]\n",
      "Steps:  22%|██▏       | 217/1000 [31:32<1:56:25,  8.92s/it, lr=0.0001, step_loss=0.0288]\n",
      "Steps:  22%|██▏       | 217/1000 [31:32<1:56:25,  8.92s/it, lr=0.0001, step_loss=0.0184]\n",
      "Steps:  22%|██▏       | 218/1000 [31:41<1:57:04,  8.98s/it, lr=0.0001, step_loss=0.0184]\n",
      "Steps:  22%|██▏       | 218/1000 [31:41<1:57:04,  8.98s/it, lr=0.0001, step_loss=0.0305]\n",
      "Steps:  22%|██▏       | 219/1000 [31:50<1:55:57,  8.91s/it, lr=0.0001, step_loss=0.0305]\n",
      "Steps:  22%|██▏       | 219/1000 [31:50<1:55:57,  8.91s/it, lr=0.0001, step_loss=0.0514]\n",
      "Steps:  22%|██▏       | 220/1000 [31:59<1:56:13,  8.94s/it, lr=0.0001, step_loss=0.0514]\n",
      "Steps:  22%|██▏       | 220/1000 [31:59<1:56:13,  8.94s/it, lr=0.0001, step_loss=0.0227]\n",
      "Steps:  22%|██▏       | 221/1000 [32:08<1:55:34,  8.90s/it, lr=0.0001, step_loss=0.0227]\n",
      "Steps:  22%|██▏       | 221/1000 [32:08<1:55:34,  8.90s/it, lr=0.0001, step_loss=0.0156]\n",
      "Steps:  22%|██▏       | 222/1000 [32:17<1:56:17,  8.97s/it, lr=0.0001, step_loss=0.0156]\n",
      "Steps:  22%|██▏       | 222/1000 [32:17<1:56:17,  8.97s/it, lr=0.0001, step_loss=0.0245]\n",
      "Steps:  22%|██▏       | 223/1000 [32:26<1:55:10,  8.89s/it, lr=0.0001, step_loss=0.0245]\n",
      "Steps:  22%|██▏       | 223/1000 [32:26<1:55:10,  8.89s/it, lr=0.0001, step_loss=0.0463]\n",
      "Steps:  22%|██▏       | 224/1000 [32:35<1:55:26,  8.93s/it, lr=0.0001, step_loss=0.0463]\n",
      "Steps:  22%|██▏       | 224/1000 [32:35<1:55:26,  8.93s/it, lr=0.0001, step_loss=0.0381]\n",
      "Steps:  22%|██▎       | 225/1000 [32:44<1:54:51,  8.89s/it, lr=0.0001, step_loss=0.0381]\n",
      "Steps:  22%|██▎       | 225/1000 [32:44<1:54:51,  8.89s/it, lr=0.0001, step_loss=0.0239]\n",
      "Steps:  23%|██▎       | 226/1000 [32:53<1:55:32,  8.96s/it, lr=0.0001, step_loss=0.0239]\n",
      "Steps:  23%|██▎       | 226/1000 [32:53<1:55:32,  8.96s/it, lr=0.0001, step_loss=0.0154]\n",
      "Steps:  23%|██▎       | 227/1000 [33:01<1:54:32,  8.89s/it, lr=0.0001, step_loss=0.0154]\n",
      "Steps:  23%|██▎       | 227/1000 [33:01<1:54:32,  8.89s/it, lr=0.0001, step_loss=0.0736]\n",
      "Steps:  23%|██▎       | 228/1000 [33:10<1:54:48,  8.92s/it, lr=0.0001, step_loss=0.0736]\n",
      "Steps:  23%|██▎       | 228/1000 [33:10<1:54:48,  8.92s/it, lr=0.0001, step_loss=0.0381]\n",
      "Steps:  23%|██▎       | 229/1000 [33:19<1:54:17,  8.89s/it, lr=0.0001, step_loss=0.0381]\n",
      "Steps:  23%|██▎       | 229/1000 [33:19<1:54:17,  8.89s/it, lr=0.0001, step_loss=0.0208]\n",
      "Steps:  23%|██▎       | 230/1000 [33:28<1:54:57,  8.96s/it, lr=0.0001, step_loss=0.0208]\n",
      "Steps:  23%|██▎       | 230/1000 [33:28<1:54:57,  8.96s/it, lr=0.0001, step_loss=0.0155]\n",
      "Steps:  23%|██▎       | 231/1000 [33:37<1:53:52,  8.89s/it, lr=0.0001, step_loss=0.0155]\n",
      "Steps:  23%|██▎       | 231/1000 [33:37<1:53:52,  8.89s/it, lr=0.0001, step_loss=0.0136]\n",
      "Steps:  23%|██▎       | 232/1000 [33:46<1:54:11,  8.92s/it, lr=0.0001, step_loss=0.0136]\n",
      "Steps:  23%|██▎       | 232/1000 [33:46<1:54:11,  8.92s/it, lr=0.0001, step_loss=0.0314]\n",
      "Steps:  23%|██▎       | 233/1000 [33:55<1:53:48,  8.90s/it, lr=0.0001, step_loss=0.0314]\n",
      "Steps:  23%|██▎       | 233/1000 [33:55<1:53:48,  8.90s/it, lr=0.0001, step_loss=0.0247]\n",
      "Steps:  23%|██▎       | 234/1000 [34:04<1:54:34,  8.97s/it, lr=0.0001, step_loss=0.0247]\n",
      "Steps:  23%|██▎       | 234/1000 [34:04<1:54:34,  8.97s/it, lr=0.0001, step_loss=0.0306]\n",
      "Steps:  24%|██▎       | 235/1000 [34:13<1:53:23,  8.89s/it, lr=0.0001, step_loss=0.0306]\n",
      "Steps:  24%|██▎       | 235/1000 [34:13<1:53:23,  8.89s/it, lr=0.0001, step_loss=0.0355]\n",
      "Steps:  24%|██▎       | 236/1000 [34:22<1:53:42,  8.93s/it, lr=0.0001, step_loss=0.0355]\n",
      "Steps:  24%|██▎       | 236/1000 [34:22<1:53:42,  8.93s/it, lr=0.0001, step_loss=0.0243]\n",
      "Steps:  24%|██▎       | 237/1000 [34:31<1:53:13,  8.90s/it, lr=0.0001, step_loss=0.0243]\n",
      "Steps:  24%|██▎       | 237/1000 [34:31<1:53:13,  8.90s/it, lr=0.0001, step_loss=0.0101]\n",
      "Steps:  24%|██▍       | 238/1000 [34:40<1:53:51,  8.97s/it, lr=0.0001, step_loss=0.0101]\n",
      "Steps:  24%|██▍       | 238/1000 [34:40<1:53:51,  8.97s/it, lr=0.0001, step_loss=0.157]\n",
      "Steps:  24%|██▍       | 239/1000 [34:48<1:52:42,  8.89s/it, lr=0.0001, step_loss=0.157]\n",
      "Steps:  24%|██▍       | 239/1000 [34:48<1:52:42,  8.89s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  24%|██▍       | 240/1000 [34:57<1:52:59,  8.92s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  24%|██▍       | 240/1000 [34:57<1:52:59,  8.92s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  24%|██▍       | 241/1000 [35:06<1:52:29,  8.89s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  24%|██▍       | 241/1000 [35:06<1:52:29,  8.89s/it, lr=0.0001, step_loss=0.0337]\n",
      "Steps:  24%|██▍       | 242/1000 [35:15<1:53:13,  8.96s/it, lr=0.0001, step_loss=0.0337]\n",
      "Steps:  24%|██▍       | 242/1000 [35:15<1:53:13,  8.96s/it, lr=0.0001, step_loss=0.0115]\n",
      "Steps:  24%|██▍       | 243/1000 [35:24<1:52:05,  8.88s/it, lr=0.0001, step_loss=0.0115]\n",
      "Steps:  24%|██▍       | 243/1000 [35:24<1:52:05,  8.88s/it, lr=0.0001, step_loss=0.0432]\n",
      "Steps:  24%|██▍       | 244/1000 [35:33<1:52:23,  8.92s/it, lr=0.0001, step_loss=0.0432]\n",
      "Steps:  24%|██▍       | 244/1000 [35:33<1:52:23,  8.92s/it, lr=0.0001, step_loss=0.0182]\n",
      "Steps:  24%|██▍       | 245/1000 [35:42<1:51:53,  8.89s/it, lr=0.0001, step_loss=0.0182]\n",
      "Steps:  24%|██▍       | 245/1000 [35:42<1:51:53,  8.89s/it, lr=0.0001, step_loss=0.0109]\n",
      "Steps:  25%|██▍       | 246/1000 [35:51<1:52:40,  8.97s/it, lr=0.0001, step_loss=0.0109]\n",
      "Steps:  25%|██▍       | 246/1000 [35:51<1:52:40,  8.97s/it, lr=0.0001, step_loss=0.0109]\n",
      "Steps:  25%|██▍       | 247/1000 [36:00<1:51:34,  8.89s/it, lr=0.0001, step_loss=0.0109]\n",
      "Steps:  25%|██▍       | 247/1000 [36:00<1:51:34,  8.89s/it, lr=0.0001, step_loss=0.012]\n",
      "Steps:  25%|██▍       | 248/1000 [36:09<1:51:55,  8.93s/it, lr=0.0001, step_loss=0.012]\n",
      "Steps:  25%|██▍       | 248/1000 [36:09<1:51:55,  8.93s/it, lr=0.0001, step_loss=0.0182]\n",
      "Steps:  25%|██▍       | 249/1000 [36:18<1:51:26,  8.90s/it, lr=0.0001, step_loss=0.0182]\n",
      "Steps:  25%|██▍       | 249/1000 [36:18<1:51:26,  8.90s/it, lr=0.0001, step_loss=0.0211]\n",
      "Steps:  25%|██▌       | 250/1000 [36:27<1:52:07,  8.97s/it, lr=0.0001, step_loss=0.0211]\n",
      "Steps:  25%|██▌       | 250/1000 [36:27<1:52:07,  8.97s/it, lr=0.0001, step_loss=0.0296]\n",
      "Steps:  25%|██▌       | 251/1000 [36:35<1:50:58,  8.89s/it, lr=0.0001, step_loss=0.0296]\n",
      "Steps:  25%|██▌       | 251/1000 [36:35<1:50:58,  8.89s/it, lr=0.0001, step_loss=0.0115]\n",
      "Steps:  25%|██▌       | 252/1000 [36:44<1:51:14,  8.92s/it, lr=0.0001, step_loss=0.0115]\n",
      "Steps:  25%|██▌       | 252/1000 [36:44<1:51:14,  8.92s/it, lr=0.0001, step_loss=0.0207]\n",
      "Steps:  25%|██▌       | 253/1000 [36:53<1:50:44,  8.89s/it, lr=0.0001, step_loss=0.0207]\n",
      "Steps:  25%|██▌       | 253/1000 [36:53<1:50:44,  8.89s/it, lr=0.0001, step_loss=0.0186]\n",
      "Steps:  25%|██▌       | 254/1000 [37:02<1:51:26,  8.96s/it, lr=0.0001, step_loss=0.0186]\n",
      "Steps:  25%|██▌       | 254/1000 [37:02<1:51:26,  8.96s/it, lr=0.0001, step_loss=0.0375]\n",
      "Steps:  26%|██▌       | 255/1000 [37:11<1:50:26,  8.89s/it, lr=0.0001, step_loss=0.0375]\n",
      "Steps:  26%|██▌       | 255/1000 [37:11<1:50:26,  8.89s/it, lr=0.0001, step_loss=0.0138]\n",
      "Steps:  26%|██▌       | 256/1000 [37:20<1:50:44,  8.93s/it, lr=0.0001, step_loss=0.0138]\n",
      "Steps:  26%|██▌       | 256/1000 [37:20<1:50:44,  8.93s/it, lr=0.0001, step_loss=0.0403]\n",
      "Steps:  26%|██▌       | 257/1000 [37:29<1:50:09,  8.90s/it, lr=0.0001, step_loss=0.0403]\n",
      "Steps:  26%|██▌       | 257/1000 [37:29<1:50:09,  8.90s/it, lr=0.0001, step_loss=0.0517]\n",
      "Steps:  26%|██▌       | 258/1000 [37:38<1:50:51,  8.96s/it, lr=0.0001, step_loss=0.0517]\n",
      "Steps:  26%|██▌       | 258/1000 [37:38<1:50:51,  8.96s/it, lr=0.0001, step_loss=0.0261]\n",
      "Steps:  26%|██▌       | 259/1000 [37:47<1:49:47,  8.89s/it, lr=0.0001, step_loss=0.0261]\n",
      "Steps:  26%|██▌       | 259/1000 [37:47<1:49:47,  8.89s/it, lr=0.0001, step_loss=0.0361]\n",
      "Steps:  26%|██▌       | 260/1000 [37:56<1:50:07,  8.93s/it, lr=0.0001, step_loss=0.0361]\n",
      "Steps:  26%|██▌       | 260/1000 [37:56<1:50:07,  8.93s/it, lr=0.0001, step_loss=0.0177]\n",
      "Steps:  26%|██▌       | 261/1000 [38:05<1:49:31,  8.89s/it, lr=0.0001, step_loss=0.0177]\n",
      "Steps:  26%|██▌       | 261/1000 [38:05<1:49:31,  8.89s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  26%|██▌       | 262/1000 [38:14<1:50:10,  8.96s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  26%|██▌       | 262/1000 [38:14<1:50:10,  8.96s/it, lr=0.0001, step_loss=0.0549]\n",
      "Steps:  26%|██▋       | 263/1000 [38:22<1:49:09,  8.89s/it, lr=0.0001, step_loss=0.0549]\n",
      "Steps:  26%|██▋       | 263/1000 [38:22<1:49:09,  8.89s/it, lr=0.0001, step_loss=0.0546]\n",
      "Steps:  26%|██▋       | 264/1000 [38:31<1:49:25,  8.92s/it, lr=0.0001, step_loss=0.0546]\n",
      "Steps:  26%|██▋       | 264/1000 [38:31<1:49:25,  8.92s/it, lr=0.0001, step_loss=0.0339]\n",
      "Steps:  26%|██▋       | 265/1000 [38:40<1:48:59,  8.90s/it, lr=0.0001, step_loss=0.0339]\n",
      "Steps:  26%|██▋       | 265/1000 [38:40<1:48:59,  8.90s/it, lr=0.0001, step_loss=0.0234]\n",
      "Steps:  27%|██▋       | 266/1000 [38:49<1:49:40,  8.97s/it, lr=0.0001, step_loss=0.0234]\n",
      "Steps:  27%|██▋       | 266/1000 [38:49<1:49:40,  8.97s/it, lr=0.0001, step_loss=0.00976]\n",
      "Steps:  27%|██▋       | 267/1000 [38:58<1:48:40,  8.90s/it, lr=0.0001, step_loss=0.00976]\n",
      "Steps:  27%|██▋       | 267/1000 [38:58<1:48:40,  8.90s/it, lr=0.0001, step_loss=0.0232]\n",
      "Steps:  27%|██▋       | 268/1000 [39:07<1:48:52,  8.92s/it, lr=0.0001, step_loss=0.0232]\n",
      "Steps:  27%|██▋       | 268/1000 [39:07<1:48:52,  8.92s/it, lr=0.0001, step_loss=0.0369]\n",
      "Steps:  27%|██▋       | 269/1000 [39:16<1:48:21,  8.89s/it, lr=0.0001, step_loss=0.0369]\n",
      "Steps:  27%|██▋       | 269/1000 [39:16<1:48:21,  8.89s/it, lr=0.0001, step_loss=0.0167]\n",
      "Steps:  27%|██▋       | 270/1000 [39:25<1:49:03,  8.96s/it, lr=0.0001, step_loss=0.0167]\n",
      "Steps:  27%|██▋       | 270/1000 [39:25<1:49:03,  8.96s/it, lr=0.0001, step_loss=0.088]\n",
      "Steps:  27%|██▋       | 271/1000 [39:34<1:47:57,  8.89s/it, lr=0.0001, step_loss=0.088]\n",
      "Steps:  27%|██▋       | 271/1000 [39:34<1:47:57,  8.89s/it, lr=0.0001, step_loss=0.138]\n",
      "Steps:  27%|██▋       | 272/1000 [39:43<1:48:13,  8.92s/it, lr=0.0001, step_loss=0.138]\n",
      "Steps:  27%|██▋       | 272/1000 [39:43<1:48:13,  8.92s/it, lr=0.0001, step_loss=0.011]\n",
      "Steps:  27%|██▋       | 273/1000 [39:52<1:47:44,  8.89s/it, lr=0.0001, step_loss=0.011]\n",
      "Steps:  27%|██▋       | 273/1000 [39:52<1:47:44,  8.89s/it, lr=0.0001, step_loss=0.0197]\n",
      "Steps:  27%|██▋       | 274/1000 [40:01<1:48:24,  8.96s/it, lr=0.0001, step_loss=0.0197]\n",
      "Steps:  27%|██▋       | 274/1000 [40:01<1:48:24,  8.96s/it, lr=0.0001, step_loss=0.0146]\n",
      "Steps:  28%|██▊       | 275/1000 [40:09<1:47:25,  8.89s/it, lr=0.0001, step_loss=0.0146]\n",
      "Steps:  28%|██▊       | 275/1000 [40:09<1:47:25,  8.89s/it, lr=0.0001, step_loss=0.0511]\n",
      "Steps:  28%|██▊       | 276/1000 [40:18<1:47:40,  8.92s/it, lr=0.0001, step_loss=0.0511]\n",
      "Steps:  28%|██▊       | 276/1000 [40:18<1:47:40,  8.92s/it, lr=0.0001, step_loss=0.172]\n",
      "Steps:  28%|██▊       | 277/1000 [40:27<1:47:10,  8.89s/it, lr=0.0001, step_loss=0.172]\n",
      "Steps:  28%|██▊       | 277/1000 [40:27<1:47:10,  8.89s/it, lr=0.0001, step_loss=0.0496]\n",
      "Steps:  28%|██▊       | 278/1000 [40:36<1:47:51,  8.96s/it, lr=0.0001, step_loss=0.0496]\n",
      "Steps:  28%|██▊       | 278/1000 [40:36<1:47:51,  8.96s/it, lr=0.0001, step_loss=0.0235]\n",
      "Steps:  28%|██▊       | 279/1000 [40:45<1:46:46,  8.89s/it, lr=0.0001, step_loss=0.0235]\n",
      "Steps:  28%|██▊       | 279/1000 [40:45<1:46:46,  8.89s/it, lr=0.0001, step_loss=0.00909]\n",
      "Steps:  28%|██▊       | 280/1000 [40:54<1:47:05,  8.92s/it, lr=0.0001, step_loss=0.00909]\n",
      "Steps:  28%|██▊       | 280/1000 [40:54<1:47:05,  8.92s/it, lr=0.0001, step_loss=0.0158]\n",
      "Steps:  28%|██▊       | 281/1000 [41:03<1:46:35,  8.90s/it, lr=0.0001, step_loss=0.0158]\n",
      "Steps:  28%|██▊       | 281/1000 [41:03<1:46:35,  8.90s/it, lr=0.0001, step_loss=0.0698]\n",
      "Steps:  28%|██▊       | 282/1000 [41:12<1:47:13,  8.96s/it, lr=0.0001, step_loss=0.0698]\n",
      "Steps:  28%|██▊       | 282/1000 [41:12<1:47:13,  8.96s/it, lr=0.0001, step_loss=0.0514]\n",
      "Steps:  28%|██▊       | 283/1000 [41:21<1:46:09,  8.88s/it, lr=0.0001, step_loss=0.0514]\n",
      "Steps:  28%|██▊       | 283/1000 [41:21<1:46:09,  8.88s/it, lr=0.0001, step_loss=0.0444]\n",
      "Steps:  28%|██▊       | 284/1000 [41:30<1:46:24,  8.92s/it, lr=0.0001, step_loss=0.0444]\n",
      "Steps:  28%|██▊       | 284/1000 [41:30<1:46:24,  8.92s/it, lr=0.0001, step_loss=0.0137]\n",
      "Steps:  28%|██▊       | 285/1000 [41:39<1:46:01,  8.90s/it, lr=0.0001, step_loss=0.0137]\n",
      "Steps:  28%|██▊       | 285/1000 [41:39<1:46:01,  8.90s/it, lr=0.0001, step_loss=0.0185]\n",
      "Steps:  29%|██▊       | 286/1000 [41:48<1:46:36,  8.96s/it, lr=0.0001, step_loss=0.0185]\n",
      "Steps:  29%|██▊       | 286/1000 [41:48<1:46:36,  8.96s/it, lr=0.0001, step_loss=0.0797]\n",
      "Steps:  29%|██▊       | 287/1000 [41:56<1:45:36,  8.89s/it, lr=0.0001, step_loss=0.0797]\n",
      "Steps:  29%|██▊       | 287/1000 [41:56<1:45:36,  8.89s/it, lr=0.0001, step_loss=0.0239]\n",
      "Steps:  29%|██▉       | 288/1000 [42:05<1:45:51,  8.92s/it, lr=0.0001, step_loss=0.0239]\n",
      "Steps:  29%|██▉       | 288/1000 [42:05<1:45:51,  8.92s/it, lr=0.0001, step_loss=0.0329]\n",
      "Steps:  29%|██▉       | 289/1000 [42:14<1:45:19,  8.89s/it, lr=0.0001, step_loss=0.0329]\n",
      "Steps:  29%|██▉       | 289/1000 [42:14<1:45:19,  8.89s/it, lr=0.0001, step_loss=0.0229]\n",
      "Steps:  29%|██▉       | 290/1000 [42:23<1:46:01,  8.96s/it, lr=0.0001, step_loss=0.0229]\n",
      "Steps:  29%|██▉       | 290/1000 [42:23<1:46:01,  8.96s/it, lr=0.0001, step_loss=0.0481]\n",
      "Steps:  29%|██▉       | 291/1000 [42:32<1:44:57,  8.88s/it, lr=0.0001, step_loss=0.0481]\n",
      "Steps:  29%|██▉       | 291/1000 [42:32<1:44:57,  8.88s/it, lr=0.0001, step_loss=0.0421]\n",
      "Steps:  29%|██▉       | 292/1000 [42:41<1:45:13,  8.92s/it, lr=0.0001, step_loss=0.0421]\n",
      "Steps:  29%|██▉       | 292/1000 [42:41<1:45:13,  8.92s/it, lr=0.0001, step_loss=0.0133]\n",
      "Steps:  29%|██▉       | 293/1000 [42:50<1:44:45,  8.89s/it, lr=0.0001, step_loss=0.0133]\n",
      "Steps:  29%|██▉       | 293/1000 [42:50<1:44:45,  8.89s/it, lr=0.0001, step_loss=0.0114]\n",
      "Steps:  29%|██▉       | 294/1000 [42:59<1:45:23,  8.96s/it, lr=0.0001, step_loss=0.0114]\n",
      "Steps:  29%|██▉       | 294/1000 [42:59<1:45:23,  8.96s/it, lr=0.0001, step_loss=0.0167]\n",
      "Steps:  30%|██▉       | 295/1000 [43:08<1:44:24,  8.89s/it, lr=0.0001, step_loss=0.0167]\n",
      "Steps:  30%|██▉       | 295/1000 [43:08<1:44:24,  8.89s/it, lr=0.0001, step_loss=0.0115]\n",
      "Steps:  30%|██▉       | 296/1000 [43:18<1:48:10,  9.22s/it, lr=0.0001, step_loss=0.0115]\n",
      "Steps:  30%|██▉       | 296/1000 [43:18<1:48:10,  9.22s/it, lr=0.0001, step_loss=0.0195]\n",
      "Steps:  30%|██▉       | 297/1000 [43:27<1:47:53,  9.21s/it, lr=0.0001, step_loss=0.0195]\n",
      "Steps:  30%|██▉       | 297/1000 [43:27<1:47:53,  9.21s/it, lr=0.0001, step_loss=0.0239]\n",
      "Steps:  30%|██▉       | 298/1000 [43:36<1:47:28,  9.19s/it, lr=0.0001, step_loss=0.0239]\n",
      "Steps:  30%|██▉       | 298/1000 [43:36<1:47:28,  9.19s/it, lr=0.0001, step_loss=0.0145]\n",
      "Steps:  30%|██▉       | 299/1000 [43:46<1:49:24,  9.36s/it, lr=0.0001, step_loss=0.0145]\n",
      "Steps:  30%|██▉       | 299/1000 [43:46<1:49:24,  9.36s/it, lr=0.0001, step_loss=0.0122]\n",
      "Steps:  30%|███       | 300/1000 [43:55<1:49:10,  9.36s/it, lr=0.0001, step_loss=0.0122]04/17/2024 04:39:01 - INFO - accelerate.accelerator - Saving current state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-300\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-300\\unet_ema\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-300/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-300\\unet\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-300/unet/diffusion_pytorch_model.safetensors\n",
      "04/17/2024 04:39:37 - INFO - accelerate.checkpointing - Optimizer state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-300\\optimizer.bin\n",
      "04/17/2024 04:39:37 - INFO - accelerate.checkpointing - Scheduler state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-300\\scheduler.bin\n",
      "04/17/2024 04:39:37 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-300\\sampler.bin\n",
      "04/17/2024 04:39:37 - INFO - accelerate.checkpointing - Random states saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-300\\random_states_0.pkl\n",
      "04/17/2024 04:39:37 - INFO - __main__ - Saved state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-300\n",
      "\n",
      "Steps:  30%|███       | 300/1000 [44:31<1:49:10,  9.36s/it, lr=0.0001, step_loss=0.0172]\n",
      "Steps:  30%|███       | 301/1000 [44:41<3:58:02, 20.43s/it, lr=0.0001, step_loss=0.0172]\n",
      "Steps:  30%|███       | 301/1000 [44:41<3:58:02, 20.43s/it, lr=0.0001, step_loss=0.0166]\n",
      "Steps:  30%|███       | 302/1000 [44:51<3:18:16, 17.04s/it, lr=0.0001, step_loss=0.0166]\n",
      "Steps:  30%|███       | 302/1000 [44:51<3:18:16, 17.04s/it, lr=0.0001, step_loss=0.0378]\n",
      "Steps:  30%|███       | 303/1000 [45:00<2:52:38, 14.86s/it, lr=0.0001, step_loss=0.0378]\n",
      "Steps:  30%|███       | 303/1000 [45:00<2:52:38, 14.86s/it, lr=0.0001, step_loss=0.025]\n",
      "Steps:  30%|███       | 304/1000 [45:10<2:33:02, 13.19s/it, lr=0.0001, step_loss=0.025]\n",
      "Steps:  30%|███       | 304/1000 [45:10<2:33:02, 13.19s/it, lr=0.0001, step_loss=0.0126]\n",
      "Steps:  30%|███       | 305/1000 [45:18<2:17:39, 11.88s/it, lr=0.0001, step_loss=0.0126]\n",
      "Steps:  30%|███       | 305/1000 [45:18<2:17:39, 11.88s/it, lr=0.0001, step_loss=0.0177]\n",
      "Steps:  31%|███       | 306/1000 [45:28<2:07:49, 11.05s/it, lr=0.0001, step_loss=0.0177]\n",
      "Steps:  31%|███       | 306/1000 [45:28<2:07:49, 11.05s/it, lr=0.0001, step_loss=0.015]\n",
      "Steps:  31%|███       | 307/1000 [45:36<1:59:37, 10.36s/it, lr=0.0001, step_loss=0.015]\n",
      "Steps:  31%|███       | 307/1000 [45:36<1:59:37, 10.36s/it, lr=0.0001, step_loss=0.0137]\n",
      "Steps:  31%|███       | 308/1000 [45:45<1:54:45,  9.95s/it, lr=0.0001, step_loss=0.0137]\n",
      "Steps:  31%|███       | 308/1000 [45:45<1:54:45,  9.95s/it, lr=0.0001, step_loss=0.0172]\n",
      "Steps:  31%|███       | 309/1000 [45:54<1:50:35,  9.60s/it, lr=0.0001, step_loss=0.0172]\n",
      "Steps:  31%|███       | 309/1000 [45:54<1:50:35,  9.60s/it, lr=0.0001, step_loss=0.0205]\n",
      "Steps:  31%|███       | 310/1000 [46:03<1:48:47,  9.46s/it, lr=0.0001, step_loss=0.0205]\n",
      "Steps:  31%|███       | 310/1000 [46:03<1:48:47,  9.46s/it, lr=0.0001, step_loss=0.031]\n",
      "Steps:  31%|███       | 311/1000 [46:12<1:46:11,  9.25s/it, lr=0.0001, step_loss=0.031]\n",
      "Steps:  31%|███       | 311/1000 [46:12<1:46:11,  9.25s/it, lr=0.0001, step_loss=0.0123]\n",
      "Steps:  31%|███       | 312/1000 [46:21<1:45:14,  9.18s/it, lr=0.0001, step_loss=0.0123]\n",
      "Steps:  31%|███       | 312/1000 [46:21<1:45:14,  9.18s/it, lr=0.0001, step_loss=0.0285]\n",
      "Steps:  31%|███▏      | 313/1000 [46:30<1:43:43,  9.06s/it, lr=0.0001, step_loss=0.0285]\n",
      "Steps:  31%|███▏      | 313/1000 [46:30<1:43:43,  9.06s/it, lr=0.0001, step_loss=0.0213]\n",
      "Steps:  31%|███▏      | 314/1000 [46:39<1:43:48,  9.08s/it, lr=0.0001, step_loss=0.0213]\n",
      "Steps:  31%|███▏      | 314/1000 [46:39<1:43:48,  9.08s/it, lr=0.0001, step_loss=0.0146]\n",
      "Steps:  32%|███▏      | 315/1000 [46:48<1:42:28,  8.98s/it, lr=0.0001, step_loss=0.0146]\n",
      "Steps:  32%|███▏      | 315/1000 [46:48<1:42:28,  8.98s/it, lr=0.0001, step_loss=0.143]\n",
      "Steps:  32%|███▏      | 316/1000 [46:57<1:42:24,  8.98s/it, lr=0.0001, step_loss=0.143]\n",
      "Steps:  32%|███▏      | 316/1000 [46:57<1:42:24,  8.98s/it, lr=0.0001, step_loss=0.0354]\n",
      "Steps:  32%|███▏      | 317/1000 [47:05<1:41:44,  8.94s/it, lr=0.0001, step_loss=0.0354]\n",
      "Steps:  32%|███▏      | 317/1000 [47:05<1:41:44,  8.94s/it, lr=0.0001, step_loss=0.177]\n",
      "Steps:  32%|███▏      | 318/1000 [47:15<1:42:10,  8.99s/it, lr=0.0001, step_loss=0.177]\n",
      "Steps:  32%|███▏      | 318/1000 [47:15<1:42:10,  8.99s/it, lr=0.0001, step_loss=0.0232]\n",
      "Steps:  32%|███▏      | 319/1000 [47:23<1:41:06,  8.91s/it, lr=0.0001, step_loss=0.0232]\n",
      "Steps:  32%|███▏      | 319/1000 [47:23<1:41:06,  8.91s/it, lr=0.0001, step_loss=0.00955]\n",
      "Steps:  32%|███▏      | 320/1000 [47:32<1:41:16,  8.94s/it, lr=0.0001, step_loss=0.00955]\n",
      "Steps:  32%|███▏      | 320/1000 [47:32<1:41:16,  8.94s/it, lr=0.0001, step_loss=0.0104]\n",
      "Steps:  32%|███▏      | 321/1000 [47:41<1:40:39,  8.89s/it, lr=0.0001, step_loss=0.0104]\n",
      "Steps:  32%|███▏      | 321/1000 [47:41<1:40:39,  8.89s/it, lr=0.0001, step_loss=0.067]\n",
      "Steps:  32%|███▏      | 322/1000 [47:50<1:41:20,  8.97s/it, lr=0.0001, step_loss=0.067]\n",
      "Steps:  32%|███▏      | 322/1000 [47:50<1:41:20,  8.97s/it, lr=0.0001, step_loss=0.0121]\n",
      "Steps:  32%|███▏      | 323/1000 [47:59<1:40:20,  8.89s/it, lr=0.0001, step_loss=0.0121]\n",
      "Steps:  32%|███▏      | 323/1000 [47:59<1:40:20,  8.89s/it, lr=0.0001, step_loss=0.0266]\n",
      "Steps:  32%|███▏      | 324/1000 [48:08<1:40:33,  8.93s/it, lr=0.0001, step_loss=0.0266]\n",
      "Steps:  32%|███▏      | 324/1000 [48:08<1:40:33,  8.93s/it, lr=0.0001, step_loss=0.0158]\n",
      "Steps:  32%|███▎      | 325/1000 [48:17<1:40:07,  8.90s/it, lr=0.0001, step_loss=0.0158]\n",
      "Steps:  32%|███▎      | 325/1000 [48:17<1:40:07,  8.90s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  33%|███▎      | 326/1000 [48:26<1:40:38,  8.96s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  33%|███▎      | 326/1000 [48:26<1:40:38,  8.96s/it, lr=0.0001, step_loss=0.0245]\n",
      "Steps:  33%|███▎      | 327/1000 [48:35<1:39:44,  8.89s/it, lr=0.0001, step_loss=0.0245]\n",
      "Steps:  33%|███▎      | 327/1000 [48:35<1:39:44,  8.89s/it, lr=0.0001, step_loss=0.0285]\n",
      "Steps:  33%|███▎      | 328/1000 [48:44<1:39:57,  8.92s/it, lr=0.0001, step_loss=0.0285]\n",
      "Steps:  33%|███▎      | 328/1000 [48:44<1:39:57,  8.92s/it, lr=0.0001, step_loss=0.0102]\n",
      "Steps:  33%|███▎      | 329/1000 [48:52<1:39:19,  8.88s/it, lr=0.0001, step_loss=0.0102]\n",
      "Steps:  33%|███▎      | 329/1000 [48:52<1:39:19,  8.88s/it, lr=0.0001, step_loss=0.00599]\n",
      "Steps:  33%|███▎      | 330/1000 [49:02<1:39:59,  8.96s/it, lr=0.0001, step_loss=0.00599]\n",
      "Steps:  33%|███▎      | 330/1000 [49:02<1:39:59,  8.96s/it, lr=0.0001, step_loss=0.0262]\n",
      "Steps:  33%|███▎      | 331/1000 [49:10<1:39:03,  8.88s/it, lr=0.0001, step_loss=0.0262]\n",
      "Steps:  33%|███▎      | 331/1000 [49:10<1:39:03,  8.88s/it, lr=0.0001, step_loss=0.0108]\n",
      "Steps:  33%|███▎      | 332/1000 [49:19<1:39:17,  8.92s/it, lr=0.0001, step_loss=0.0108]\n",
      "Steps:  33%|███▎      | 332/1000 [49:19<1:39:17,  8.92s/it, lr=0.0001, step_loss=0.0166]\n",
      "Steps:  33%|███▎      | 333/1000 [49:28<1:38:51,  8.89s/it, lr=0.0001, step_loss=0.0166]\n",
      "Steps:  33%|███▎      | 333/1000 [49:28<1:38:51,  8.89s/it, lr=0.0001, step_loss=0.0161]\n",
      "Steps:  33%|███▎      | 334/1000 [49:37<1:39:31,  8.97s/it, lr=0.0001, step_loss=0.0161]\n",
      "Steps:  33%|███▎      | 334/1000 [49:37<1:39:31,  8.97s/it, lr=0.0001, step_loss=0.136]\n",
      "Steps:  34%|███▎      | 335/1000 [49:46<1:38:27,  8.88s/it, lr=0.0001, step_loss=0.136]\n",
      "Steps:  34%|███▎      | 335/1000 [49:46<1:38:27,  8.88s/it, lr=0.0001, step_loss=0.00787]\n",
      "Steps:  34%|███▎      | 336/1000 [49:55<1:38:41,  8.92s/it, lr=0.0001, step_loss=0.00787]\n",
      "Steps:  34%|███▎      | 336/1000 [49:55<1:38:41,  8.92s/it, lr=0.0001, step_loss=0.0374]\n",
      "Steps:  34%|███▎      | 337/1000 [50:04<1:38:12,  8.89s/it, lr=0.0001, step_loss=0.0374]\n",
      "Steps:  34%|███▎      | 337/1000 [50:04<1:38:12,  8.89s/it, lr=0.0001, step_loss=0.0142]\n",
      "Steps:  34%|███▍      | 338/1000 [50:13<1:38:44,  8.95s/it, lr=0.0001, step_loss=0.0142]\n",
      "Steps:  34%|███▍      | 338/1000 [50:13<1:38:44,  8.95s/it, lr=0.0001, step_loss=0.013]\n",
      "Steps:  34%|███▍      | 339/1000 [50:22<1:37:52,  8.88s/it, lr=0.0001, step_loss=0.013]\n",
      "Steps:  34%|███▍      | 339/1000 [50:22<1:37:52,  8.88s/it, lr=0.0001, step_loss=0.0168]\n",
      "Steps:  34%|███▍      | 340/1000 [50:31<1:38:07,  8.92s/it, lr=0.0001, step_loss=0.0168]\n",
      "Steps:  34%|███▍      | 340/1000 [50:31<1:38:07,  8.92s/it, lr=0.0001, step_loss=0.0133]\n",
      "Steps:  34%|███▍      | 341/1000 [50:39<1:37:42,  8.90s/it, lr=0.0001, step_loss=0.0133]\n",
      "Steps:  34%|███▍      | 341/1000 [50:39<1:37:42,  8.90s/it, lr=0.0001, step_loss=0.0158]\n",
      "Steps:  34%|███▍      | 342/1000 [50:49<1:38:16,  8.96s/it, lr=0.0001, step_loss=0.0158]\n",
      "Steps:  34%|███▍      | 342/1000 [50:49<1:38:16,  8.96s/it, lr=0.0001, step_loss=0.132]\n",
      "Steps:  34%|███▍      | 343/1000 [50:57<1:37:18,  8.89s/it, lr=0.0001, step_loss=0.132]\n",
      "Steps:  34%|███▍      | 343/1000 [50:57<1:37:18,  8.89s/it, lr=0.0001, step_loss=0.0082]\n",
      "Steps:  34%|███▍      | 344/1000 [51:06<1:37:32,  8.92s/it, lr=0.0001, step_loss=0.0082]\n",
      "Steps:  34%|███▍      | 344/1000 [51:06<1:37:32,  8.92s/it, lr=0.0001, step_loss=0.023]\n",
      "Steps:  34%|███▍      | 345/1000 [51:15<1:36:58,  8.88s/it, lr=0.0001, step_loss=0.023]\n",
      "Steps:  34%|███▍      | 345/1000 [51:15<1:36:58,  8.88s/it, lr=0.0001, step_loss=0.0139]\n",
      "Steps:  35%|███▍      | 346/1000 [51:24<1:37:34,  8.95s/it, lr=0.0001, step_loss=0.0139]\n",
      "Steps:  35%|███▍      | 346/1000 [51:24<1:37:34,  8.95s/it, lr=0.0001, step_loss=0.0083]\n",
      "Steps:  35%|███▍      | 347/1000 [51:33<1:36:43,  8.89s/it, lr=0.0001, step_loss=0.0083]\n",
      "Steps:  35%|███▍      | 347/1000 [51:33<1:36:43,  8.89s/it, lr=0.0001, step_loss=0.00795]\n",
      "Steps:  35%|███▍      | 348/1000 [51:42<1:36:58,  8.92s/it, lr=0.0001, step_loss=0.00795]\n",
      "Steps:  35%|███▍      | 348/1000 [51:42<1:36:58,  8.92s/it, lr=0.0001, step_loss=0.0197]\n",
      "Steps:  35%|███▍      | 349/1000 [51:51<1:36:29,  8.89s/it, lr=0.0001, step_loss=0.0197]\n",
      "Steps:  35%|███▍      | 349/1000 [51:51<1:36:29,  8.89s/it, lr=0.0001, step_loss=0.0127]\n",
      "Steps:  35%|███▌      | 350/1000 [52:00<1:37:04,  8.96s/it, lr=0.0001, step_loss=0.0127]\n",
      "Steps:  35%|███▌      | 350/1000 [52:00<1:37:04,  8.96s/it, lr=0.0001, step_loss=0.00764]\n",
      "Steps:  35%|███▌      | 351/1000 [52:09<1:36:09,  8.89s/it, lr=0.0001, step_loss=0.00764]\n",
      "Steps:  35%|███▌      | 351/1000 [52:09<1:36:09,  8.89s/it, lr=0.0001, step_loss=0.00549]\n",
      "Steps:  35%|███▌      | 352/1000 [52:18<1:36:30,  8.94s/it, lr=0.0001, step_loss=0.00549]\n",
      "Steps:  35%|███▌      | 352/1000 [52:18<1:36:30,  8.94s/it, lr=0.0001, step_loss=0.0184]\n",
      "Steps:  35%|███▌      | 353/1000 [52:26<1:35:58,  8.90s/it, lr=0.0001, step_loss=0.0184]\n",
      "Steps:  35%|███▌      | 353/1000 [52:26<1:35:58,  8.90s/it, lr=0.0001, step_loss=0.0101]\n",
      "Steps:  35%|███▌      | 354/1000 [52:36<1:36:32,  8.97s/it, lr=0.0001, step_loss=0.0101]\n",
      "Steps:  35%|███▌      | 354/1000 [52:36<1:36:32,  8.97s/it, lr=0.0001, step_loss=0.0195]\n",
      "Steps:  36%|███▌      | 355/1000 [52:44<1:35:33,  8.89s/it, lr=0.0001, step_loss=0.0195]\n",
      "Steps:  36%|███▌      | 355/1000 [52:44<1:35:33,  8.89s/it, lr=0.0001, step_loss=0.0166]\n",
      "Steps:  36%|███▌      | 356/1000 [52:53<1:35:49,  8.93s/it, lr=0.0001, step_loss=0.0166]\n",
      "Steps:  36%|███▌      | 356/1000 [52:53<1:35:49,  8.93s/it, lr=0.0001, step_loss=0.00529]\n",
      "Steps:  36%|███▌      | 357/1000 [53:02<1:35:10,  8.88s/it, lr=0.0001, step_loss=0.00529]\n",
      "Steps:  36%|███▌      | 357/1000 [53:02<1:35:10,  8.88s/it, lr=0.0001, step_loss=0.0737]\n",
      "Steps:  36%|███▌      | 358/1000 [53:11<1:35:50,  8.96s/it, lr=0.0001, step_loss=0.0737]\n",
      "Steps:  36%|███▌      | 358/1000 [53:11<1:35:50,  8.96s/it, lr=0.0001, step_loss=0.014]\n",
      "Steps:  36%|███▌      | 359/1000 [53:20<1:34:57,  8.89s/it, lr=0.0001, step_loss=0.014]\n",
      "Steps:  36%|███▌      | 359/1000 [53:20<1:34:57,  8.89s/it, lr=0.0001, step_loss=0.0154]\n",
      "Steps:  36%|███▌      | 360/1000 [53:29<1:35:13,  8.93s/it, lr=0.0001, step_loss=0.0154]\n",
      "Steps:  36%|███▌      | 360/1000 [53:29<1:35:13,  8.93s/it, lr=0.0001, step_loss=0.0132]\n",
      "Steps:  36%|███▌      | 361/1000 [53:38<1:34:45,  8.90s/it, lr=0.0001, step_loss=0.0132]\n",
      "Steps:  36%|███▌      | 361/1000 [53:38<1:34:45,  8.90s/it, lr=0.0001, step_loss=0.0246]\n",
      "Steps:  36%|███▌      | 362/1000 [53:47<1:35:20,  8.97s/it, lr=0.0001, step_loss=0.0246]\n",
      "Steps:  36%|███▌      | 362/1000 [53:47<1:35:20,  8.97s/it, lr=0.0001, step_loss=0.00605]\n",
      "Steps:  36%|███▋      | 363/1000 [53:56<1:34:23,  8.89s/it, lr=0.0001, step_loss=0.00605]\n",
      "Steps:  36%|███▋      | 363/1000 [53:56<1:34:23,  8.89s/it, lr=0.0001, step_loss=0.0165]\n",
      "Steps:  36%|███▋      | 364/1000 [54:05<1:34:39,  8.93s/it, lr=0.0001, step_loss=0.0165]\n",
      "Steps:  36%|███▋      | 364/1000 [54:05<1:34:39,  8.93s/it, lr=0.0001, step_loss=0.00889]\n",
      "Steps:  36%|███▋      | 365/1000 [54:13<1:34:05,  8.89s/it, lr=0.0001, step_loss=0.00889]\n",
      "Steps:  36%|███▋      | 365/1000 [54:13<1:34:05,  8.89s/it, lr=0.0001, step_loss=0.00497]\n",
      "Steps:  37%|███▋      | 366/1000 [54:23<1:34:39,  8.96s/it, lr=0.0001, step_loss=0.00497]\n",
      "Steps:  37%|███▋      | 366/1000 [54:23<1:34:39,  8.96s/it, lr=0.0001, step_loss=0.0139]\n",
      "Steps:  37%|███▋      | 367/1000 [54:31<1:33:40,  8.88s/it, lr=0.0001, step_loss=0.0139]\n",
      "Steps:  37%|███▋      | 367/1000 [54:31<1:33:40,  8.88s/it, lr=0.0001, step_loss=0.0251]\n",
      "Steps:  37%|███▋      | 368/1000 [54:40<1:33:56,  8.92s/it, lr=0.0001, step_loss=0.0251]\n",
      "Steps:  37%|███▋      | 368/1000 [54:40<1:33:56,  8.92s/it, lr=0.0001, step_loss=0.0151]\n",
      "Steps:  37%|███▋      | 369/1000 [54:49<1:33:29,  8.89s/it, lr=0.0001, step_loss=0.0151]\n",
      "Steps:  37%|███▋      | 369/1000 [54:49<1:33:29,  8.89s/it, lr=0.0001, step_loss=0.0105]\n",
      "Steps:  37%|███▋      | 370/1000 [54:58<1:34:03,  8.96s/it, lr=0.0001, step_loss=0.0105]\n",
      "Steps:  37%|███▋      | 370/1000 [54:58<1:34:03,  8.96s/it, lr=0.0001, step_loss=0.0426]\n",
      "Steps:  37%|███▋      | 371/1000 [55:07<1:33:10,  8.89s/it, lr=0.0001, step_loss=0.0426]\n",
      "Steps:  37%|███▋      | 371/1000 [55:07<1:33:10,  8.89s/it, lr=0.0001, step_loss=0.0105]\n",
      "Steps:  37%|███▋      | 372/1000 [55:16<1:33:24,  8.92s/it, lr=0.0001, step_loss=0.0105]\n",
      "Steps:  37%|███▋      | 372/1000 [55:16<1:33:24,  8.92s/it, lr=0.0001, step_loss=0.00841]\n",
      "Steps:  37%|███▋      | 373/1000 [55:25<1:32:58,  8.90s/it, lr=0.0001, step_loss=0.00841]\n",
      "Steps:  37%|███▋      | 373/1000 [55:25<1:32:58,  8.90s/it, lr=0.0001, step_loss=0.032]\n",
      "Steps:  37%|███▋      | 374/1000 [55:34<1:33:28,  8.96s/it, lr=0.0001, step_loss=0.032]\n",
      "Steps:  37%|███▋      | 374/1000 [55:34<1:33:28,  8.96s/it, lr=0.0001, step_loss=0.0132]\n",
      "Steps:  38%|███▊      | 375/1000 [55:43<1:32:34,  8.89s/it, lr=0.0001, step_loss=0.0132]\n",
      "Steps:  38%|███▊      | 375/1000 [55:43<1:32:34,  8.89s/it, lr=0.0001, step_loss=0.0041]\n",
      "Steps:  38%|███▊      | 376/1000 [55:52<1:32:47,  8.92s/it, lr=0.0001, step_loss=0.0041]\n",
      "Steps:  38%|███▊      | 376/1000 [55:52<1:32:47,  8.92s/it, lr=0.0001, step_loss=0.0142]\n",
      "Steps:  38%|███▊      | 377/1000 [56:00<1:32:12,  8.88s/it, lr=0.0001, step_loss=0.0142]\n",
      "Steps:  38%|███▊      | 377/1000 [56:00<1:32:12,  8.88s/it, lr=0.0001, step_loss=0.00794]\n",
      "Steps:  38%|███▊      | 378/1000 [56:09<1:32:51,  8.96s/it, lr=0.0001, step_loss=0.00794]\n",
      "Steps:  38%|███▊      | 378/1000 [56:09<1:32:51,  8.96s/it, lr=0.0001, step_loss=0.0151]\n",
      "Steps:  38%|███▊      | 379/1000 [56:18<1:31:59,  8.89s/it, lr=0.0001, step_loss=0.0151]\n",
      "Steps:  38%|███▊      | 379/1000 [56:18<1:31:59,  8.89s/it, lr=0.0001, step_loss=0.0262]\n",
      "Steps:  38%|███▊      | 380/1000 [56:27<1:32:13,  8.92s/it, lr=0.0001, step_loss=0.0262]\n",
      "Steps:  38%|███▊      | 380/1000 [56:27<1:32:13,  8.92s/it, lr=0.0001, step_loss=0.00845]\n",
      "Steps:  38%|███▊      | 381/1000 [56:36<1:31:47,  8.90s/it, lr=0.0001, step_loss=0.00845]\n",
      "Steps:  38%|███▊      | 381/1000 [56:36<1:31:47,  8.90s/it, lr=0.0001, step_loss=0.041]\n",
      "Steps:  38%|███▊      | 382/1000 [56:45<1:32:19,  8.96s/it, lr=0.0001, step_loss=0.041]\n",
      "Steps:  38%|███▊      | 382/1000 [56:45<1:32:19,  8.96s/it, lr=0.0001, step_loss=0.0204]\n",
      "Steps:  38%|███▊      | 383/1000 [56:54<1:31:26,  8.89s/it, lr=0.0001, step_loss=0.0204]\n",
      "Steps:  38%|███▊      | 383/1000 [56:54<1:31:26,  8.89s/it, lr=0.0001, step_loss=0.0348]\n",
      "Steps:  38%|███▊      | 384/1000 [57:03<1:31:34,  8.92s/it, lr=0.0001, step_loss=0.0348]\n",
      "Steps:  38%|███▊      | 384/1000 [57:03<1:31:34,  8.92s/it, lr=0.0001, step_loss=0.0109]\n",
      "Steps:  38%|███▊      | 385/1000 [57:12<1:31:12,  8.90s/it, lr=0.0001, step_loss=0.0109]\n",
      "Steps:  38%|███▊      | 385/1000 [57:12<1:31:12,  8.90s/it, lr=0.0001, step_loss=0.0132]\n",
      "Steps:  39%|███▊      | 386/1000 [57:21<1:31:43,  8.96s/it, lr=0.0001, step_loss=0.0132]\n",
      "Steps:  39%|███▊      | 386/1000 [57:21<1:31:43,  8.96s/it, lr=0.0001, step_loss=0.0319]\n",
      "Steps:  39%|███▊      | 387/1000 [57:30<1:30:48,  8.89s/it, lr=0.0001, step_loss=0.0319]\n",
      "Steps:  39%|███▊      | 387/1000 [57:30<1:30:48,  8.89s/it, lr=0.0001, step_loss=0.0133]\n",
      "Steps:  39%|███▉      | 388/1000 [57:39<1:30:58,  8.92s/it, lr=0.0001, step_loss=0.0133]\n",
      "Steps:  39%|███▉      | 388/1000 [57:39<1:30:58,  8.92s/it, lr=0.0001, step_loss=0.00582]\n",
      "Steps:  39%|███▉      | 389/1000 [57:47<1:30:34,  8.89s/it, lr=0.0001, step_loss=0.00582]\n",
      "Steps:  39%|███▉      | 389/1000 [57:47<1:30:34,  8.89s/it, lr=0.0001, step_loss=0.183]\n",
      "Steps:  39%|███▉      | 390/1000 [57:57<1:31:09,  8.97s/it, lr=0.0001, step_loss=0.183]\n",
      "Steps:  39%|███▉      | 390/1000 [57:57<1:31:09,  8.97s/it, lr=0.0001, step_loss=0.0328]\n",
      "Steps:  39%|███▉      | 391/1000 [58:05<1:30:10,  8.88s/it, lr=0.0001, step_loss=0.0328]\n",
      "Steps:  39%|███▉      | 391/1000 [58:05<1:30:10,  8.88s/it, lr=0.0001, step_loss=0.00894]\n",
      "Steps:  39%|███▉      | 392/1000 [58:14<1:30:24,  8.92s/it, lr=0.0001, step_loss=0.00894]\n",
      "Steps:  39%|███▉      | 392/1000 [58:14<1:30:24,  8.92s/it, lr=0.0001, step_loss=0.0102]\n",
      "Steps:  39%|███▉      | 393/1000 [58:23<1:29:50,  8.88s/it, lr=0.0001, step_loss=0.0102]\n",
      "Steps:  39%|███▉      | 393/1000 [58:23<1:29:50,  8.88s/it, lr=0.0001, step_loss=0.0371]\n",
      "Steps:  39%|███▉      | 394/1000 [58:32<1:30:27,  8.96s/it, lr=0.0001, step_loss=0.0371]\n",
      "Steps:  39%|███▉      | 394/1000 [58:32<1:30:27,  8.96s/it, lr=0.0001, step_loss=0.0392]\n",
      "Steps:  40%|███▉      | 395/1000 [58:41<1:29:45,  8.90s/it, lr=0.0001, step_loss=0.0392]\n",
      "Steps:  40%|███▉      | 395/1000 [58:41<1:29:45,  8.90s/it, lr=0.0001, step_loss=0.0281]\n",
      "Steps:  40%|███▉      | 396/1000 [58:50<1:29:55,  8.93s/it, lr=0.0001, step_loss=0.0281]\n",
      "Steps:  40%|███▉      | 396/1000 [58:50<1:29:55,  8.93s/it, lr=0.0001, step_loss=0.0156]\n",
      "Steps:  40%|███▉      | 397/1000 [58:59<1:29:24,  8.90s/it, lr=0.0001, step_loss=0.0156]\n",
      "Steps:  40%|███▉      | 397/1000 [58:59<1:29:24,  8.90s/it, lr=0.0001, step_loss=0.0143]\n",
      "Steps:  40%|███▉      | 398/1000 [59:08<1:29:53,  8.96s/it, lr=0.0001, step_loss=0.0143]\n",
      "Steps:  40%|███▉      | 398/1000 [59:08<1:29:53,  8.96s/it, lr=0.0001, step_loss=0.00851]\n",
      "Steps:  40%|███▉      | 399/1000 [59:17<1:29:05,  8.89s/it, lr=0.0001, step_loss=0.00851]\n",
      "Steps:  40%|███▉      | 399/1000 [59:17<1:29:05,  8.89s/it, lr=0.0001, step_loss=0.0124]\n",
      "Steps:  40%|████      | 400/1000 [59:26<1:29:16,  8.93s/it, lr=0.0001, step_loss=0.0124]04/17/2024 04:54:31 - INFO - accelerate.accelerator - Saving current state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-400\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-400\\unet_ema\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-400/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-400\\unet\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-400/unet/diffusion_pytorch_model.safetensors\n",
      "04/17/2024 04:55:09 - INFO - accelerate.checkpointing - Optimizer state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-400\\optimizer.bin\n",
      "04/17/2024 04:55:09 - INFO - accelerate.checkpointing - Scheduler state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-400\\scheduler.bin\n",
      "04/17/2024 04:55:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-400\\sampler.bin\n",
      "04/17/2024 04:55:09 - INFO - accelerate.checkpointing - Random states saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-400\\random_states_0.pkl\n",
      "04/17/2024 04:55:09 - INFO - __main__ - Saved state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-400\n",
      "\n",
      "Steps:  40%|████      | 400/1000 [1:00:03<1:29:16,  8.93s/it, lr=0.0001, step_loss=0.0211]\n",
      "Steps:  40%|████      | 401/1000 [1:00:12<3:21:35, 20.19s/it, lr=0.0001, step_loss=0.0211]\n",
      "Steps:  40%|████      | 401/1000 [1:00:12<3:21:35, 20.19s/it, lr=0.0001, step_loss=0.0233]\n",
      "Steps:  40%|████      | 402/1000 [1:00:21<2:48:10, 16.87s/it, lr=0.0001, step_loss=0.0233]\n",
      "Steps:  40%|████      | 402/1000 [1:00:21<2:48:10, 16.87s/it, lr=0.0001, step_loss=0.00911]\n",
      "Steps:  40%|████      | 403/1000 [1:00:30<2:23:37, 14.43s/it, lr=0.0001, step_loss=0.00911]\n",
      "Steps:  40%|████      | 403/1000 [1:00:30<2:23:37, 14.43s/it, lr=0.0001, step_loss=0.0361]\n",
      "Steps:  40%|████      | 404/1000 [1:00:39<2:07:09, 12.80s/it, lr=0.0001, step_loss=0.0361]\n",
      "Steps:  40%|████      | 404/1000 [1:00:39<2:07:09, 12.80s/it, lr=0.0001, step_loss=0.015]\n",
      "Steps:  40%|████      | 405/1000 [1:00:48<1:55:09, 11.61s/it, lr=0.0001, step_loss=0.015]\n",
      "Steps:  40%|████      | 405/1000 [1:00:48<1:55:09, 11.61s/it, lr=0.0001, step_loss=0.0204]\n",
      "Steps:  41%|████      | 406/1000 [1:00:57<1:47:30, 10.86s/it, lr=0.0001, step_loss=0.0204]\n",
      "Steps:  41%|████      | 406/1000 [1:00:57<1:47:30, 10.86s/it, lr=0.0001, step_loss=0.00587]\n",
      "Steps:  41%|████      | 407/1000 [1:01:06<1:41:00, 10.22s/it, lr=0.0001, step_loss=0.00587]\n",
      "Steps:  41%|████      | 407/1000 [1:01:06<1:41:00, 10.22s/it, lr=0.0001, step_loss=0.0128]\n",
      "Steps:  41%|████      | 408/1000 [1:01:15<1:37:14,  9.86s/it, lr=0.0001, step_loss=0.0128]\n",
      "Steps:  41%|████      | 408/1000 [1:01:15<1:37:14,  9.86s/it, lr=0.0001, step_loss=0.0239]\n",
      "Steps:  41%|████      | 409/1000 [1:01:23<1:34:02,  9.55s/it, lr=0.0001, step_loss=0.0239]\n",
      "Steps:  41%|████      | 409/1000 [1:01:23<1:34:02,  9.55s/it, lr=0.0001, step_loss=0.0081]\n",
      "Steps:  41%|████      | 410/1000 [1:01:33<1:32:34,  9.41s/it, lr=0.0001, step_loss=0.0081]\n",
      "Steps:  41%|████      | 410/1000 [1:01:33<1:32:34,  9.41s/it, lr=0.0001, step_loss=0.00778]\n",
      "Steps:  41%|████      | 411/1000 [1:01:41<1:30:23,  9.21s/it, lr=0.0001, step_loss=0.00778]\n",
      "Steps:  41%|████      | 411/1000 [1:01:41<1:30:23,  9.21s/it, lr=0.0001, step_loss=0.00611]\n",
      "Steps:  41%|████      | 412/1000 [1:01:50<1:29:36,  9.14s/it, lr=0.0001, step_loss=0.00611]\n",
      "Steps:  41%|████      | 412/1000 [1:01:50<1:29:36,  9.14s/it, lr=0.0001, step_loss=0.00715]\n",
      "Steps:  41%|████▏     | 413/1000 [1:01:59<1:28:35,  9.05s/it, lr=0.0001, step_loss=0.00715]\n",
      "Steps:  41%|████▏     | 413/1000 [1:01:59<1:28:35,  9.05s/it, lr=0.0001, step_loss=0.0122]\n",
      "Steps:  41%|████▏     | 414/1000 [1:02:08<1:28:32,  9.07s/it, lr=0.0001, step_loss=0.0122]\n",
      "Steps:  41%|████▏     | 414/1000 [1:02:08<1:28:32,  9.07s/it, lr=0.0001, step_loss=0.0306]\n",
      "Steps:  42%|████▏     | 415/1000 [1:02:17<1:27:23,  8.96s/it, lr=0.0001, step_loss=0.0306]\n",
      "Steps:  42%|████▏     | 415/1000 [1:02:17<1:27:23,  8.96s/it, lr=0.0001, step_loss=0.0281]\n",
      "Steps:  42%|████▏     | 416/1000 [1:02:26<1:27:22,  8.98s/it, lr=0.0001, step_loss=0.0281]\n",
      "Steps:  42%|████▏     | 416/1000 [1:02:26<1:27:22,  8.98s/it, lr=0.0001, step_loss=0.0102]\n",
      "Steps:  42%|████▏     | 417/1000 [1:02:35<1:26:48,  8.93s/it, lr=0.0001, step_loss=0.0102]\n",
      "Steps:  42%|████▏     | 417/1000 [1:02:35<1:26:48,  8.93s/it, lr=0.0001, step_loss=0.045]\n",
      "Steps:  42%|████▏     | 418/1000 [1:02:44<1:27:09,  8.98s/it, lr=0.0001, step_loss=0.045]\n",
      "Steps:  42%|████▏     | 418/1000 [1:02:44<1:27:09,  8.98s/it, lr=0.0001, step_loss=0.019]\n",
      "Steps:  42%|████▏     | 419/1000 [1:02:53<1:26:14,  8.91s/it, lr=0.0001, step_loss=0.019]\n",
      "Steps:  42%|████▏     | 419/1000 [1:02:53<1:26:14,  8.91s/it, lr=0.0001, step_loss=0.00491]\n",
      "Steps:  42%|████▏     | 420/1000 [1:03:02<1:26:20,  8.93s/it, lr=0.0001, step_loss=0.00491]\n",
      "Steps:  42%|████▏     | 420/1000 [1:03:02<1:26:20,  8.93s/it, lr=0.0001, step_loss=0.0051]\n",
      "Steps:  42%|████▏     | 421/1000 [1:03:10<1:25:54,  8.90s/it, lr=0.0001, step_loss=0.0051]\n",
      "Steps:  42%|████▏     | 421/1000 [1:03:10<1:25:54,  8.90s/it, lr=0.0001, step_loss=0.00543]\n",
      "Steps:  42%|████▏     | 422/1000 [1:03:20<1:26:22,  8.97s/it, lr=0.0001, step_loss=0.00543]\n",
      "Steps:  42%|████▏     | 422/1000 [1:03:20<1:26:22,  8.97s/it, lr=0.0001, step_loss=0.00368]\n",
      "Steps:  42%|████▏     | 423/1000 [1:03:28<1:25:27,  8.89s/it, lr=0.0001, step_loss=0.00368]\n",
      "Steps:  42%|████▏     | 423/1000 [1:03:28<1:25:27,  8.89s/it, lr=0.0001, step_loss=0.00845]\n",
      "Steps:  42%|████▏     | 424/1000 [1:03:37<1:25:38,  8.92s/it, lr=0.0001, step_loss=0.00845]\n",
      "Steps:  42%|████▏     | 424/1000 [1:03:37<1:25:38,  8.92s/it, lr=0.0001, step_loss=0.00479]\n",
      "Steps:  42%|████▎     | 425/1000 [1:03:46<1:25:13,  8.89s/it, lr=0.0001, step_loss=0.00479]\n",
      "Steps:  42%|████▎     | 425/1000 [1:03:46<1:25:13,  8.89s/it, lr=0.0001, step_loss=0.00859]\n",
      "Steps:  43%|████▎     | 426/1000 [1:03:55<1:25:46,  8.97s/it, lr=0.0001, step_loss=0.00859]\n",
      "Steps:  43%|████▎     | 426/1000 [1:03:55<1:25:46,  8.97s/it, lr=0.0001, step_loss=0.0103]\n",
      "Steps:  43%|████▎     | 427/1000 [1:04:04<1:24:53,  8.89s/it, lr=0.0001, step_loss=0.0103]\n",
      "Steps:  43%|████▎     | 427/1000 [1:04:04<1:24:53,  8.89s/it, lr=0.0001, step_loss=0.011]\n",
      "Steps:  43%|████▎     | 428/1000 [1:04:13<1:25:02,  8.92s/it, lr=0.0001, step_loss=0.011]\n",
      "Steps:  43%|████▎     | 428/1000 [1:04:13<1:25:02,  8.92s/it, lr=0.0001, step_loss=0.0089]\n",
      "Steps:  43%|████▎     | 429/1000 [1:04:22<1:24:49,  8.91s/it, lr=0.0001, step_loss=0.0089]\n",
      "Steps:  43%|████▎     | 429/1000 [1:04:22<1:24:49,  8.91s/it, lr=0.0001, step_loss=0.005]\n",
      "Steps:  43%|████▎     | 430/1000 [1:04:31<1:25:41,  9.02s/it, lr=0.0001, step_loss=0.005]\n",
      "Steps:  43%|████▎     | 430/1000 [1:04:31<1:25:41,  9.02s/it, lr=0.0001, step_loss=0.0112]\n",
      "Steps:  43%|████▎     | 431/1000 [1:04:40<1:25:01,  8.96s/it, lr=0.0001, step_loss=0.0112]\n",
      "Steps:  43%|████▎     | 431/1000 [1:04:40<1:25:01,  8.96s/it, lr=0.0001, step_loss=0.00387]\n",
      "Steps:  43%|████▎     | 432/1000 [1:04:49<1:25:04,  8.99s/it, lr=0.0001, step_loss=0.00387]\n",
      "Steps:  43%|████▎     | 432/1000 [1:04:49<1:25:04,  8.99s/it, lr=0.0001, step_loss=0.0127]\n",
      "Steps:  43%|████▎     | 433/1000 [1:04:58<1:24:27,  8.94s/it, lr=0.0001, step_loss=0.0127]\n",
      "Steps:  43%|████▎     | 433/1000 [1:04:58<1:24:27,  8.94s/it, lr=0.0001, step_loss=0.0126]\n",
      "Steps:  43%|████▎     | 434/1000 [1:05:07<1:24:48,  8.99s/it, lr=0.0001, step_loss=0.0126]\n",
      "Steps:  43%|████▎     | 434/1000 [1:05:07<1:24:48,  8.99s/it, lr=0.0001, step_loss=0.00746]\n",
      "Steps:  44%|████▎     | 435/1000 [1:05:16<1:23:48,  8.90s/it, lr=0.0001, step_loss=0.00746]\n",
      "Steps:  44%|████▎     | 435/1000 [1:05:16<1:23:48,  8.90s/it, lr=0.0001, step_loss=0.00981]\n",
      "Steps:  44%|████▎     | 436/1000 [1:05:25<1:23:59,  8.94s/it, lr=0.0001, step_loss=0.00981]\n",
      "Steps:  44%|████▎     | 436/1000 [1:05:25<1:23:59,  8.94s/it, lr=0.0001, step_loss=0.0126]\n",
      "Steps:  44%|████▎     | 437/1000 [1:05:33<1:23:31,  8.90s/it, lr=0.0001, step_loss=0.0126]\n",
      "Steps:  44%|████▎     | 437/1000 [1:05:33<1:23:31,  8.90s/it, lr=0.0001, step_loss=0.00503]\n",
      "Steps:  44%|████▍     | 438/1000 [1:05:43<1:23:58,  8.96s/it, lr=0.0001, step_loss=0.00503]\n",
      "Steps:  44%|████▍     | 438/1000 [1:05:43<1:23:58,  8.96s/it, lr=0.0001, step_loss=0.00736]\n",
      "Steps:  44%|████▍     | 439/1000 [1:05:51<1:23:06,  8.89s/it, lr=0.0001, step_loss=0.00736]\n",
      "Steps:  44%|████▍     | 439/1000 [1:05:51<1:23:06,  8.89s/it, lr=0.0001, step_loss=0.00326]\n",
      "Steps:  44%|████▍     | 440/1000 [1:06:00<1:23:14,  8.92s/it, lr=0.0001, step_loss=0.00326]\n",
      "Steps:  44%|████▍     | 440/1000 [1:06:00<1:23:14,  8.92s/it, lr=0.0001, step_loss=0.00457]\n",
      "Steps:  44%|████▍     | 441/1000 [1:06:09<1:22:52,  8.89s/it, lr=0.0001, step_loss=0.00457]\n",
      "Steps:  44%|████▍     | 441/1000 [1:06:09<1:22:52,  8.89s/it, lr=0.0001, step_loss=0.0135]\n",
      "Steps:  44%|████▍     | 442/1000 [1:06:18<1:23:17,  8.96s/it, lr=0.0001, step_loss=0.0135]\n",
      "Steps:  44%|████▍     | 442/1000 [1:06:18<1:23:17,  8.96s/it, lr=0.0001, step_loss=0.00489]\n",
      "Steps:  44%|████▍     | 443/1000 [1:06:27<1:22:28,  8.88s/it, lr=0.0001, step_loss=0.00489]\n",
      "Steps:  44%|████▍     | 443/1000 [1:06:27<1:22:28,  8.88s/it, lr=0.0001, step_loss=0.011]\n",
      "Steps:  44%|████▍     | 444/1000 [1:06:36<1:22:39,  8.92s/it, lr=0.0001, step_loss=0.011]\n",
      "Steps:  44%|████▍     | 444/1000 [1:06:36<1:22:39,  8.92s/it, lr=0.0001, step_loss=0.0166]\n",
      "Steps:  44%|████▍     | 445/1000 [1:06:45<1:22:18,  8.90s/it, lr=0.0001, step_loss=0.0166]\n",
      "Steps:  44%|████▍     | 445/1000 [1:06:45<1:22:18,  8.90s/it, lr=0.0001, step_loss=0.00493]\n",
      "Steps:  45%|████▍     | 446/1000 [1:06:54<1:22:46,  8.96s/it, lr=0.0001, step_loss=0.00493]\n",
      "Steps:  45%|████▍     | 446/1000 [1:06:54<1:22:46,  8.96s/it, lr=0.0001, step_loss=0.00382]\n",
      "Steps:  45%|████▍     | 447/1000 [1:07:03<1:21:54,  8.89s/it, lr=0.0001, step_loss=0.00382]\n",
      "Steps:  45%|████▍     | 447/1000 [1:07:03<1:21:54,  8.89s/it, lr=0.0001, step_loss=0.00169]\n",
      "Steps:  45%|████▍     | 448/1000 [1:07:12<1:22:04,  8.92s/it, lr=0.0001, step_loss=0.00169]\n",
      "Steps:  45%|████▍     | 448/1000 [1:07:12<1:22:04,  8.92s/it, lr=0.0001, step_loss=0.0101]\n",
      "Steps:  45%|████▍     | 449/1000 [1:07:20<1:21:36,  8.89s/it, lr=0.0001, step_loss=0.0101]\n",
      "Steps:  45%|████▍     | 449/1000 [1:07:20<1:21:36,  8.89s/it, lr=0.0001, step_loss=0.00852]\n",
      "Steps:  45%|████▌     | 450/1000 [1:07:29<1:22:05,  8.96s/it, lr=0.0001, step_loss=0.00852]\n",
      "Steps:  45%|████▌     | 450/1000 [1:07:29<1:22:05,  8.96s/it, lr=0.0001, step_loss=0.00355]\n",
      "Steps:  45%|████▌     | 451/1000 [1:07:38<1:21:16,  8.88s/it, lr=0.0001, step_loss=0.00355]\n",
      "Steps:  45%|████▌     | 451/1000 [1:07:38<1:21:16,  8.88s/it, lr=0.0001, step_loss=0.00808]\n",
      "Steps:  45%|████▌     | 452/1000 [1:07:47<1:21:30,  8.92s/it, lr=0.0001, step_loss=0.00808]\n",
      "Steps:  45%|████▌     | 452/1000 [1:07:47<1:21:30,  8.92s/it, lr=0.0001, step_loss=0.00543]\n",
      "Steps:  45%|████▌     | 453/1000 [1:07:56<1:21:03,  8.89s/it, lr=0.0001, step_loss=0.00543]\n",
      "Steps:  45%|████▌     | 453/1000 [1:07:56<1:21:03,  8.89s/it, lr=0.0001, step_loss=0.00519]\n",
      "Steps:  45%|████▌     | 454/1000 [1:08:05<1:21:32,  8.96s/it, lr=0.0001, step_loss=0.00519]\n",
      "Steps:  45%|████▌     | 454/1000 [1:08:05<1:21:32,  8.96s/it, lr=0.0001, step_loss=0.145]\n",
      "Steps:  46%|████▌     | 455/1000 [1:08:14<1:20:41,  8.88s/it, lr=0.0001, step_loss=0.145]\n",
      "Steps:  46%|████▌     | 455/1000 [1:08:14<1:20:41,  8.88s/it, lr=0.0001, step_loss=0.0117]\n",
      "Steps:  46%|████▌     | 456/1000 [1:08:23<1:20:54,  8.92s/it, lr=0.0001, step_loss=0.0117]\n",
      "Steps:  46%|████▌     | 456/1000 [1:08:23<1:20:54,  8.92s/it, lr=0.0001, step_loss=0.0303]\n",
      "Steps:  46%|████▌     | 457/1000 [1:08:32<1:20:26,  8.89s/it, lr=0.0001, step_loss=0.0303]\n",
      "Steps:  46%|████▌     | 457/1000 [1:08:32<1:20:26,  8.89s/it, lr=0.0001, step_loss=0.0181]\n",
      "Steps:  46%|████▌     | 458/1000 [1:08:41<1:20:56,  8.96s/it, lr=0.0001, step_loss=0.0181]\n",
      "Steps:  46%|████▌     | 458/1000 [1:08:41<1:20:56,  8.96s/it, lr=0.0001, step_loss=0.00774]\n",
      "Steps:  46%|████▌     | 459/1000 [1:08:50<1:20:07,  8.89s/it, lr=0.0001, step_loss=0.00774]\n",
      "Steps:  46%|████▌     | 459/1000 [1:08:50<1:20:07,  8.89s/it, lr=0.0001, step_loss=0.00523]\n",
      "Steps:  46%|████▌     | 460/1000 [1:08:59<1:20:18,  8.92s/it, lr=0.0001, step_loss=0.00523]\n",
      "Steps:  46%|████▌     | 460/1000 [1:08:59<1:20:18,  8.92s/it, lr=0.0001, step_loss=0.00715]\n",
      "Steps:  46%|████▌     | 461/1000 [1:09:07<1:19:50,  8.89s/it, lr=0.0001, step_loss=0.00715]\n",
      "Steps:  46%|████▌     | 461/1000 [1:09:07<1:19:50,  8.89s/it, lr=0.0001, step_loss=0.0184]\n",
      "Steps:  46%|████▌     | 462/1000 [1:09:16<1:20:15,  8.95s/it, lr=0.0001, step_loss=0.0184]\n",
      "Steps:  46%|████▌     | 462/1000 [1:09:16<1:20:15,  8.95s/it, lr=0.0001, step_loss=0.0442]\n",
      "Steps:  46%|████▋     | 463/1000 [1:09:25<1:19:32,  8.89s/it, lr=0.0001, step_loss=0.0442]\n",
      "Steps:  46%|████▋     | 463/1000 [1:09:25<1:19:32,  8.89s/it, lr=0.0001, step_loss=0.00354]\n",
      "Steps:  46%|████▋     | 464/1000 [1:09:34<1:19:40,  8.92s/it, lr=0.0001, step_loss=0.00354]\n",
      "Steps:  46%|████▋     | 464/1000 [1:09:34<1:19:40,  8.92s/it, lr=0.0001, step_loss=0.0386]\n",
      "Steps:  46%|████▋     | 465/1000 [1:09:43<1:19:18,  8.89s/it, lr=0.0001, step_loss=0.0386]\n",
      "Steps:  46%|████▋     | 465/1000 [1:09:43<1:19:18,  8.89s/it, lr=0.0001, step_loss=0.00749]\n",
      "Steps:  47%|████▋     | 466/1000 [1:09:52<1:19:45,  8.96s/it, lr=0.0001, step_loss=0.00749]\n",
      "Steps:  47%|████▋     | 466/1000 [1:09:52<1:19:45,  8.96s/it, lr=0.0001, step_loss=0.00478]\n",
      "Steps:  47%|████▋     | 467/1000 [1:10:01<1:18:56,  8.89s/it, lr=0.0001, step_loss=0.00478]\n",
      "Steps:  47%|████▋     | 467/1000 [1:10:01<1:18:56,  8.89s/it, lr=0.0001, step_loss=0.0147]\n",
      "Steps:  47%|████▋     | 468/1000 [1:10:10<1:19:05,  8.92s/it, lr=0.0001, step_loss=0.0147]\n",
      "Steps:  47%|████▋     | 468/1000 [1:10:10<1:19:05,  8.92s/it, lr=0.0001, step_loss=0.00599]\n",
      "Steps:  47%|████▋     | 469/1000 [1:10:19<1:18:36,  8.88s/it, lr=0.0001, step_loss=0.00599]\n",
      "Steps:  47%|████▋     | 469/1000 [1:10:19<1:18:36,  8.88s/it, lr=0.0001, step_loss=0.002]\n",
      "Steps:  47%|████▋     | 470/1000 [1:10:28<1:19:09,  8.96s/it, lr=0.0001, step_loss=0.002]\n",
      "Steps:  47%|████▋     | 470/1000 [1:10:28<1:19:09,  8.96s/it, lr=0.0001, step_loss=0.0486]\n",
      "Steps:  47%|████▋     | 471/1000 [1:10:36<1:18:21,  8.89s/it, lr=0.0001, step_loss=0.0486]\n",
      "Steps:  47%|████▋     | 471/1000 [1:10:36<1:18:21,  8.89s/it, lr=0.0001, step_loss=0.00541]\n",
      "Steps:  47%|████▋     | 472/1000 [1:10:45<1:18:30,  8.92s/it, lr=0.0001, step_loss=0.00541]\n",
      "Steps:  47%|████▋     | 472/1000 [1:10:45<1:18:30,  8.92s/it, lr=0.0001, step_loss=0.00419]\n",
      "Steps:  47%|████▋     | 473/1000 [1:10:54<1:18:03,  8.89s/it, lr=0.0001, step_loss=0.00419]\n",
      "Steps:  47%|████▋     | 473/1000 [1:10:54<1:18:03,  8.89s/it, lr=0.0001, step_loss=0.00962]\n",
      "Steps:  47%|████▋     | 474/1000 [1:11:03<1:18:28,  8.95s/it, lr=0.0001, step_loss=0.00962]\n",
      "Steps:  47%|████▋     | 474/1000 [1:11:03<1:18:28,  8.95s/it, lr=0.0001, step_loss=0.00431]\n",
      "Steps:  48%|████▊     | 475/1000 [1:11:12<1:17:45,  8.89s/it, lr=0.0001, step_loss=0.00431]\n",
      "Steps:  48%|████▊     | 475/1000 [1:11:12<1:17:45,  8.89s/it, lr=0.0001, step_loss=0.0158]\n",
      "Steps:  48%|████▊     | 476/1000 [1:11:21<1:17:56,  8.92s/it, lr=0.0001, step_loss=0.0158]\n",
      "Steps:  48%|████▊     | 476/1000 [1:11:21<1:17:56,  8.92s/it, lr=0.0001, step_loss=0.00589]\n",
      "Steps:  48%|████▊     | 477/1000 [1:11:30<1:17:33,  8.90s/it, lr=0.0001, step_loss=0.00589]\n",
      "Steps:  48%|████▊     | 477/1000 [1:11:30<1:17:33,  8.90s/it, lr=0.0001, step_loss=0.00967]\n",
      "Steps:  48%|████▊     | 478/1000 [1:11:39<1:17:59,  8.96s/it, lr=0.0001, step_loss=0.00967]\n",
      "Steps:  48%|████▊     | 478/1000 [1:11:39<1:17:59,  8.96s/it, lr=0.0001, step_loss=0.0157]\n",
      "Steps:  48%|████▊     | 479/1000 [1:11:48<1:17:12,  8.89s/it, lr=0.0001, step_loss=0.0157]\n",
      "Steps:  48%|████▊     | 479/1000 [1:11:48<1:17:12,  8.89s/it, lr=0.0001, step_loss=0.00339]\n",
      "Steps:  48%|████▊     | 480/1000 [1:11:57<1:17:20,  8.92s/it, lr=0.0001, step_loss=0.00339]\n",
      "Steps:  48%|████▊     | 480/1000 [1:11:57<1:17:20,  8.92s/it, lr=0.0001, step_loss=0.0102]\n",
      "Steps:  48%|████▊     | 481/1000 [1:12:06<1:16:54,  8.89s/it, lr=0.0001, step_loss=0.0102]\n",
      "Steps:  48%|████▊     | 481/1000 [1:12:06<1:16:54,  8.89s/it, lr=0.0001, step_loss=0.0037]\n",
      "Steps:  48%|████▊     | 482/1000 [1:12:15<1:17:17,  8.95s/it, lr=0.0001, step_loss=0.0037]\n",
      "Steps:  48%|████▊     | 482/1000 [1:12:15<1:17:17,  8.95s/it, lr=0.0001, step_loss=0.0298]\n",
      "Steps:  48%|████▊     | 483/1000 [1:12:23<1:16:31,  8.88s/it, lr=0.0001, step_loss=0.0298]\n",
      "Steps:  48%|████▊     | 483/1000 [1:12:23<1:16:31,  8.88s/it, lr=0.0001, step_loss=0.00377]\n",
      "Steps:  48%|████▊     | 484/1000 [1:12:32<1:16:42,  8.92s/it, lr=0.0001, step_loss=0.00377]\n",
      "Steps:  48%|████▊     | 484/1000 [1:12:32<1:16:42,  8.92s/it, lr=0.0001, step_loss=0.00438]\n",
      "Steps:  48%|████▊     | 485/1000 [1:12:41<1:16:13,  8.88s/it, lr=0.0001, step_loss=0.00438]\n",
      "Steps:  48%|████▊     | 485/1000 [1:12:41<1:16:13,  8.88s/it, lr=0.0001, step_loss=0.0422]\n",
      "Steps:  49%|████▊     | 486/1000 [1:12:50<1:16:43,  8.96s/it, lr=0.0001, step_loss=0.0422]\n",
      "Steps:  49%|████▊     | 486/1000 [1:12:50<1:16:43,  8.96s/it, lr=0.0001, step_loss=0.00455]\n",
      "Steps:  49%|████▊     | 487/1000 [1:12:59<1:16:01,  8.89s/it, lr=0.0001, step_loss=0.00455]\n",
      "Steps:  49%|████▊     | 487/1000 [1:12:59<1:16:01,  8.89s/it, lr=0.0001, step_loss=0.0288]\n",
      "Steps:  49%|████▉     | 488/1000 [1:13:08<1:16:10,  8.93s/it, lr=0.0001, step_loss=0.0288]\n",
      "Steps:  49%|████▉     | 488/1000 [1:13:08<1:16:10,  8.93s/it, lr=0.0001, step_loss=0.00995]\n",
      "Steps:  49%|████▉     | 489/1000 [1:13:17<1:15:43,  8.89s/it, lr=0.0001, step_loss=0.00995]\n",
      "Steps:  49%|████▉     | 489/1000 [1:13:17<1:15:43,  8.89s/it, lr=0.0001, step_loss=0.00864]\n",
      "Steps:  49%|████▉     | 490/1000 [1:13:26<1:16:07,  8.96s/it, lr=0.0001, step_loss=0.00864]\n",
      "Steps:  49%|████▉     | 490/1000 [1:13:26<1:16:07,  8.96s/it, lr=0.0001, step_loss=0.00722]\n",
      "Steps:  49%|████▉     | 491/1000 [1:13:35<1:15:23,  8.89s/it, lr=0.0001, step_loss=0.00722]\n",
      "Steps:  49%|████▉     | 491/1000 [1:13:35<1:15:23,  8.89s/it, lr=0.0001, step_loss=0.0177]\n",
      "Steps:  49%|████▉     | 492/1000 [1:13:44<1:15:33,  8.92s/it, lr=0.0001, step_loss=0.0177]\n",
      "Steps:  49%|████▉     | 492/1000 [1:13:44<1:15:33,  8.92s/it, lr=0.0001, step_loss=0.00721]\n",
      "Steps:  49%|████▉     | 493/1000 [1:13:53<1:15:05,  8.89s/it, lr=0.0001, step_loss=0.00721]\n",
      "Steps:  49%|████▉     | 493/1000 [1:13:53<1:15:05,  8.89s/it, lr=0.0001, step_loss=0.0206]\n",
      "Steps:  49%|████▉     | 494/1000 [1:14:02<1:15:29,  8.95s/it, lr=0.0001, step_loss=0.0206]\n",
      "Steps:  49%|████▉     | 494/1000 [1:14:02<1:15:29,  8.95s/it, lr=0.0001, step_loss=0.00455]\n",
      "Steps:  50%|████▉     | 495/1000 [1:14:10<1:14:49,  8.89s/it, lr=0.0001, step_loss=0.00455]\n",
      "Steps:  50%|████▉     | 495/1000 [1:14:10<1:14:49,  8.89s/it, lr=0.0001, step_loss=0.0187]\n",
      "Steps:  50%|████▉     | 496/1000 [1:14:19<1:14:57,  8.92s/it, lr=0.0001, step_loss=0.0187]\n",
      "Steps:  50%|████▉     | 496/1000 [1:14:19<1:14:57,  8.92s/it, lr=0.0001, step_loss=0.0104]\n",
      "Steps:  50%|████▉     | 497/1000 [1:14:28<1:14:30,  8.89s/it, lr=0.0001, step_loss=0.0104]\n",
      "Steps:  50%|████▉     | 497/1000 [1:14:28<1:14:30,  8.89s/it, lr=0.0001, step_loss=0.0109]\n",
      "Steps:  50%|████▉     | 498/1000 [1:14:37<1:14:56,  8.96s/it, lr=0.0001, step_loss=0.0109]\n",
      "Steps:  50%|████▉     | 498/1000 [1:14:37<1:14:56,  8.96s/it, lr=0.0001, step_loss=0.00884]\n",
      "Steps:  50%|████▉     | 499/1000 [1:14:46<1:14:09,  8.88s/it, lr=0.0001, step_loss=0.00884]\n",
      "Steps:  50%|████▉     | 499/1000 [1:14:46<1:14:09,  8.88s/it, lr=0.0001, step_loss=0.0228]\n",
      "Steps:  50%|█████     | 500/1000 [1:14:55<1:14:19,  8.92s/it, lr=0.0001, step_loss=0.0228]04/17/2024 05:10:01 - INFO - accelerate.accelerator - Saving current state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-500\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-500\\unet_ema\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-500/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-500\\unet\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-500/unet/diffusion_pytorch_model.safetensors\n",
      "04/17/2024 05:10:38 - INFO - accelerate.checkpointing - Optimizer state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-500\\optimizer.bin\n",
      "04/17/2024 05:10:38 - INFO - accelerate.checkpointing - Scheduler state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-500\\scheduler.bin\n",
      "04/17/2024 05:10:38 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-500\\sampler.bin\n",
      "04/17/2024 05:10:38 - INFO - accelerate.checkpointing - Random states saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-500\\random_states_0.pkl\n",
      "04/17/2024 05:10:38 - INFO - __main__ - Saved state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-500\n",
      "\n",
      "Steps:  50%|█████     | 500/1000 [1:15:32<1:14:19,  8.92s/it, lr=0.0001, step_loss=0.0299]\n",
      "Steps:  50%|█████     | 501/1000 [1:15:41<2:46:30, 20.02s/it, lr=0.0001, step_loss=0.0299]\n",
      "Steps:  50%|█████     | 501/1000 [1:15:41<2:46:30, 20.02s/it, lr=0.0001, step_loss=0.0144]\n",
      "Steps:  50%|█████     | 502/1000 [1:15:50<2:18:59, 16.75s/it, lr=0.0001, step_loss=0.0144]\n",
      "Steps:  50%|█████     | 502/1000 [1:15:50<2:18:59, 16.75s/it, lr=0.0001, step_loss=0.0277]\n",
      "Steps:  50%|█████     | 503/1000 [1:15:59<1:58:46, 14.34s/it, lr=0.0001, step_loss=0.0277]\n",
      "Steps:  50%|█████     | 503/1000 [1:15:59<1:58:46, 14.34s/it, lr=0.0001, step_loss=0.0195]\n",
      "Steps:  50%|█████     | 504/1000 [1:16:08<1:45:16, 12.74s/it, lr=0.0001, step_loss=0.0195]\n",
      "Steps:  50%|█████     | 504/1000 [1:16:08<1:45:16, 12.74s/it, lr=0.0001, step_loss=0.00677]\n",
      "Steps:  50%|█████     | 505/1000 [1:16:17<1:35:24, 11.56s/it, lr=0.0001, step_loss=0.00677]\n",
      "Steps:  50%|█████     | 505/1000 [1:16:17<1:35:24, 11.56s/it, lr=0.0001, step_loss=0.00815]\n",
      "Steps:  51%|█████     | 506/1000 [1:16:26<1:29:09, 10.83s/it, lr=0.0001, step_loss=0.00815]\n",
      "Steps:  51%|█████     | 506/1000 [1:16:26<1:29:09, 10.83s/it, lr=0.0001, step_loss=0.0107]\n",
      "Steps:  51%|█████     | 507/1000 [1:16:34<1:23:44, 10.19s/it, lr=0.0001, step_loss=0.0107]\n",
      "Steps:  51%|█████     | 507/1000 [1:16:34<1:23:44, 10.19s/it, lr=0.0001, step_loss=0.00996]\n",
      "Steps:  51%|█████     | 508/1000 [1:16:43<1:20:37,  9.83s/it, lr=0.0001, step_loss=0.00996]\n",
      "Steps:  51%|█████     | 508/1000 [1:16:43<1:20:37,  9.83s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  51%|█████     | 509/1000 [1:16:52<1:17:58,  9.53s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  51%|█████     | 509/1000 [1:16:52<1:17:58,  9.53s/it, lr=0.0001, step_loss=0.0374]\n",
      "Steps:  51%|█████     | 510/1000 [1:17:01<1:16:48,  9.41s/it, lr=0.0001, step_loss=0.0374]\n",
      "Steps:  51%|█████     | 510/1000 [1:17:01<1:16:48,  9.41s/it, lr=0.0001, step_loss=0.00257]\n",
      "Steps:  51%|█████     | 511/1000 [1:17:10<1:14:56,  9.20s/it, lr=0.0001, step_loss=0.00257]\n",
      "Steps:  51%|█████     | 511/1000 [1:17:10<1:14:56,  9.20s/it, lr=0.0001, step_loss=0.0227]\n",
      "Steps:  51%|█████     | 512/1000 [1:17:19<1:14:18,  9.14s/it, lr=0.0001, step_loss=0.0227]\n",
      "Steps:  51%|█████     | 512/1000 [1:17:19<1:14:18,  9.14s/it, lr=0.0001, step_loss=0.00665]\n",
      "Steps:  51%|█████▏    | 513/1000 [1:17:28<1:13:24,  9.05s/it, lr=0.0001, step_loss=0.00665]\n",
      "Steps:  51%|█████▏    | 513/1000 [1:17:28<1:13:24,  9.05s/it, lr=0.0001, step_loss=0.0126]\n",
      "Steps:  51%|█████▏    | 514/1000 [1:17:37<1:13:22,  9.06s/it, lr=0.0001, step_loss=0.0126]\n",
      "Steps:  51%|█████▏    | 514/1000 [1:17:37<1:13:22,  9.06s/it, lr=0.0001, step_loss=0.00525]\n",
      "Steps:  52%|█████▏    | 515/1000 [1:17:46<1:12:29,  8.97s/it, lr=0.0001, step_loss=0.00525]\n",
      "Steps:  52%|█████▏    | 515/1000 [1:17:46<1:12:29,  8.97s/it, lr=0.0001, step_loss=0.00893]\n",
      "Steps:  52%|█████▏    | 516/1000 [1:17:55<1:12:25,  8.98s/it, lr=0.0001, step_loss=0.00893]\n",
      "Steps:  52%|█████▏    | 516/1000 [1:17:55<1:12:25,  8.98s/it, lr=0.0001, step_loss=0.00909]\n",
      "Steps:  52%|█████▏    | 517/1000 [1:18:04<1:11:52,  8.93s/it, lr=0.0001, step_loss=0.00909]\n",
      "Steps:  52%|█████▏    | 517/1000 [1:18:04<1:11:52,  8.93s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  52%|█████▏    | 518/1000 [1:18:13<1:12:09,  8.98s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  52%|█████▏    | 518/1000 [1:18:13<1:12:09,  8.98s/it, lr=0.0001, step_loss=0.00674]\n",
      "Steps:  52%|█████▏    | 519/1000 [1:18:21<1:11:25,  8.91s/it, lr=0.0001, step_loss=0.00674]\n",
      "Steps:  52%|█████▏    | 519/1000 [1:18:21<1:11:25,  8.91s/it, lr=0.0001, step_loss=0.00958]\n",
      "Steps:  52%|█████▏    | 520/1000 [1:18:30<1:11:30,  8.94s/it, lr=0.0001, step_loss=0.00958]\n",
      "Steps:  52%|█████▏    | 520/1000 [1:18:30<1:11:30,  8.94s/it, lr=0.0001, step_loss=0.0096]\n",
      "Steps:  52%|█████▏    | 521/1000 [1:18:39<1:10:59,  8.89s/it, lr=0.0001, step_loss=0.0096]\n",
      "Steps:  52%|█████▏    | 521/1000 [1:18:39<1:10:59,  8.89s/it, lr=0.0001, step_loss=0.00258]\n",
      "Steps:  52%|█████▏    | 522/1000 [1:18:48<1:11:25,  8.97s/it, lr=0.0001, step_loss=0.00258]\n",
      "Steps:  52%|█████▏    | 522/1000 [1:18:48<1:11:25,  8.97s/it, lr=0.0001, step_loss=0.00994]\n",
      "Steps:  52%|█████▏    | 523/1000 [1:18:57<1:10:42,  8.89s/it, lr=0.0001, step_loss=0.00994]\n",
      "Steps:  52%|█████▏    | 523/1000 [1:18:57<1:10:42,  8.89s/it, lr=0.0001, step_loss=0.00277]\n",
      "Steps:  52%|█████▏    | 524/1000 [1:19:06<1:10:50,  8.93s/it, lr=0.0001, step_loss=0.00277]\n",
      "Steps:  52%|█████▏    | 524/1000 [1:19:06<1:10:50,  8.93s/it, lr=0.0001, step_loss=0.00227]\n",
      "Steps:  52%|█████▎    | 525/1000 [1:19:15<1:10:28,  8.90s/it, lr=0.0001, step_loss=0.00227]\n",
      "Steps:  52%|█████▎    | 525/1000 [1:19:15<1:10:28,  8.90s/it, lr=0.0001, step_loss=0.00423]\n",
      "Steps:  53%|█████▎    | 526/1000 [1:19:24<1:10:50,  8.97s/it, lr=0.0001, step_loss=0.00423]\n",
      "Steps:  53%|█████▎    | 526/1000 [1:19:24<1:10:50,  8.97s/it, lr=0.0001, step_loss=0.0112]\n",
      "Steps:  53%|█████▎    | 527/1000 [1:19:33<1:10:05,  8.89s/it, lr=0.0001, step_loss=0.0112]\n",
      "Steps:  53%|█████▎    | 527/1000 [1:19:33<1:10:05,  8.89s/it, lr=0.0001, step_loss=0.00273]\n",
      "Steps:  53%|█████▎    | 528/1000 [1:19:42<1:10:11,  8.92s/it, lr=0.0001, step_loss=0.00273]\n",
      "Steps:  53%|█████▎    | 528/1000 [1:19:42<1:10:11,  8.92s/it, lr=0.0001, step_loss=0.00428]\n",
      "Steps:  53%|█████▎    | 529/1000 [1:19:51<1:09:47,  8.89s/it, lr=0.0001, step_loss=0.00428]\n",
      "Steps:  53%|█████▎    | 529/1000 [1:19:51<1:09:47,  8.89s/it, lr=0.0001, step_loss=0.00761]\n",
      "Steps:  53%|█████▎    | 530/1000 [1:20:00<1:10:10,  8.96s/it, lr=0.0001, step_loss=0.00761]\n",
      "Steps:  53%|█████▎    | 530/1000 [1:20:00<1:10:10,  8.96s/it, lr=0.0001, step_loss=0.00744]\n",
      "Steps:  53%|█████▎    | 531/1000 [1:20:08<1:09:26,  8.88s/it, lr=0.0001, step_loss=0.00744]\n",
      "Steps:  53%|█████▎    | 531/1000 [1:20:08<1:09:26,  8.88s/it, lr=0.0001, step_loss=0.0156]\n",
      "Steps:  53%|█████▎    | 532/1000 [1:20:17<1:09:34,  8.92s/it, lr=0.0001, step_loss=0.0156]\n",
      "Steps:  53%|█████▎    | 532/1000 [1:20:17<1:09:34,  8.92s/it, lr=0.0001, step_loss=0.0229]\n",
      "Steps:  53%|█████▎    | 533/1000 [1:20:26<1:09:07,  8.88s/it, lr=0.0001, step_loss=0.0229]\n",
      "Steps:  53%|█████▎    | 533/1000 [1:20:26<1:09:07,  8.88s/it, lr=0.0001, step_loss=0.0154]\n",
      "Steps:  53%|█████▎    | 534/1000 [1:20:35<1:09:31,  8.95s/it, lr=0.0001, step_loss=0.0154]\n",
      "Steps:  53%|█████▎    | 534/1000 [1:20:35<1:09:31,  8.95s/it, lr=0.0001, step_loss=0.00654]\n",
      "Steps:  54%|█████▎    | 535/1000 [1:20:44<1:08:53,  8.89s/it, lr=0.0001, step_loss=0.00654]\n",
      "Steps:  54%|█████▎    | 535/1000 [1:20:44<1:08:53,  8.89s/it, lr=0.0001, step_loss=0.019]\n",
      "Steps:  54%|█████▎    | 536/1000 [1:20:53<1:09:00,  8.92s/it, lr=0.0001, step_loss=0.019]\n",
      "Steps:  54%|█████▎    | 536/1000 [1:20:53<1:09:00,  8.92s/it, lr=0.0001, step_loss=0.0099]\n",
      "Steps:  54%|█████▎    | 537/1000 [1:21:02<1:08:35,  8.89s/it, lr=0.0001, step_loss=0.0099]\n",
      "Steps:  54%|█████▎    | 537/1000 [1:21:02<1:08:35,  8.89s/it, lr=0.0001, step_loss=0.00349]\n",
      "Steps:  54%|█████▍    | 538/1000 [1:21:11<1:09:04,  8.97s/it, lr=0.0001, step_loss=0.00349]\n",
      "Steps:  54%|█████▍    | 538/1000 [1:21:11<1:09:04,  8.97s/it, lr=0.0001, step_loss=0.00843]\n",
      "Steps:  54%|█████▍    | 539/1000 [1:21:20<1:08:22,  8.90s/it, lr=0.0001, step_loss=0.00843]\n",
      "Steps:  54%|█████▍    | 539/1000 [1:21:20<1:08:22,  8.90s/it, lr=0.0001, step_loss=0.0211]\n",
      "Steps:  54%|█████▍    | 540/1000 [1:21:29<1:08:28,  8.93s/it, lr=0.0001, step_loss=0.0211]\n",
      "Steps:  54%|█████▍    | 540/1000 [1:21:29<1:08:28,  8.93s/it, lr=0.0001, step_loss=0.0118]\n",
      "Steps:  54%|█████▍    | 541/1000 [1:21:38<1:08:06,  8.90s/it, lr=0.0001, step_loss=0.0118]\n",
      "Steps:  54%|█████▍    | 541/1000 [1:21:38<1:08:06,  8.90s/it, lr=0.0001, step_loss=0.00965]\n",
      "Steps:  54%|█████▍    | 542/1000 [1:21:47<1:08:26,  8.97s/it, lr=0.0001, step_loss=0.00965]\n",
      "Steps:  54%|█████▍    | 542/1000 [1:21:47<1:08:26,  8.97s/it, lr=0.0001, step_loss=0.00819]\n",
      "Steps:  54%|█████▍    | 543/1000 [1:21:55<1:07:45,  8.90s/it, lr=0.0001, step_loss=0.00819]\n",
      "Steps:  54%|█████▍    | 543/1000 [1:21:55<1:07:45,  8.90s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  54%|█████▍    | 544/1000 [1:22:04<1:07:49,  8.92s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  54%|█████▍    | 544/1000 [1:22:04<1:07:49,  8.92s/it, lr=0.0001, step_loss=0.00735]\n",
      "Steps:  55%|█████▍    | 545/1000 [1:22:13<1:07:21,  8.88s/it, lr=0.0001, step_loss=0.00735]\n",
      "Steps:  55%|█████▍    | 545/1000 [1:22:13<1:07:21,  8.88s/it, lr=0.0001, step_loss=0.0447]\n",
      "Steps:  55%|█████▍    | 546/1000 [1:22:22<1:07:42,  8.95s/it, lr=0.0001, step_loss=0.0447]\n",
      "Steps:  55%|█████▍    | 546/1000 [1:22:22<1:07:42,  8.95s/it, lr=0.0001, step_loss=0.00347]\n",
      "Steps:  55%|█████▍    | 547/1000 [1:22:31<1:07:05,  8.89s/it, lr=0.0001, step_loss=0.00347]\n",
      "Steps:  55%|█████▍    | 547/1000 [1:22:31<1:07:05,  8.89s/it, lr=0.0001, step_loss=0.0044]\n",
      "Steps:  55%|█████▍    | 548/1000 [1:22:40<1:07:13,  8.92s/it, lr=0.0001, step_loss=0.0044]\n",
      "Steps:  55%|█████▍    | 548/1000 [1:22:40<1:07:13,  8.92s/it, lr=0.0001, step_loss=0.0183]\n",
      "Steps:  55%|█████▍    | 549/1000 [1:22:49<1:06:50,  8.89s/it, lr=0.0001, step_loss=0.0183]\n",
      "Steps:  55%|█████▍    | 549/1000 [1:22:49<1:06:50,  8.89s/it, lr=0.0001, step_loss=0.00147]\n",
      "Steps:  55%|█████▌    | 550/1000 [1:22:58<1:07:14,  8.96s/it, lr=0.0001, step_loss=0.00147]\n",
      "Steps:  55%|█████▌    | 550/1000 [1:22:58<1:07:14,  8.96s/it, lr=0.0001, step_loss=0.0064]\n",
      "Steps:  55%|█████▌    | 551/1000 [1:23:07<1:06:28,  8.88s/it, lr=0.0001, step_loss=0.0064]\n",
      "Steps:  55%|█████▌    | 551/1000 [1:23:07<1:06:28,  8.88s/it, lr=0.0001, step_loss=0.0403]\n",
      "Steps:  55%|█████▌    | 552/1000 [1:23:16<1:06:36,  8.92s/it, lr=0.0001, step_loss=0.0403]\n",
      "Steps:  55%|█████▌    | 552/1000 [1:23:16<1:06:36,  8.92s/it, lr=0.0001, step_loss=0.00411]\n",
      "Steps:  55%|█████▌    | 553/1000 [1:23:25<1:06:14,  8.89s/it, lr=0.0001, step_loss=0.00411]\n",
      "Steps:  55%|█████▌    | 553/1000 [1:23:25<1:06:14,  8.89s/it, lr=0.0001, step_loss=0.0091]\n",
      "Steps:  55%|█████▌    | 554/1000 [1:23:34<1:06:37,  8.96s/it, lr=0.0001, step_loss=0.0091]\n",
      "Steps:  55%|█████▌    | 554/1000 [1:23:34<1:06:37,  8.96s/it, lr=0.0001, step_loss=0.00264]\n",
      "Steps:  56%|█████▌    | 555/1000 [1:23:42<1:05:53,  8.88s/it, lr=0.0001, step_loss=0.00264]\n",
      "Steps:  56%|█████▌    | 555/1000 [1:23:42<1:05:53,  8.88s/it, lr=0.0001, step_loss=0.0196]\n",
      "Steps:  56%|█████▌    | 556/1000 [1:23:51<1:06:00,  8.92s/it, lr=0.0001, step_loss=0.0196]\n",
      "Steps:  56%|█████▌    | 556/1000 [1:23:51<1:06:00,  8.92s/it, lr=0.0001, step_loss=0.00337]\n",
      "Steps:  56%|█████▌    | 557/1000 [1:24:00<1:05:40,  8.90s/it, lr=0.0001, step_loss=0.00337]\n",
      "Steps:  56%|█████▌    | 557/1000 [1:24:00<1:05:40,  8.90s/it, lr=0.0001, step_loss=0.0216]\n",
      "Steps:  56%|█████▌    | 558/1000 [1:24:09<1:06:03,  8.97s/it, lr=0.0001, step_loss=0.0216]\n",
      "Steps:  56%|█████▌    | 558/1000 [1:24:09<1:06:03,  8.97s/it, lr=0.0001, step_loss=0.027]\n",
      "Steps:  56%|█████▌    | 559/1000 [1:24:18<1:05:18,  8.89s/it, lr=0.0001, step_loss=0.027]\n",
      "Steps:  56%|█████▌    | 559/1000 [1:24:18<1:05:18,  8.89s/it, lr=0.0001, step_loss=0.0346]\n",
      "Steps:  56%|█████▌    | 560/1000 [1:24:27<1:05:22,  8.92s/it, lr=0.0001, step_loss=0.0346]\n",
      "Steps:  56%|█████▌    | 560/1000 [1:24:27<1:05:22,  8.92s/it, lr=0.0001, step_loss=0.0164]\n",
      "Steps:  56%|█████▌    | 561/1000 [1:24:36<1:05:01,  8.89s/it, lr=0.0001, step_loss=0.0164]\n",
      "Steps:  56%|█████▌    | 561/1000 [1:24:36<1:05:01,  8.89s/it, lr=0.0001, step_loss=0.0057]\n",
      "Steps:  56%|█████▌    | 562/1000 [1:24:45<1:05:23,  8.96s/it, lr=0.0001, step_loss=0.0057]\n",
      "Steps:  56%|█████▌    | 562/1000 [1:24:45<1:05:23,  8.96s/it, lr=0.0001, step_loss=0.0349]\n",
      "Steps:  56%|█████▋    | 563/1000 [1:24:54<1:04:43,  8.89s/it, lr=0.0001, step_loss=0.0349]\n",
      "Steps:  56%|█████▋    | 563/1000 [1:24:54<1:04:43,  8.89s/it, lr=0.0001, step_loss=0.016]\n",
      "Steps:  56%|█████▋    | 564/1000 [1:25:03<1:04:49,  8.92s/it, lr=0.0001, step_loss=0.016]\n",
      "Steps:  56%|█████▋    | 564/1000 [1:25:03<1:04:49,  8.92s/it, lr=0.0001, step_loss=0.0137]\n",
      "Steps:  56%|█████▋    | 565/1000 [1:25:12<1:04:25,  8.89s/it, lr=0.0001, step_loss=0.0137]\n",
      "Steps:  56%|█████▋    | 565/1000 [1:25:12<1:04:25,  8.89s/it, lr=0.0001, step_loss=0.00762]\n",
      "Steps:  57%|█████▋    | 566/1000 [1:25:21<1:04:47,  8.96s/it, lr=0.0001, step_loss=0.00762]\n",
      "Steps:  57%|█████▋    | 566/1000 [1:25:21<1:04:47,  8.96s/it, lr=0.0001, step_loss=0.00429]\n",
      "Steps:  57%|█████▋    | 567/1000 [1:25:29<1:04:06,  8.88s/it, lr=0.0001, step_loss=0.00429]\n",
      "Steps:  57%|█████▋    | 567/1000 [1:25:29<1:04:06,  8.88s/it, lr=0.0001, step_loss=0.00737]\n",
      "Steps:  57%|█████▋    | 568/1000 [1:25:38<1:04:14,  8.92s/it, lr=0.0001, step_loss=0.00737]\n",
      "Steps:  57%|█████▋    | 568/1000 [1:25:38<1:04:14,  8.92s/it, lr=0.0001, step_loss=0.00988]\n",
      "Steps:  57%|█████▋    | 569/1000 [1:25:47<1:03:49,  8.88s/it, lr=0.0001, step_loss=0.00988]\n",
      "Steps:  57%|█████▋    | 569/1000 [1:25:47<1:03:49,  8.88s/it, lr=0.0001, step_loss=0.00558]\n",
      "Steps:  57%|█████▋    | 570/1000 [1:25:56<1:04:09,  8.95s/it, lr=0.0001, step_loss=0.00558]\n",
      "Steps:  57%|█████▋    | 570/1000 [1:25:56<1:04:09,  8.95s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  57%|█████▋    | 571/1000 [1:26:05<1:03:32,  8.89s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  57%|█████▋    | 571/1000 [1:26:05<1:03:32,  8.89s/it, lr=0.0001, step_loss=0.0185]\n",
      "Steps:  57%|█████▋    | 572/1000 [1:26:14<1:03:38,  8.92s/it, lr=0.0001, step_loss=0.0185]\n",
      "Steps:  57%|█████▋    | 572/1000 [1:26:14<1:03:38,  8.92s/it, lr=0.0001, step_loss=0.00869]\n",
      "Steps:  57%|█████▋    | 573/1000 [1:26:23<1:03:19,  8.90s/it, lr=0.0001, step_loss=0.00869]\n",
      "Steps:  57%|█████▋    | 573/1000 [1:26:23<1:03:19,  8.90s/it, lr=0.0001, step_loss=0.00315]\n",
      "Steps:  57%|█████▋    | 574/1000 [1:26:32<1:03:38,  8.96s/it, lr=0.0001, step_loss=0.00315]\n",
      "Steps:  57%|█████▋    | 574/1000 [1:26:32<1:03:38,  8.96s/it, lr=0.0001, step_loss=0.144]\n",
      "Steps:  57%|█████▊    | 575/1000 [1:26:41<1:02:57,  8.89s/it, lr=0.0001, step_loss=0.144]\n",
      "Steps:  57%|█████▊    | 575/1000 [1:26:41<1:02:57,  8.89s/it, lr=0.0001, step_loss=0.0103]\n",
      "Steps:  58%|█████▊    | 576/1000 [1:26:50<1:03:04,  8.92s/it, lr=0.0001, step_loss=0.0103]\n",
      "Steps:  58%|█████▊    | 576/1000 [1:26:50<1:03:04,  8.92s/it, lr=0.0001, step_loss=0.0127]\n",
      "Steps:  58%|█████▊    | 577/1000 [1:26:59<1:02:41,  8.89s/it, lr=0.0001, step_loss=0.0127]\n",
      "Steps:  58%|█████▊    | 577/1000 [1:26:59<1:02:41,  8.89s/it, lr=0.0001, step_loss=0.00299]\n",
      "Steps:  58%|█████▊    | 578/1000 [1:27:08<1:02:59,  8.96s/it, lr=0.0001, step_loss=0.00299]\n",
      "Steps:  58%|█████▊    | 578/1000 [1:27:08<1:02:59,  8.96s/it, lr=0.0001, step_loss=0.00957]\n",
      "Steps:  58%|█████▊    | 579/1000 [1:27:16<1:02:23,  8.89s/it, lr=0.0001, step_loss=0.00957]\n",
      "Steps:  58%|█████▊    | 579/1000 [1:27:16<1:02:23,  8.89s/it, lr=0.0001, step_loss=0.0105]\n",
      "Steps:  58%|█████▊    | 580/1000 [1:27:25<1:02:28,  8.92s/it, lr=0.0001, step_loss=0.0105]\n",
      "Steps:  58%|█████▊    | 580/1000 [1:27:25<1:02:28,  8.92s/it, lr=0.0001, step_loss=0.00851]\n",
      "Steps:  58%|█████▊    | 581/1000 [1:27:34<1:02:04,  8.89s/it, lr=0.0001, step_loss=0.00851]\n",
      "Steps:  58%|█████▊    | 581/1000 [1:27:34<1:02:04,  8.89s/it, lr=0.0001, step_loss=0.00547]\n",
      "Steps:  58%|█████▊    | 582/1000 [1:27:43<1:02:27,  8.97s/it, lr=0.0001, step_loss=0.00547]\n",
      "Steps:  58%|█████▊    | 582/1000 [1:27:43<1:02:27,  8.97s/it, lr=0.0001, step_loss=0.00368]\n",
      "Steps:  58%|█████▊    | 583/1000 [1:27:52<1:01:47,  8.89s/it, lr=0.0001, step_loss=0.00368]\n",
      "Steps:  58%|█████▊    | 583/1000 [1:27:52<1:01:47,  8.89s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  58%|█████▊    | 584/1000 [1:28:01<1:01:52,  8.92s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  58%|█████▊    | 584/1000 [1:28:01<1:01:52,  8.92s/it, lr=0.0001, step_loss=0.0104]\n",
      "Steps:  58%|█████▊    | 585/1000 [1:28:10<1:01:28,  8.89s/it, lr=0.0001, step_loss=0.0104]\n",
      "Steps:  58%|█████▊    | 585/1000 [1:28:10<1:01:28,  8.89s/it, lr=0.0001, step_loss=0.00704]\n",
      "Steps:  59%|█████▊    | 586/1000 [1:28:19<1:01:48,  8.96s/it, lr=0.0001, step_loss=0.00704]\n",
      "Steps:  59%|█████▊    | 586/1000 [1:28:19<1:01:48,  8.96s/it, lr=0.0001, step_loss=0.0065]\n",
      "Steps:  59%|█████▊    | 587/1000 [1:28:28<1:01:07,  8.88s/it, lr=0.0001, step_loss=0.0065]\n",
      "Steps:  59%|█████▊    | 587/1000 [1:28:28<1:01:07,  8.88s/it, lr=0.0001, step_loss=0.00332]\n",
      "Steps:  59%|█████▉    | 588/1000 [1:28:37<1:01:15,  8.92s/it, lr=0.0001, step_loss=0.00332]\n",
      "Steps:  59%|█████▉    | 588/1000 [1:28:37<1:01:15,  8.92s/it, lr=0.0001, step_loss=0.0049]\n",
      "Steps:  59%|█████▉    | 589/1000 [1:28:45<1:00:52,  8.89s/it, lr=0.0001, step_loss=0.0049]\n",
      "Steps:  59%|█████▉    | 589/1000 [1:28:45<1:00:52,  8.89s/it, lr=0.0001, step_loss=0.0123]\n",
      "Steps:  59%|█████▉    | 590/1000 [1:28:55<1:01:14,  8.96s/it, lr=0.0001, step_loss=0.0123]\n",
      "Steps:  59%|█████▉    | 590/1000 [1:28:55<1:01:14,  8.96s/it, lr=0.0001, step_loss=0.0163]\n",
      "Steps:  59%|█████▉    | 591/1000 [1:29:03<1:00:32,  8.88s/it, lr=0.0001, step_loss=0.0163]\n",
      "Steps:  59%|█████▉    | 591/1000 [1:29:03<1:00:32,  8.88s/it, lr=0.0001, step_loss=0.00544]\n",
      "Steps:  59%|█████▉    | 592/1000 [1:29:12<1:00:39,  8.92s/it, lr=0.0001, step_loss=0.00544]\n",
      "Steps:  59%|█████▉    | 592/1000 [1:29:12<1:00:39,  8.92s/it, lr=0.0001, step_loss=0.00928]\n",
      "Steps:  59%|█████▉    | 593/1000 [1:29:21<1:00:17,  8.89s/it, lr=0.0001, step_loss=0.00928]\n",
      "Steps:  59%|█████▉    | 593/1000 [1:29:21<1:00:17,  8.89s/it, lr=0.0001, step_loss=0.0046]\n",
      "Steps:  59%|█████▉    | 594/1000 [1:29:30<1:00:38,  8.96s/it, lr=0.0001, step_loss=0.0046]\n",
      "Steps:  59%|█████▉    | 594/1000 [1:29:30<1:00:38,  8.96s/it, lr=0.0001, step_loss=0.0145]\n",
      "Steps:  60%|█████▉    | 595/1000 [1:29:39<59:59,  8.89s/it, lr=0.0001, step_loss=0.0145]\n",
      "Steps:  60%|█████▉    | 595/1000 [1:29:39<59:59,  8.89s/it, lr=0.0001, step_loss=0.00875]\n",
      "Steps:  60%|█████▉    | 596/1000 [1:29:48<1:00:05,  8.92s/it, lr=0.0001, step_loss=0.00875]\n",
      "Steps:  60%|█████▉    | 596/1000 [1:29:48<1:00:05,  8.92s/it, lr=0.0001, step_loss=0.00673]\n",
      "Steps:  60%|█████▉    | 597/1000 [1:29:57<59:43,  8.89s/it, lr=0.0001, step_loss=0.00673]\n",
      "Steps:  60%|█████▉    | 597/1000 [1:29:57<59:43,  8.89s/it, lr=0.0001, step_loss=0.00448]\n",
      "Steps:  60%|█████▉    | 598/1000 [1:30:06<59:59,  8.95s/it, lr=0.0001, step_loss=0.00448]\n",
      "Steps:  60%|█████▉    | 598/1000 [1:30:06<59:59,  8.95s/it, lr=0.0001, step_loss=0.0134]\n",
      "Steps:  60%|█████▉    | 599/1000 [1:30:15<59:24,  8.89s/it, lr=0.0001, step_loss=0.0134]\n",
      "Steps:  60%|█████▉    | 599/1000 [1:30:15<59:24,  8.89s/it, lr=0.0001, step_loss=0.00987]\n",
      "Steps:  60%|██████    | 600/1000 [1:30:24<59:28,  8.92s/it, lr=0.0001, step_loss=0.00987]04/17/2024 05:25:30 - INFO - accelerate.accelerator - Saving current state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-600\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-600\\unet_ema\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-600/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-600\\unet\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-600/unet/diffusion_pytorch_model.safetensors\n",
      "04/17/2024 05:26:06 - INFO - accelerate.checkpointing - Optimizer state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-600\\optimizer.bin\n",
      "04/17/2024 05:26:06 - INFO - accelerate.checkpointing - Scheduler state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-600\\scheduler.bin\n",
      "04/17/2024 05:26:06 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-600\\sampler.bin\n",
      "04/17/2024 05:26:06 - INFO - accelerate.checkpointing - Random states saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-600\\random_states_0.pkl\n",
      "04/17/2024 05:26:06 - INFO - __main__ - Saved state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-600\n",
      "\n",
      "Steps:  60%|██████    | 600/1000 [1:31:00<59:28,  8.92s/it, lr=0.0001, step_loss=0.0127]\n",
      "Steps:  60%|██████    | 601/1000 [1:31:09<2:12:15, 19.89s/it, lr=0.0001, step_loss=0.0127]\n",
      "Steps:  60%|██████    | 601/1000 [1:31:09<2:12:15, 19.89s/it, lr=0.0001, step_loss=0.00277]\n",
      "Steps:  60%|██████    | 602/1000 [1:31:18<1:50:30, 16.66s/it, lr=0.0001, step_loss=0.00277]\n",
      "Steps:  60%|██████    | 602/1000 [1:31:18<1:50:30, 16.66s/it, lr=0.0001, step_loss=0.00117]\n",
      "Steps:  60%|██████    | 603/1000 [1:31:27<1:34:26, 14.27s/it, lr=0.0001, step_loss=0.00117]\n",
      "Steps:  60%|██████    | 603/1000 [1:31:27<1:34:26, 14.27s/it, lr=0.0001, step_loss=0.0121]\n",
      "Steps:  60%|██████    | 604/1000 [1:31:36<1:23:46, 12.69s/it, lr=0.0001, step_loss=0.0121]\n",
      "Steps:  60%|██████    | 604/1000 [1:31:36<1:23:46, 12.69s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  60%|██████    | 605/1000 [1:31:45<1:15:56, 11.54s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  60%|██████    | 605/1000 [1:31:45<1:15:56, 11.54s/it, lr=0.0001, step_loss=0.00245]\n",
      "Steps:  61%|██████    | 606/1000 [1:31:54<1:10:57, 10.81s/it, lr=0.0001, step_loss=0.00245]\n",
      "Steps:  61%|██████    | 606/1000 [1:31:54<1:10:57, 10.81s/it, lr=0.0001, step_loss=0.00749]\n",
      "Steps:  61%|██████    | 607/1000 [1:32:03<1:06:40, 10.18s/it, lr=0.0001, step_loss=0.00749]\n",
      "Steps:  61%|██████    | 607/1000 [1:32:03<1:06:40, 10.18s/it, lr=0.0001, step_loss=0.012]\n",
      "Steps:  61%|██████    | 608/1000 [1:32:12<1:04:11,  9.83s/it, lr=0.0001, step_loss=0.012]\n",
      "Steps:  61%|██████    | 608/1000 [1:32:12<1:04:11,  9.83s/it, lr=0.0001, step_loss=0.00688]\n",
      "Steps:  61%|██████    | 609/1000 [1:32:20<1:02:02,  9.52s/it, lr=0.0001, step_loss=0.00688]\n",
      "Steps:  61%|██████    | 609/1000 [1:32:20<1:02:02,  9.52s/it, lr=0.0001, step_loss=0.00565]\n",
      "Steps:  61%|██████    | 610/1000 [1:32:30<1:01:05,  9.40s/it, lr=0.0001, step_loss=0.00565]\n",
      "Steps:  61%|██████    | 610/1000 [1:32:30<1:01:05,  9.40s/it, lr=0.0001, step_loss=0.00761]\n",
      "Steps:  61%|██████    | 611/1000 [1:32:38<59:36,  9.19s/it, lr=0.0001, step_loss=0.00761]\n",
      "Steps:  61%|██████    | 611/1000 [1:32:38<59:36,  9.19s/it, lr=0.0001, step_loss=0.00391]\n",
      "Steps:  61%|██████    | 612/1000 [1:32:47<59:04,  9.14s/it, lr=0.0001, step_loss=0.00391]\n",
      "Steps:  61%|██████    | 612/1000 [1:32:47<59:04,  9.14s/it, lr=0.0001, step_loss=0.00387]\n",
      "Steps:  61%|██████▏   | 613/1000 [1:32:56<58:18,  9.04s/it, lr=0.0001, step_loss=0.00387]\n",
      "Steps:  61%|██████▏   | 613/1000 [1:32:56<58:18,  9.04s/it, lr=0.0001, step_loss=0.00452]\n",
      "Steps:  61%|██████▏   | 614/1000 [1:33:05<58:16,  9.06s/it, lr=0.0001, step_loss=0.00452]\n",
      "Steps:  61%|██████▏   | 614/1000 [1:33:05<58:16,  9.06s/it, lr=0.0001, step_loss=0.0145]\n",
      "Steps:  62%|██████▏   | 615/1000 [1:33:14<57:31,  8.96s/it, lr=0.0001, step_loss=0.0145]\n",
      "Steps:  62%|██████▏   | 615/1000 [1:33:14<57:31,  8.96s/it, lr=0.0001, step_loss=0.00312]\n",
      "Steps:  62%|██████▏   | 616/1000 [1:33:23<57:25,  8.97s/it, lr=0.0001, step_loss=0.00312]\n",
      "Steps:  62%|██████▏   | 616/1000 [1:33:23<57:25,  8.97s/it, lr=0.0001, step_loss=0.00458]\n",
      "Steps:  62%|██████▏   | 617/1000 [1:33:32<56:55,  8.92s/it, lr=0.0001, step_loss=0.00458]\n",
      "Steps:  62%|██████▏   | 617/1000 [1:33:32<56:55,  8.92s/it, lr=0.0001, step_loss=0.0302]\n",
      "Steps:  62%|██████▏   | 618/1000 [1:33:41<57:09,  8.98s/it, lr=0.0001, step_loss=0.0302]\n",
      "Steps:  62%|██████▏   | 618/1000 [1:33:41<57:09,  8.98s/it, lr=0.0001, step_loss=0.0116]\n",
      "Steps:  62%|██████▏   | 619/1000 [1:33:50<56:33,  8.91s/it, lr=0.0001, step_loss=0.0116]\n",
      "Steps:  62%|██████▏   | 619/1000 [1:33:50<56:33,  8.91s/it, lr=0.0001, step_loss=0.00716]\n",
      "Steps:  62%|██████▏   | 620/1000 [1:33:59<56:35,  8.94s/it, lr=0.0001, step_loss=0.00716]\n",
      "Steps:  62%|██████▏   | 620/1000 [1:33:59<56:35,  8.94s/it, lr=0.0001, step_loss=0.00676]\n",
      "Steps:  62%|██████▏   | 621/1000 [1:34:07<56:13,  8.90s/it, lr=0.0001, step_loss=0.00676]\n",
      "Steps:  62%|██████▏   | 621/1000 [1:34:07<56:13,  8.90s/it, lr=0.0001, step_loss=0.00693]\n",
      "Steps:  62%|██████▏   | 622/1000 [1:34:16<56:28,  8.97s/it, lr=0.0001, step_loss=0.00693]\n",
      "Steps:  62%|██████▏   | 622/1000 [1:34:16<56:28,  8.97s/it, lr=0.0001, step_loss=0.0107]\n",
      "Steps:  62%|██████▏   | 623/1000 [1:34:25<55:52,  8.89s/it, lr=0.0001, step_loss=0.0107]\n",
      "Steps:  62%|██████▏   | 623/1000 [1:34:25<55:52,  8.89s/it, lr=0.0001, step_loss=0.00806]\n",
      "Steps:  62%|██████▏   | 624/1000 [1:34:34<55:58,  8.93s/it, lr=0.0001, step_loss=0.00806]\n",
      "Steps:  62%|██████▏   | 624/1000 [1:34:34<55:58,  8.93s/it, lr=0.0001, step_loss=0.00655]\n",
      "Steps:  62%|██████▎   | 625/1000 [1:34:43<55:33,  8.89s/it, lr=0.0001, step_loss=0.00655]\n",
      "Steps:  62%|██████▎   | 625/1000 [1:34:43<55:33,  8.89s/it, lr=0.0001, step_loss=0.0309]\n",
      "Steps:  63%|██████▎   | 626/1000 [1:34:52<55:52,  8.97s/it, lr=0.0001, step_loss=0.0309]\n",
      "Steps:  63%|██████▎   | 626/1000 [1:34:52<55:52,  8.97s/it, lr=0.0001, step_loss=0.00363]\n",
      "Steps:  63%|██████▎   | 627/1000 [1:35:01<55:16,  8.89s/it, lr=0.0001, step_loss=0.00363]\n",
      "Steps:  63%|██████▎   | 627/1000 [1:35:01<55:16,  8.89s/it, lr=0.0001, step_loss=0.00922]\n",
      "Steps:  63%|██████▎   | 628/1000 [1:35:10<55:20,  8.93s/it, lr=0.0001, step_loss=0.00922]\n",
      "Steps:  63%|██████▎   | 628/1000 [1:35:10<55:20,  8.93s/it, lr=0.0001, step_loss=0.00512]\n",
      "Steps:  63%|██████▎   | 629/1000 [1:35:19<54:59,  8.89s/it, lr=0.0001, step_loss=0.00512]\n",
      "Steps:  63%|██████▎   | 629/1000 [1:35:19<54:59,  8.89s/it, lr=0.0001, step_loss=0.003]\n",
      "Steps:  63%|██████▎   | 630/1000 [1:35:28<55:17,  8.97s/it, lr=0.0001, step_loss=0.003]\n",
      "Steps:  63%|██████▎   | 630/1000 [1:35:28<55:17,  8.97s/it, lr=0.0001, step_loss=0.0415]\n",
      "Steps:  63%|██████▎   | 631/1000 [1:35:37<54:38,  8.88s/it, lr=0.0001, step_loss=0.0415]\n",
      "Steps:  63%|██████▎   | 631/1000 [1:35:37<54:38,  8.88s/it, lr=0.0001, step_loss=0.00715]\n",
      "Steps:  63%|██████▎   | 632/1000 [1:35:46<54:43,  8.92s/it, lr=0.0001, step_loss=0.00715]\n",
      "Steps:  63%|██████▎   | 632/1000 [1:35:46<54:43,  8.92s/it, lr=0.0001, step_loss=0.00404]\n",
      "Steps:  63%|██████▎   | 633/1000 [1:35:54<54:24,  8.90s/it, lr=0.0001, step_loss=0.00404]\n",
      "Steps:  63%|██████▎   | 633/1000 [1:35:54<54:24,  8.90s/it, lr=0.0001, step_loss=0.00434]\n",
      "Steps:  63%|██████▎   | 634/1000 [1:36:03<54:37,  8.96s/it, lr=0.0001, step_loss=0.00434]\n",
      "Steps:  63%|██████▎   | 634/1000 [1:36:03<54:37,  8.96s/it, lr=0.0001, step_loss=0.0325]\n",
      "Steps:  64%|██████▎   | 635/1000 [1:36:12<54:03,  8.89s/it, lr=0.0001, step_loss=0.0325]\n",
      "Steps:  64%|██████▎   | 635/1000 [1:36:12<54:03,  8.89s/it, lr=0.0001, step_loss=0.00586]\n",
      "Steps:  64%|██████▎   | 636/1000 [1:36:21<54:08,  8.92s/it, lr=0.0001, step_loss=0.00586]\n",
      "Steps:  64%|██████▎   | 636/1000 [1:36:21<54:08,  8.92s/it, lr=0.0001, step_loss=0.00957]\n",
      "Steps:  64%|██████▎   | 637/1000 [1:36:30<53:46,  8.89s/it, lr=0.0001, step_loss=0.00957]\n",
      "Steps:  64%|██████▎   | 637/1000 [1:36:30<53:46,  8.89s/it, lr=0.0001, step_loss=0.00731]\n",
      "Steps:  64%|██████▍   | 638/1000 [1:36:39<54:02,  8.96s/it, lr=0.0001, step_loss=0.00731]\n",
      "Steps:  64%|██████▍   | 638/1000 [1:36:39<54:02,  8.96s/it, lr=0.0001, step_loss=0.0229]\n",
      "Steps:  64%|██████▍   | 639/1000 [1:36:48<53:28,  8.89s/it, lr=0.0001, step_loss=0.0229]\n",
      "Steps:  64%|██████▍   | 639/1000 [1:36:48<53:28,  8.89s/it, lr=0.0001, step_loss=0.00851]\n",
      "Steps:  64%|██████▍   | 640/1000 [1:36:57<53:34,  8.93s/it, lr=0.0001, step_loss=0.00851]\n",
      "Steps:  64%|██████▍   | 640/1000 [1:36:57<53:34,  8.93s/it, lr=0.0001, step_loss=0.0125]\n",
      "Steps:  64%|██████▍   | 641/1000 [1:37:06<53:13,  8.90s/it, lr=0.0001, step_loss=0.0125]\n",
      "Steps:  64%|██████▍   | 641/1000 [1:37:06<53:13,  8.90s/it, lr=0.0001, step_loss=0.00439]\n",
      "Steps:  64%|██████▍   | 642/1000 [1:37:15<53:26,  8.96s/it, lr=0.0001, step_loss=0.00439]\n",
      "Steps:  64%|██████▍   | 642/1000 [1:37:15<53:26,  8.96s/it, lr=0.0001, step_loss=0.0151]\n",
      "Steps:  64%|██████▍   | 643/1000 [1:37:24<52:54,  8.89s/it, lr=0.0001, step_loss=0.0151]\n",
      "Steps:  64%|██████▍   | 643/1000 [1:37:24<52:54,  8.89s/it, lr=0.0001, step_loss=0.00829]\n",
      "Steps:  64%|██████▍   | 644/1000 [1:37:33<52:56,  8.92s/it, lr=0.0001, step_loss=0.00829]\n",
      "Steps:  64%|██████▍   | 644/1000 [1:37:33<52:56,  8.92s/it, lr=0.0001, step_loss=0.00372]\n",
      "Steps:  64%|██████▍   | 645/1000 [1:37:41<52:34,  8.89s/it, lr=0.0001, step_loss=0.00372]\n",
      "Steps:  64%|██████▍   | 645/1000 [1:37:41<52:34,  8.89s/it, lr=0.0001, step_loss=0.00735]\n",
      "Steps:  65%|██████▍   | 646/1000 [1:37:50<52:52,  8.96s/it, lr=0.0001, step_loss=0.00735]\n",
      "Steps:  65%|██████▍   | 646/1000 [1:37:50<52:52,  8.96s/it, lr=0.0001, step_loss=0.00522]\n",
      "Steps:  65%|██████▍   | 647/1000 [1:37:59<52:14,  8.88s/it, lr=0.0001, step_loss=0.00522]\n",
      "Steps:  65%|██████▍   | 647/1000 [1:37:59<52:14,  8.88s/it, lr=0.0001, step_loss=0.005]\n",
      "Steps:  65%|██████▍   | 648/1000 [1:38:08<52:19,  8.92s/it, lr=0.0001, step_loss=0.005]\n",
      "Steps:  65%|██████▍   | 648/1000 [1:38:08<52:19,  8.92s/it, lr=0.0001, step_loss=0.162]\n",
      "Steps:  65%|██████▍   | 649/1000 [1:38:17<51:59,  8.89s/it, lr=0.0001, step_loss=0.162]\n",
      "Steps:  65%|██████▍   | 649/1000 [1:38:17<51:59,  8.89s/it, lr=0.0001, step_loss=0.00353]\n",
      "Steps:  65%|██████▌   | 650/1000 [1:38:26<52:15,  8.96s/it, lr=0.0001, step_loss=0.00353]\n",
      "Steps:  65%|██████▌   | 650/1000 [1:38:26<52:15,  8.96s/it, lr=0.0001, step_loss=0.0262]\n",
      "Steps:  65%|██████▌   | 651/1000 [1:38:35<51:40,  8.88s/it, lr=0.0001, step_loss=0.0262]\n",
      "Steps:  65%|██████▌   | 651/1000 [1:38:35<51:40,  8.88s/it, lr=0.0001, step_loss=0.0343]\n",
      "Steps:  65%|██████▌   | 652/1000 [1:38:44<51:43,  8.92s/it, lr=0.0001, step_loss=0.0343]\n",
      "Steps:  65%|██████▌   | 652/1000 [1:38:44<51:43,  8.92s/it, lr=0.0001, step_loss=0.00487]\n",
      "Steps:  65%|██████▌   | 653/1000 [1:38:53<51:24,  8.89s/it, lr=0.0001, step_loss=0.00487]\n",
      "Steps:  65%|██████▌   | 653/1000 [1:38:53<51:24,  8.89s/it, lr=0.0001, step_loss=0.0325]\n",
      "Steps:  65%|██████▌   | 654/1000 [1:39:02<51:39,  8.96s/it, lr=0.0001, step_loss=0.0325]\n",
      "Steps:  65%|██████▌   | 654/1000 [1:39:02<51:39,  8.96s/it, lr=0.0001, step_loss=0.0153]\n",
      "Steps:  66%|██████▌   | 655/1000 [1:39:10<51:05,  8.88s/it, lr=0.0001, step_loss=0.0153]\n",
      "Steps:  66%|██████▌   | 655/1000 [1:39:10<51:05,  8.88s/it, lr=0.0001, step_loss=0.00443]\n",
      "Steps:  66%|██████▌   | 656/1000 [1:39:19<51:08,  8.92s/it, lr=0.0001, step_loss=0.00443]\n",
      "Steps:  66%|██████▌   | 656/1000 [1:39:20<51:08,  8.92s/it, lr=0.0001, step_loss=0.0132]\n",
      "Steps:  66%|██████▌   | 657/1000 [1:39:28<50:47,  8.88s/it, lr=0.0001, step_loss=0.0132]\n",
      "Steps:  66%|██████▌   | 657/1000 [1:39:28<50:47,  8.88s/it, lr=0.0001, step_loss=0.00765]\n",
      "Steps:  66%|██████▌   | 658/1000 [1:39:37<51:04,  8.96s/it, lr=0.0001, step_loss=0.00765]\n",
      "Steps:  66%|██████▌   | 658/1000 [1:39:37<51:04,  8.96s/it, lr=0.0001, step_loss=0.0163]\n",
      "Steps:  66%|██████▌   | 659/1000 [1:39:46<50:29,  8.88s/it, lr=0.0001, step_loss=0.0163]\n",
      "Steps:  66%|██████▌   | 659/1000 [1:39:46<50:29,  8.88s/it, lr=0.0001, step_loss=0.00551]\n",
      "Steps:  66%|██████▌   | 660/1000 [1:39:55<50:33,  8.92s/it, lr=0.0001, step_loss=0.00551]\n",
      "Steps:  66%|██████▌   | 660/1000 [1:39:55<50:33,  8.92s/it, lr=0.0001, step_loss=0.0225]\n",
      "Steps:  66%|██████▌   | 661/1000 [1:40:04<50:13,  8.89s/it, lr=0.0001, step_loss=0.0225]\n",
      "Steps:  66%|██████▌   | 661/1000 [1:40:04<50:13,  8.89s/it, lr=0.0001, step_loss=0.00896]\n",
      "Steps:  66%|██████▌   | 662/1000 [1:40:13<50:26,  8.95s/it, lr=0.0001, step_loss=0.00896]\n",
      "Steps:  66%|██████▌   | 662/1000 [1:40:13<50:26,  8.95s/it, lr=0.0001, step_loss=0.00765]\n",
      "Steps:  66%|██████▋   | 663/1000 [1:40:22<49:54,  8.89s/it, lr=0.0001, step_loss=0.00765]\n",
      "Steps:  66%|██████▋   | 663/1000 [1:40:22<49:54,  8.89s/it, lr=0.0001, step_loss=0.00898]\n",
      "Steps:  66%|██████▋   | 664/1000 [1:40:31<49:56,  8.92s/it, lr=0.0001, step_loss=0.00898]\n",
      "Steps:  66%|██████▋   | 664/1000 [1:40:31<49:56,  8.92s/it, lr=0.0001, step_loss=0.0133]\n",
      "Steps:  66%|██████▋   | 665/1000 [1:40:40<49:39,  8.89s/it, lr=0.0001, step_loss=0.0133]\n",
      "Steps:  66%|██████▋   | 665/1000 [1:40:40<49:39,  8.89s/it, lr=0.0001, step_loss=0.00436]\n",
      "Steps:  67%|██████▋   | 666/1000 [1:40:49<49:50,  8.95s/it, lr=0.0001, step_loss=0.00436]\n",
      "Steps:  67%|██████▋   | 666/1000 [1:40:49<49:50,  8.95s/it, lr=0.0001, step_loss=0.0117]\n",
      "Steps:  67%|██████▋   | 667/1000 [1:40:57<49:16,  8.88s/it, lr=0.0001, step_loss=0.0117]\n",
      "Steps:  67%|██████▋   | 667/1000 [1:40:57<49:16,  8.88s/it, lr=0.0001, step_loss=0.0183]\n",
      "Steps:  67%|██████▋   | 668/1000 [1:41:06<49:21,  8.92s/it, lr=0.0001, step_loss=0.0183]\n",
      "Steps:  67%|██████▋   | 668/1000 [1:41:06<49:21,  8.92s/it, lr=0.0001, step_loss=0.00543]\n",
      "Steps:  67%|██████▋   | 669/1000 [1:41:15<49:03,  8.89s/it, lr=0.0001, step_loss=0.00543]\n",
      "Steps:  67%|██████▋   | 669/1000 [1:41:15<49:03,  8.89s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  67%|██████▋   | 670/1000 [1:41:24<49:15,  8.96s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  67%|██████▋   | 670/1000 [1:41:24<49:15,  8.96s/it, lr=0.0001, step_loss=0.0057]\n",
      "Steps:  67%|██████▋   | 671/1000 [1:41:33<48:43,  8.89s/it, lr=0.0001, step_loss=0.0057]\n",
      "Steps:  67%|██████▋   | 671/1000 [1:41:33<48:43,  8.89s/it, lr=0.0001, step_loss=0.0181]\n",
      "Steps:  67%|██████▋   | 672/1000 [1:41:42<48:46,  8.92s/it, lr=0.0001, step_loss=0.0181]\n",
      "Steps:  67%|██████▋   | 672/1000 [1:41:42<48:46,  8.92s/it, lr=0.0001, step_loss=0.021]\n",
      "Steps:  67%|██████▋   | 673/1000 [1:41:51<48:27,  8.89s/it, lr=0.0001, step_loss=0.021]\n",
      "Steps:  67%|██████▋   | 673/1000 [1:41:51<48:27,  8.89s/it, lr=0.0001, step_loss=0.0146]\n",
      "Steps:  67%|██████▋   | 674/1000 [1:42:00<48:36,  8.95s/it, lr=0.0001, step_loss=0.0146]\n",
      "Steps:  67%|██████▋   | 674/1000 [1:42:00<48:36,  8.95s/it, lr=0.0001, step_loss=0.00299]\n",
      "Steps:  68%|██████▊   | 675/1000 [1:42:09<48:09,  8.89s/it, lr=0.0001, step_loss=0.00299]\n",
      "Steps:  68%|██████▊   | 675/1000 [1:42:09<48:09,  8.89s/it, lr=0.0001, step_loss=0.00473]\n",
      "Steps:  68%|██████▊   | 676/1000 [1:42:18<48:11,  8.92s/it, lr=0.0001, step_loss=0.00473]\n",
      "Steps:  68%|██████▊   | 676/1000 [1:42:18<48:11,  8.92s/it, lr=0.0001, step_loss=0.00558]\n",
      "Steps:  68%|██████▊   | 677/1000 [1:42:27<47:50,  8.89s/it, lr=0.0001, step_loss=0.00558]\n",
      "Steps:  68%|██████▊   | 677/1000 [1:42:27<47:50,  8.89s/it, lr=0.0001, step_loss=0.00808]\n",
      "Steps:  68%|██████▊   | 678/1000 [1:42:36<48:03,  8.96s/it, lr=0.0001, step_loss=0.00808]\n",
      "Steps:  68%|██████▊   | 678/1000 [1:42:36<48:03,  8.96s/it, lr=0.0001, step_loss=0.00931]\n",
      "Steps:  68%|██████▊   | 679/1000 [1:42:44<47:32,  8.89s/it, lr=0.0001, step_loss=0.00931]\n",
      "Steps:  68%|██████▊   | 679/1000 [1:42:44<47:32,  8.89s/it, lr=0.0001, step_loss=0.00303]\n",
      "Steps:  68%|██████▊   | 680/1000 [1:42:53<47:34,  8.92s/it, lr=0.0001, step_loss=0.00303]\n",
      "Steps:  68%|██████▊   | 680/1000 [1:42:53<47:34,  8.92s/it, lr=0.0001, step_loss=0.0152]\n",
      "Steps:  68%|██████▊   | 681/1000 [1:43:02<47:19,  8.90s/it, lr=0.0001, step_loss=0.0152]\n",
      "Steps:  68%|██████▊   | 681/1000 [1:43:02<47:19,  8.90s/it, lr=0.0001, step_loss=0.00761]\n",
      "Steps:  68%|██████▊   | 682/1000 [1:43:11<47:27,  8.96s/it, lr=0.0001, step_loss=0.00761]\n",
      "Steps:  68%|██████▊   | 682/1000 [1:43:11<47:27,  8.96s/it, lr=0.0001, step_loss=0.00424]\n",
      "Steps:  68%|██████▊   | 683/1000 [1:43:20<46:55,  8.88s/it, lr=0.0001, step_loss=0.00424]\n",
      "Steps:  68%|██████▊   | 683/1000 [1:43:20<46:55,  8.88s/it, lr=0.0001, step_loss=0.00397]\n",
      "Steps:  68%|██████▊   | 684/1000 [1:43:29<46:59,  8.92s/it, lr=0.0001, step_loss=0.00397]\n",
      "Steps:  68%|██████▊   | 684/1000 [1:43:29<46:59,  8.92s/it, lr=0.0001, step_loss=0.00625]\n",
      "Steps:  68%|██████▊   | 685/1000 [1:43:38<46:38,  8.88s/it, lr=0.0001, step_loss=0.00625]\n",
      "Steps:  68%|██████▊   | 685/1000 [1:43:38<46:38,  8.88s/it, lr=0.0001, step_loss=0.0437]\n",
      "Steps:  69%|██████▊   | 686/1000 [1:43:47<46:52,  8.96s/it, lr=0.0001, step_loss=0.0437]\n",
      "Steps:  69%|██████▊   | 686/1000 [1:43:47<46:52,  8.96s/it, lr=0.0001, step_loss=0.135]\n",
      "Steps:  69%|██████▊   | 687/1000 [1:43:56<46:20,  8.88s/it, lr=0.0001, step_loss=0.135]\n",
      "Steps:  69%|██████▊   | 687/1000 [1:43:56<46:20,  8.88s/it, lr=0.0001, step_loss=0.00788]\n",
      "Steps:  69%|██████▉   | 688/1000 [1:44:05<46:22,  8.92s/it, lr=0.0001, step_loss=0.00788]\n",
      "Steps:  69%|██████▉   | 688/1000 [1:44:05<46:22,  8.92s/it, lr=0.0001, step_loss=0.00971]\n",
      "Steps:  69%|██████▉   | 689/1000 [1:44:13<46:02,  8.88s/it, lr=0.0001, step_loss=0.00971]\n",
      "Steps:  69%|██████▉   | 689/1000 [1:44:13<46:02,  8.88s/it, lr=0.0001, step_loss=0.00588]\n",
      "Steps:  69%|██████▉   | 690/1000 [1:44:23<46:17,  8.96s/it, lr=0.0001, step_loss=0.00588]\n",
      "Steps:  69%|██████▉   | 690/1000 [1:44:23<46:17,  8.96s/it, lr=0.0001, step_loss=0.00468]\n",
      "Steps:  69%|██████▉   | 691/1000 [1:44:31<45:45,  8.89s/it, lr=0.0001, step_loss=0.00468]\n",
      "Steps:  69%|██████▉   | 691/1000 [1:44:31<45:45,  8.89s/it, lr=0.0001, step_loss=0.00337]\n",
      "Steps:  69%|██████▉   | 692/1000 [1:44:40<45:48,  8.92s/it, lr=0.0001, step_loss=0.00337]\n",
      "Steps:  69%|██████▉   | 692/1000 [1:44:40<45:48,  8.92s/it, lr=0.0001, step_loss=0.0023]\n",
      "Steps:  69%|██████▉   | 693/1000 [1:44:49<45:30,  8.89s/it, lr=0.0001, step_loss=0.0023]\n",
      "Steps:  69%|██████▉   | 693/1000 [1:44:49<45:30,  8.89s/it, lr=0.0001, step_loss=0.00506]\n",
      "Steps:  69%|██████▉   | 694/1000 [1:44:58<45:41,  8.96s/it, lr=0.0001, step_loss=0.00506]\n",
      "Steps:  69%|██████▉   | 694/1000 [1:44:58<45:41,  8.96s/it, lr=0.0001, step_loss=0.0149]\n",
      "Steps:  70%|██████▉   | 695/1000 [1:45:07<45:13,  8.90s/it, lr=0.0001, step_loss=0.0149]\n",
      "Steps:  70%|██████▉   | 695/1000 [1:45:07<45:13,  8.90s/it, lr=0.0001, step_loss=0.00834]\n",
      "Steps:  70%|██████▉   | 696/1000 [1:45:16<45:13,  8.93s/it, lr=0.0001, step_loss=0.00834]\n",
      "Steps:  70%|██████▉   | 696/1000 [1:45:16<45:13,  8.93s/it, lr=0.0001, step_loss=0.00284]\n",
      "Steps:  70%|██████▉   | 697/1000 [1:45:25<44:55,  8.90s/it, lr=0.0001, step_loss=0.00284]\n",
      "Steps:  70%|██████▉   | 697/1000 [1:45:25<44:55,  8.90s/it, lr=0.0001, step_loss=0.0031]\n",
      "Steps:  70%|██████▉   | 698/1000 [1:45:34<45:06,  8.96s/it, lr=0.0001, step_loss=0.0031]\n",
      "Steps:  70%|██████▉   | 698/1000 [1:45:34<45:06,  8.96s/it, lr=0.0001, step_loss=0.00883]\n",
      "Steps:  70%|██████▉   | 699/1000 [1:45:43<44:37,  8.90s/it, lr=0.0001, step_loss=0.00883]\n",
      "Steps:  70%|██████▉   | 699/1000 [1:45:43<44:37,  8.90s/it, lr=0.0001, step_loss=0.00247]\n",
      "Steps:  70%|███████   | 700/1000 [1:45:52<44:38,  8.93s/it, lr=0.0001, step_loss=0.00247]04/17/2024 05:40:58 - INFO - accelerate.accelerator - Saving current state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-700\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-700\\unet_ema\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-700/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-700\\unet\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-700/unet/diffusion_pytorch_model.safetensors\n",
      "04/17/2024 05:41:35 - INFO - accelerate.checkpointing - Optimizer state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-700\\optimizer.bin\n",
      "04/17/2024 05:41:35 - INFO - accelerate.checkpointing - Scheduler state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-700\\scheduler.bin\n",
      "04/17/2024 05:41:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-700\\sampler.bin\n",
      "04/17/2024 05:41:35 - INFO - accelerate.checkpointing - Random states saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-700\\random_states_0.pkl\n",
      "04/17/2024 05:41:35 - INFO - __main__ - Saved state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-700\n",
      "\n",
      "Steps:  70%|███████   | 700/1000 [1:46:29<44:38,  8.93s/it, lr=0.0001, step_loss=0.0165]\n",
      "Steps:  70%|███████   | 701/1000 [1:46:38<1:40:01, 20.07s/it, lr=0.0001, step_loss=0.0165]\n",
      "Steps:  70%|███████   | 701/1000 [1:46:38<1:40:01, 20.07s/it, lr=0.0001, step_loss=0.00324]\n",
      "Steps:  70%|███████   | 702/1000 [1:46:47<1:23:20, 16.78s/it, lr=0.0001, step_loss=0.00324]\n",
      "Steps:  70%|███████   | 702/1000 [1:46:47<1:23:20, 16.78s/it, lr=0.0001, step_loss=0.0069]\n",
      "Steps:  70%|███████   | 703/1000 [1:46:56<1:11:06, 14.36s/it, lr=0.0001, step_loss=0.0069]\n",
      "Steps:  70%|███████   | 703/1000 [1:46:56<1:11:06, 14.36s/it, lr=0.0001, step_loss=0.00819]\n",
      "Steps:  70%|███████   | 704/1000 [1:47:05<1:02:55, 12.75s/it, lr=0.0001, step_loss=0.00819]\n",
      "Steps:  70%|███████   | 704/1000 [1:47:05<1:02:55, 12.75s/it, lr=0.0001, step_loss=0.00276]\n",
      "Steps:  70%|███████   | 705/1000 [1:47:13<56:53, 11.57s/it, lr=0.0001, step_loss=0.00276]\n",
      "Steps:  70%|███████   | 705/1000 [1:47:13<56:53, 11.57s/it, lr=0.0001, step_loss=0.00564]\n",
      "Steps:  71%|███████   | 706/1000 [1:47:23<53:05, 10.83s/it, lr=0.0001, step_loss=0.00564]\n",
      "Steps:  71%|███████   | 706/1000 [1:47:23<53:05, 10.83s/it, lr=0.0001, step_loss=0.00321]\n",
      "Steps:  71%|███████   | 707/1000 [1:47:31<49:49, 10.20s/it, lr=0.0001, step_loss=0.00321]\n",
      "Steps:  71%|███████   | 707/1000 [1:47:31<49:49, 10.20s/it, lr=0.0001, step_loss=0.000996]\n",
      "Steps:  71%|███████   | 708/1000 [1:47:40<47:54,  9.84s/it, lr=0.0001, step_loss=0.000996]\n",
      "Steps:  71%|███████   | 708/1000 [1:47:40<47:54,  9.84s/it, lr=0.0001, step_loss=0.00572]\n",
      "Steps:  71%|███████   | 709/1000 [1:47:49<46:14,  9.53s/it, lr=0.0001, step_loss=0.00572]\n",
      "Steps:  71%|███████   | 709/1000 [1:47:49<46:14,  9.53s/it, lr=0.0001, step_loss=0.00443]\n",
      "Steps:  71%|███████   | 710/1000 [1:47:58<45:29,  9.41s/it, lr=0.0001, step_loss=0.00443]\n",
      "Steps:  71%|███████   | 710/1000 [1:47:58<45:29,  9.41s/it, lr=0.0001, step_loss=0.0112]\n",
      "Steps:  71%|███████   | 711/1000 [1:48:07<44:19,  9.20s/it, lr=0.0001, step_loss=0.0112]\n",
      "Steps:  71%|███████   | 711/1000 [1:48:07<44:19,  9.20s/it, lr=0.0001, step_loss=0.00405]\n",
      "Steps:  71%|███████   | 712/1000 [1:48:16<43:53,  9.14s/it, lr=0.0001, step_loss=0.00405]\n",
      "Steps:  71%|███████   | 712/1000 [1:48:16<43:53,  9.14s/it, lr=0.0001, step_loss=0.00799]\n",
      "Steps:  71%|███████▏  | 713/1000 [1:48:25<43:13,  9.04s/it, lr=0.0001, step_loss=0.00799]\n",
      "Steps:  71%|███████▏  | 713/1000 [1:48:25<43:13,  9.04s/it, lr=0.0001, step_loss=0.00313]\n",
      "Steps:  71%|███████▏  | 714/1000 [1:48:34<43:15,  9.07s/it, lr=0.0001, step_loss=0.00313]\n",
      "Steps:  71%|███████▏  | 714/1000 [1:48:34<43:15,  9.07s/it, lr=0.0001, step_loss=0.0143]\n",
      "Steps:  72%|███████▏  | 715/1000 [1:48:43<42:34,  8.96s/it, lr=0.0001, step_loss=0.0143]\n",
      "Steps:  72%|███████▏  | 715/1000 [1:48:43<42:34,  8.96s/it, lr=0.0001, step_loss=0.0131]\n",
      "Steps:  72%|███████▏  | 716/1000 [1:48:52<42:29,  8.98s/it, lr=0.0001, step_loss=0.0131]\n",
      "Steps:  72%|███████▏  | 716/1000 [1:48:52<42:29,  8.98s/it, lr=0.0001, step_loss=0.00347]\n",
      "Steps:  72%|███████▏  | 717/1000 [1:49:00<42:05,  8.93s/it, lr=0.0001, step_loss=0.00347]\n",
      "Steps:  72%|███████▏  | 717/1000 [1:49:00<42:05,  8.93s/it, lr=0.0001, step_loss=0.0105]\n",
      "Steps:  72%|███████▏  | 718/1000 [1:49:10<42:14,  8.99s/it, lr=0.0001, step_loss=0.0105]\n",
      "Steps:  72%|███████▏  | 718/1000 [1:49:10<42:14,  8.99s/it, lr=0.0001, step_loss=0.0393]\n",
      "Steps:  72%|███████▏  | 719/1000 [1:49:18<41:43,  8.91s/it, lr=0.0001, step_loss=0.0393]\n",
      "Steps:  72%|███████▏  | 719/1000 [1:49:18<41:43,  8.91s/it, lr=0.0001, step_loss=0.0197]\n",
      "Steps:  72%|███████▏  | 720/1000 [1:49:27<41:41,  8.93s/it, lr=0.0001, step_loss=0.0197]\n",
      "Steps:  72%|███████▏  | 720/1000 [1:49:27<41:41,  8.93s/it, lr=0.0001, step_loss=0.0178]\n",
      "Steps:  72%|███████▏  | 721/1000 [1:49:36<41:25,  8.91s/it, lr=0.0001, step_loss=0.0178]\n",
      "Steps:  72%|███████▏  | 721/1000 [1:49:36<41:25,  8.91s/it, lr=0.0001, step_loss=0.000961]\n",
      "Steps:  72%|███████▏  | 722/1000 [1:49:45<41:31,  8.96s/it, lr=0.0001, step_loss=0.000961]\n",
      "Steps:  72%|███████▏  | 722/1000 [1:49:45<41:31,  8.96s/it, lr=0.0001, step_loss=0.00557]\n",
      "Steps:  72%|███████▏  | 723/1000 [1:49:54<41:02,  8.89s/it, lr=0.0001, step_loss=0.00557]\n",
      "Steps:  72%|███████▏  | 723/1000 [1:49:54<41:02,  8.89s/it, lr=0.0001, step_loss=0.00535]\n",
      "Steps:  72%|███████▏  | 724/1000 [1:50:03<41:03,  8.93s/it, lr=0.0001, step_loss=0.00535]\n",
      "Steps:  72%|███████▏  | 724/1000 [1:50:03<41:03,  8.93s/it, lr=0.0001, step_loss=0.00852]\n",
      "Steps:  72%|███████▎  | 725/1000 [1:50:12<40:44,  8.89s/it, lr=0.0001, step_loss=0.00852]\n",
      "Steps:  72%|███████▎  | 725/1000 [1:50:12<40:44,  8.89s/it, lr=0.0001, step_loss=0.00966]\n",
      "Steps:  73%|███████▎  | 726/1000 [1:50:21<40:52,  8.95s/it, lr=0.0001, step_loss=0.00966]\n",
      "Steps:  73%|███████▎  | 726/1000 [1:50:21<40:52,  8.95s/it, lr=0.0001, step_loss=0.00599]\n",
      "Steps:  73%|███████▎  | 727/1000 [1:50:30<40:27,  8.89s/it, lr=0.0001, step_loss=0.00599]\n",
      "Steps:  73%|███████▎  | 727/1000 [1:50:30<40:27,  8.89s/it, lr=0.0001, step_loss=0.00263]\n",
      "Steps:  73%|███████▎  | 728/1000 [1:50:39<40:27,  8.92s/it, lr=0.0001, step_loss=0.00263]\n",
      "Steps:  73%|███████▎  | 728/1000 [1:50:39<40:27,  8.92s/it, lr=0.0001, step_loss=0.0116]\n",
      "Steps:  73%|███████▎  | 729/1000 [1:50:47<40:07,  8.88s/it, lr=0.0001, step_loss=0.0116]\n",
      "Steps:  73%|███████▎  | 729/1000 [1:50:47<40:07,  8.88s/it, lr=0.0001, step_loss=0.004]\n",
      "Steps:  73%|███████▎  | 730/1000 [1:50:57<40:18,  8.96s/it, lr=0.0001, step_loss=0.004]\n",
      "Steps:  73%|███████▎  | 730/1000 [1:50:57<40:18,  8.96s/it, lr=0.0001, step_loss=0.00308]\n",
      "Steps:  73%|███████▎  | 731/1000 [1:51:05<39:51,  8.89s/it, lr=0.0001, step_loss=0.00308]\n",
      "Steps:  73%|███████▎  | 731/1000 [1:51:05<39:51,  8.89s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  73%|███████▎  | 732/1000 [1:51:14<39:51,  8.92s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  73%|███████▎  | 732/1000 [1:51:14<39:51,  8.92s/it, lr=0.0001, step_loss=0.0295]\n",
      "Steps:  73%|███████▎  | 733/1000 [1:51:23<39:34,  8.89s/it, lr=0.0001, step_loss=0.0295]\n",
      "Steps:  73%|███████▎  | 733/1000 [1:51:23<39:34,  8.89s/it, lr=0.0001, step_loss=0.00355]\n",
      "Steps:  73%|███████▎  | 734/1000 [1:51:32<39:43,  8.96s/it, lr=0.0001, step_loss=0.00355]\n",
      "Steps:  73%|███████▎  | 734/1000 [1:51:32<39:43,  8.96s/it, lr=0.0001, step_loss=0.0239]\n",
      "Steps:  74%|███████▎  | 735/1000 [1:51:41<39:14,  8.89s/it, lr=0.0001, step_loss=0.0239]\n",
      "Steps:  74%|███████▎  | 735/1000 [1:51:41<39:14,  8.89s/it, lr=0.0001, step_loss=0.00706]\n",
      "Steps:  74%|███████▎  | 736/1000 [1:51:50<39:14,  8.92s/it, lr=0.0001, step_loss=0.00706]\n",
      "Steps:  74%|███████▎  | 736/1000 [1:51:50<39:14,  8.92s/it, lr=0.0001, step_loss=0.0139]\n",
      "Steps:  74%|███████▎  | 737/1000 [1:51:59<38:55,  8.88s/it, lr=0.0001, step_loss=0.0139]\n",
      "Steps:  74%|███████▎  | 737/1000 [1:51:59<38:55,  8.88s/it, lr=0.0001, step_loss=0.0141]\n",
      "Steps:  74%|███████▍  | 738/1000 [1:52:08<39:07,  8.96s/it, lr=0.0001, step_loss=0.0141]\n",
      "Steps:  74%|███████▍  | 738/1000 [1:52:08<39:07,  8.96s/it, lr=0.0001, step_loss=0.0283]\n",
      "Steps:  74%|███████▍  | 739/1000 [1:52:17<38:38,  8.88s/it, lr=0.0001, step_loss=0.0283]\n",
      "Steps:  74%|███████▍  | 739/1000 [1:52:17<38:38,  8.88s/it, lr=0.0001, step_loss=0.0172]\n",
      "Steps:  74%|███████▍  | 740/1000 [1:52:26<38:40,  8.92s/it, lr=0.0001, step_loss=0.0172]\n",
      "Steps:  74%|███████▍  | 740/1000 [1:52:26<38:40,  8.92s/it, lr=0.0001, step_loss=0.0014]\n",
      "Steps:  74%|███████▍  | 741/1000 [1:52:34<38:21,  8.89s/it, lr=0.0001, step_loss=0.0014]\n",
      "Steps:  74%|███████▍  | 741/1000 [1:52:34<38:21,  8.89s/it, lr=0.0001, step_loss=0.00818]\n",
      "Steps:  74%|███████▍  | 742/1000 [1:52:43<38:28,  8.95s/it, lr=0.0001, step_loss=0.00818]\n",
      "Steps:  74%|███████▍  | 742/1000 [1:52:43<38:28,  8.95s/it, lr=0.0001, step_loss=0.0142]\n",
      "Steps:  74%|███████▍  | 743/1000 [1:52:52<38:04,  8.89s/it, lr=0.0001, step_loss=0.0142]\n",
      "Steps:  74%|███████▍  | 743/1000 [1:52:52<38:04,  8.89s/it, lr=0.0001, step_loss=0.00824]\n",
      "Steps:  74%|███████▍  | 744/1000 [1:53:01<38:03,  8.92s/it, lr=0.0001, step_loss=0.00824]\n",
      "Steps:  74%|███████▍  | 744/1000 [1:53:01<38:03,  8.92s/it, lr=0.0001, step_loss=0.00859]\n",
      "Steps:  74%|███████▍  | 745/1000 [1:53:10<37:46,  8.89s/it, lr=0.0001, step_loss=0.00859]\n",
      "Steps:  74%|███████▍  | 745/1000 [1:53:10<37:46,  8.89s/it, lr=0.0001, step_loss=0.004]\n",
      "Steps:  75%|███████▍  | 746/1000 [1:53:19<37:57,  8.96s/it, lr=0.0001, step_loss=0.004]\n",
      "Steps:  75%|███████▍  | 746/1000 [1:53:19<37:57,  8.96s/it, lr=0.0001, step_loss=0.00527]\n",
      "Steps:  75%|███████▍  | 747/1000 [1:53:28<37:28,  8.89s/it, lr=0.0001, step_loss=0.00527]\n",
      "Steps:  75%|███████▍  | 747/1000 [1:53:28<37:28,  8.89s/it, lr=0.0001, step_loss=0.0364]\n",
      "Steps:  75%|███████▍  | 748/1000 [1:53:37<37:29,  8.93s/it, lr=0.0001, step_loss=0.0364]\n",
      "Steps:  75%|███████▍  | 748/1000 [1:53:37<37:29,  8.93s/it, lr=0.0001, step_loss=0.0135]\n",
      "Steps:  75%|███████▍  | 749/1000 [1:53:46<37:14,  8.90s/it, lr=0.0001, step_loss=0.0135]\n",
      "Steps:  75%|███████▍  | 749/1000 [1:53:46<37:14,  8.90s/it, lr=0.0001, step_loss=0.00728]\n",
      "Steps:  75%|███████▌  | 750/1000 [1:53:55<37:17,  8.95s/it, lr=0.0001, step_loss=0.00728]\n",
      "Steps:  75%|███████▌  | 750/1000 [1:53:55<37:17,  8.95s/it, lr=0.0001, step_loss=0.00413]\n",
      "Steps:  75%|███████▌  | 751/1000 [1:54:04<36:53,  8.89s/it, lr=0.0001, step_loss=0.00413]\n",
      "Steps:  75%|███████▌  | 751/1000 [1:54:04<36:53,  8.89s/it, lr=0.0001, step_loss=0.00378]\n",
      "Steps:  75%|███████▌  | 752/1000 [1:54:13<36:54,  8.93s/it, lr=0.0001, step_loss=0.00378]\n",
      "Steps:  75%|███████▌  | 752/1000 [1:54:13<36:54,  8.93s/it, lr=0.0001, step_loss=0.00198]\n",
      "Steps:  75%|███████▌  | 753/1000 [1:54:21<36:38,  8.90s/it, lr=0.0001, step_loss=0.00198]\n",
      "Steps:  75%|███████▌  | 753/1000 [1:54:21<36:38,  8.90s/it, lr=0.0001, step_loss=0.0386]\n",
      "Steps:  75%|███████▌  | 754/1000 [1:54:30<36:46,  8.97s/it, lr=0.0001, step_loss=0.0386]\n",
      "Steps:  75%|███████▌  | 754/1000 [1:54:30<36:46,  8.97s/it, lr=0.0001, step_loss=0.00417]\n",
      "Steps:  76%|███████▌  | 755/1000 [1:54:39<36:16,  8.88s/it, lr=0.0001, step_loss=0.00417]\n",
      "Steps:  76%|███████▌  | 755/1000 [1:54:39<36:16,  8.88s/it, lr=0.0001, step_loss=0.0243]\n",
      "Steps:  76%|███████▌  | 756/1000 [1:54:48<36:16,  8.92s/it, lr=0.0001, step_loss=0.0243]\n",
      "Steps:  76%|███████▌  | 756/1000 [1:54:48<36:16,  8.92s/it, lr=0.0001, step_loss=0.00463]\n",
      "Steps:  76%|███████▌  | 757/1000 [1:54:57<35:59,  8.89s/it, lr=0.0001, step_loss=0.00463]\n",
      "Steps:  76%|███████▌  | 757/1000 [1:54:57<35:59,  8.89s/it, lr=0.0001, step_loss=0.00673]\n",
      "Steps:  76%|███████▌  | 758/1000 [1:55:06<36:06,  8.95s/it, lr=0.0001, step_loss=0.00673]\n",
      "Steps:  76%|███████▌  | 758/1000 [1:55:06<36:06,  8.95s/it, lr=0.0001, step_loss=0.00677]\n",
      "Steps:  76%|███████▌  | 759/1000 [1:55:15<35:42,  8.89s/it, lr=0.0001, step_loss=0.00677]\n",
      "Steps:  76%|███████▌  | 759/1000 [1:55:15<35:42,  8.89s/it, lr=0.0001, step_loss=0.00658]\n",
      "Steps:  76%|███████▌  | 760/1000 [1:55:24<35:40,  8.92s/it, lr=0.0001, step_loss=0.00658]\n",
      "Steps:  76%|███████▌  | 760/1000 [1:55:24<35:40,  8.92s/it, lr=0.0001, step_loss=0.0155]\n",
      "Steps:  76%|███████▌  | 761/1000 [1:55:33<35:24,  8.89s/it, lr=0.0001, step_loss=0.0155]\n",
      "Steps:  76%|███████▌  | 761/1000 [1:55:33<35:24,  8.89s/it, lr=0.0001, step_loss=0.00903]\n",
      "Steps:  76%|███████▌  | 762/1000 [1:55:42<35:31,  8.96s/it, lr=0.0001, step_loss=0.00903]\n",
      "Steps:  76%|███████▌  | 762/1000 [1:55:42<35:31,  8.96s/it, lr=0.0001, step_loss=0.0233]\n",
      "Steps:  76%|███████▋  | 763/1000 [1:55:50<35:06,  8.89s/it, lr=0.0001, step_loss=0.0233]\n",
      "Steps:  76%|███████▋  | 763/1000 [1:55:51<35:06,  8.89s/it, lr=0.0001, step_loss=0.00434]\n",
      "Steps:  76%|███████▋  | 764/1000 [1:55:59<35:05,  8.92s/it, lr=0.0001, step_loss=0.00434]\n",
      "Steps:  76%|███████▋  | 764/1000 [1:56:00<35:05,  8.92s/it, lr=0.0001, step_loss=0.00612]\n",
      "Steps:  76%|███████▋  | 765/1000 [1:56:08<34:50,  8.89s/it, lr=0.0001, step_loss=0.00612]\n",
      "Steps:  76%|███████▋  | 765/1000 [1:56:08<34:50,  8.89s/it, lr=0.0001, step_loss=0.00689]\n",
      "Steps:  77%|███████▋  | 766/1000 [1:56:17<34:57,  8.96s/it, lr=0.0001, step_loss=0.00689]\n",
      "Steps:  77%|███████▋  | 766/1000 [1:56:17<34:57,  8.96s/it, lr=0.0001, step_loss=0.0144]\n",
      "Steps:  77%|███████▋  | 767/1000 [1:56:26<34:31,  8.89s/it, lr=0.0001, step_loss=0.0144]\n",
      "Steps:  77%|███████▋  | 767/1000 [1:56:26<34:31,  8.89s/it, lr=0.0001, step_loss=0.00786]\n",
      "Steps:  77%|███████▋  | 768/1000 [1:56:35<34:30,  8.93s/it, lr=0.0001, step_loss=0.00786]\n",
      "Steps:  77%|███████▋  | 768/1000 [1:56:35<34:30,  8.93s/it, lr=0.0001, step_loss=0.00712]\n",
      "Steps:  77%|███████▋  | 769/1000 [1:56:44<34:14,  8.89s/it, lr=0.0001, step_loss=0.00712]\n",
      "Steps:  77%|███████▋  | 769/1000 [1:56:44<34:14,  8.89s/it, lr=0.0001, step_loss=0.013]\n",
      "Steps:  77%|███████▋  | 770/1000 [1:56:53<34:19,  8.95s/it, lr=0.0001, step_loss=0.013]\n",
      "Steps:  77%|███████▋  | 770/1000 [1:56:53<34:19,  8.95s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  77%|███████▋  | 771/1000 [1:57:02<33:55,  8.89s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  77%|███████▋  | 771/1000 [1:57:02<33:55,  8.89s/it, lr=0.0001, step_loss=0.00886]\n",
      "Steps:  77%|███████▋  | 772/1000 [1:57:11<33:54,  8.92s/it, lr=0.0001, step_loss=0.00886]\n",
      "Steps:  77%|███████▋  | 772/1000 [1:57:11<33:54,  8.92s/it, lr=0.0001, step_loss=0.00392]\n",
      "Steps:  77%|███████▋  | 773/1000 [1:57:20<33:37,  8.89s/it, lr=0.0001, step_loss=0.00392]\n",
      "Steps:  77%|███████▋  | 773/1000 [1:57:20<33:37,  8.89s/it, lr=0.0001, step_loss=0.0041]\n",
      "Steps:  77%|███████▋  | 774/1000 [1:57:29<33:43,  8.96s/it, lr=0.0001, step_loss=0.0041]\n",
      "Steps:  77%|███████▋  | 774/1000 [1:57:29<33:43,  8.96s/it, lr=0.0001, step_loss=0.00387]\n",
      "Steps:  78%|███████▊  | 775/1000 [1:57:37<33:19,  8.88s/it, lr=0.0001, step_loss=0.00387]\n",
      "Steps:  78%|███████▊  | 775/1000 [1:57:37<33:19,  8.88s/it, lr=0.0001, step_loss=0.00648]\n",
      "Steps:  78%|███████▊  | 776/1000 [1:57:46<33:18,  8.92s/it, lr=0.0001, step_loss=0.00648]\n",
      "Steps:  78%|███████▊  | 776/1000 [1:57:46<33:18,  8.92s/it, lr=0.0001, step_loss=0.0423]\n",
      "Steps:  78%|███████▊  | 777/1000 [1:57:55<33:04,  8.90s/it, lr=0.0001, step_loss=0.0423]\n",
      "Steps:  78%|███████▊  | 777/1000 [1:57:55<33:04,  8.90s/it, lr=0.0001, step_loss=0.0215]\n",
      "Steps:  78%|███████▊  | 778/1000 [1:58:04<33:09,  8.96s/it, lr=0.0001, step_loss=0.0215]\n",
      "Steps:  78%|███████▊  | 778/1000 [1:58:04<33:09,  8.96s/it, lr=0.0001, step_loss=0.00847]\n",
      "Steps:  78%|███████▊  | 779/1000 [1:58:13<32:42,  8.88s/it, lr=0.0001, step_loss=0.00847]\n",
      "Steps:  78%|███████▊  | 779/1000 [1:58:13<32:42,  8.88s/it, lr=0.0001, step_loss=0.006]\n",
      "Steps:  78%|███████▊  | 780/1000 [1:58:22<32:42,  8.92s/it, lr=0.0001, step_loss=0.006]\n",
      "Steps:  78%|███████▊  | 780/1000 [1:58:22<32:42,  8.92s/it, lr=0.0001, step_loss=0.0228]\n",
      "Steps:  78%|███████▊  | 781/1000 [1:58:31<32:25,  8.88s/it, lr=0.0001, step_loss=0.0228]\n",
      "Steps:  78%|███████▊  | 781/1000 [1:58:31<32:25,  8.88s/it, lr=0.0001, step_loss=0.00425]\n",
      "Steps:  78%|███████▊  | 782/1000 [1:58:40<32:34,  8.97s/it, lr=0.0001, step_loss=0.00425]\n",
      "Steps:  78%|███████▊  | 782/1000 [1:58:40<32:34,  8.97s/it, lr=0.0001, step_loss=0.0143]\n",
      "Steps:  78%|███████▊  | 783/1000 [1:58:49<32:07,  8.88s/it, lr=0.0001, step_loss=0.0143]\n",
      "Steps:  78%|███████▊  | 783/1000 [1:58:49<32:07,  8.88s/it, lr=0.0001, step_loss=0.00535]\n",
      "Steps:  78%|███████▊  | 784/1000 [1:58:58<32:07,  8.92s/it, lr=0.0001, step_loss=0.00535]\n",
      "Steps:  78%|███████▊  | 784/1000 [1:58:58<32:07,  8.92s/it, lr=0.0001, step_loss=0.00935]\n",
      "Steps:  78%|███████▊  | 785/1000 [1:59:07<31:50,  8.89s/it, lr=0.0001, step_loss=0.00935]\n",
      "Steps:  78%|███████▊  | 785/1000 [1:59:07<31:50,  8.89s/it, lr=0.0001, step_loss=0.00657]\n",
      "Steps:  79%|███████▊  | 786/1000 [1:59:16<31:57,  8.96s/it, lr=0.0001, step_loss=0.00657]\n",
      "Steps:  79%|███████▊  | 786/1000 [1:59:16<31:57,  8.96s/it, lr=0.0001, step_loss=0.0118]\n",
      "Steps:  79%|███████▊  | 787/1000 [1:59:24<31:32,  8.88s/it, lr=0.0001, step_loss=0.0118]\n",
      "Steps:  79%|███████▊  | 787/1000 [1:59:24<31:32,  8.88s/it, lr=0.0001, step_loss=0.0192]\n",
      "Steps:  79%|███████▉  | 788/1000 [1:59:33<31:30,  8.92s/it, lr=0.0001, step_loss=0.0192]\n",
      "Steps:  79%|███████▉  | 788/1000 [1:59:33<31:30,  8.92s/it, lr=0.0001, step_loss=0.004]\n",
      "Steps:  79%|███████▉  | 789/1000 [1:59:42<31:15,  8.89s/it, lr=0.0001, step_loss=0.004]\n",
      "Steps:  79%|███████▉  | 789/1000 [1:59:42<31:15,  8.89s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  79%|███████▉  | 790/1000 [1:59:51<31:19,  8.95s/it, lr=0.0001, step_loss=0.0169]\n",
      "Steps:  79%|███████▉  | 790/1000 [1:59:51<31:19,  8.95s/it, lr=0.0001, step_loss=0.0176]\n",
      "Steps:  79%|███████▉  | 791/1000 [2:00:00<30:55,  8.88s/it, lr=0.0001, step_loss=0.0176]\n",
      "Steps:  79%|███████▉  | 791/1000 [2:00:00<30:55,  8.88s/it, lr=0.0001, step_loss=0.133]\n",
      "Steps:  79%|███████▉  | 792/1000 [2:00:09<30:55,  8.92s/it, lr=0.0001, step_loss=0.133]\n",
      "Steps:  79%|███████▉  | 792/1000 [2:00:09<30:55,  8.92s/it, lr=0.0001, step_loss=0.0082]\n",
      "Steps:  79%|███████▉  | 793/1000 [2:00:18<30:40,  8.89s/it, lr=0.0001, step_loss=0.0082]\n",
      "Steps:  79%|███████▉  | 793/1000 [2:00:18<30:40,  8.89s/it, lr=0.0001, step_loss=0.00437]\n",
      "Steps:  79%|███████▉  | 794/1000 [2:00:27<30:44,  8.95s/it, lr=0.0001, step_loss=0.00437]\n",
      "Steps:  79%|███████▉  | 794/1000 [2:00:27<30:44,  8.95s/it, lr=0.0001, step_loss=0.00582]\n",
      "Steps:  80%|███████▉  | 795/1000 [2:00:36<30:21,  8.88s/it, lr=0.0001, step_loss=0.00582]\n",
      "Steps:  80%|███████▉  | 795/1000 [2:00:36<30:21,  8.88s/it, lr=0.0001, step_loss=0.00749]\n",
      "Steps:  80%|███████▉  | 796/1000 [2:00:45<30:19,  8.92s/it, lr=0.0001, step_loss=0.00749]\n",
      "Steps:  80%|███████▉  | 796/1000 [2:00:45<30:19,  8.92s/it, lr=0.0001, step_loss=0.0131]\n",
      "Steps:  80%|███████▉  | 797/1000 [2:00:54<30:05,  8.89s/it, lr=0.0001, step_loss=0.0131]\n",
      "Steps:  80%|███████▉  | 797/1000 [2:00:54<30:05,  8.89s/it, lr=0.0001, step_loss=0.00706]\n",
      "Steps:  80%|███████▉  | 798/1000 [2:01:03<30:10,  8.96s/it, lr=0.0001, step_loss=0.00706]\n",
      "Steps:  80%|███████▉  | 798/1000 [2:01:03<30:10,  8.96s/it, lr=0.0001, step_loss=0.00429]\n",
      "Steps:  80%|███████▉  | 799/1000 [2:01:11<29:45,  8.88s/it, lr=0.0001, step_loss=0.00429]\n",
      "Steps:  80%|███████▉  | 799/1000 [2:01:11<29:45,  8.88s/it, lr=0.0001, step_loss=0.0138]\n",
      "Steps:  80%|████████  | 800/1000 [2:01:20<29:44,  8.92s/it, lr=0.0001, step_loss=0.0138]04/17/2024 05:56:26 - INFO - accelerate.accelerator - Saving current state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-800\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-800\\unet_ema\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-800/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-800\\unet\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-800/unet/diffusion_pytorch_model.safetensors\n",
      "04/17/2024 05:57:03 - INFO - accelerate.checkpointing - Optimizer state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-800\\optimizer.bin\n",
      "04/17/2024 05:57:03 - INFO - accelerate.checkpointing - Scheduler state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-800\\scheduler.bin\n",
      "04/17/2024 05:57:03 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-800\\sampler.bin\n",
      "04/17/2024 05:57:03 - INFO - accelerate.checkpointing - Random states saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-800\\random_states_0.pkl\n",
      "04/17/2024 05:57:03 - INFO - __main__ - Saved state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-800\n",
      "\n",
      "Steps:  80%|████████  | 800/1000 [2:01:57<29:44,  8.92s/it, lr=0.0001, step_loss=0.00184]\n",
      "Steps:  80%|████████  | 801/1000 [2:02:06<1:06:23, 20.02s/it, lr=0.0001, step_loss=0.00184]\n",
      "Steps:  80%|████████  | 801/1000 [2:02:06<1:06:23, 20.02s/it, lr=0.0001, step_loss=0.00405]\n",
      "Steps:  80%|████████  | 802/1000 [2:02:15<55:17, 16.75s/it, lr=0.0001, step_loss=0.00405]\n",
      "Steps:  80%|████████  | 802/1000 [2:02:15<55:17, 16.75s/it, lr=0.0001, step_loss=0.00731]\n",
      "Steps:  80%|████████  | 803/1000 [2:02:24<47:03, 14.33s/it, lr=0.0001, step_loss=0.00731]\n",
      "Steps:  80%|████████  | 803/1000 [2:02:24<47:03, 14.33s/it, lr=0.0001, step_loss=0.00337]\n",
      "Steps:  80%|████████  | 804/1000 [2:02:33<41:36, 12.74s/it, lr=0.0001, step_loss=0.00337]\n",
      "Steps:  80%|████████  | 804/1000 [2:02:33<41:36, 12.74s/it, lr=0.0001, step_loss=0.00848]\n",
      "Steps:  80%|████████  | 805/1000 [2:02:42<37:34, 11.56s/it, lr=0.0001, step_loss=0.00848]\n",
      "Steps:  80%|████████  | 805/1000 [2:02:42<37:34, 11.56s/it, lr=0.0001, step_loss=0.00349]\n",
      "Steps:  81%|████████  | 806/1000 [2:02:51<35:01, 10.83s/it, lr=0.0001, step_loss=0.00349]\n",
      "Steps:  81%|████████  | 806/1000 [2:02:51<35:01, 10.83s/it, lr=0.0001, step_loss=0.00631]\n",
      "Steps:  81%|████████  | 807/1000 [2:03:00<32:47, 10.19s/it, lr=0.0001, step_loss=0.00631]\n",
      "Steps:  81%|████████  | 807/1000 [2:03:00<32:47, 10.19s/it, lr=0.0001, step_loss=0.00687]\n",
      "Steps:  81%|████████  | 808/1000 [2:03:09<31:28,  9.84s/it, lr=0.0001, step_loss=0.00687]\n",
      "Steps:  81%|████████  | 808/1000 [2:03:09<31:28,  9.84s/it, lr=0.0001, step_loss=0.0079]\n",
      "Steps:  81%|████████  | 809/1000 [2:03:18<30:21,  9.54s/it, lr=0.0001, step_loss=0.0079]\n",
      "Steps:  81%|████████  | 809/1000 [2:03:18<30:21,  9.54s/it, lr=0.0001, step_loss=0.00266]\n",
      "Steps:  81%|████████  | 810/1000 [2:03:27<29:47,  9.41s/it, lr=0.0001, step_loss=0.00266]\n",
      "Steps:  81%|████████  | 810/1000 [2:03:27<29:47,  9.41s/it, lr=0.0001, step_loss=0.00505]\n",
      "Steps:  81%|████████  | 811/1000 [2:03:35<28:58,  9.20s/it, lr=0.0001, step_loss=0.00505]\n",
      "Steps:  81%|████████  | 811/1000 [2:03:35<28:58,  9.20s/it, lr=0.0001, step_loss=0.0241]\n",
      "Steps:  81%|████████  | 812/1000 [2:03:44<28:38,  9.14s/it, lr=0.0001, step_loss=0.0241]\n",
      "Steps:  81%|████████  | 812/1000 [2:03:44<28:38,  9.14s/it, lr=0.0001, step_loss=0.00407]\n",
      "Steps:  81%|████████▏ | 813/1000 [2:03:53<28:11,  9.04s/it, lr=0.0001, step_loss=0.00407]\n",
      "Steps:  81%|████████▏ | 813/1000 [2:03:53<28:11,  9.04s/it, lr=0.0001, step_loss=0.00377]\n",
      "Steps:  81%|████████▏ | 814/1000 [2:04:02<28:06,  9.07s/it, lr=0.0001, step_loss=0.00377]\n",
      "Steps:  81%|████████▏ | 814/1000 [2:04:02<28:06,  9.07s/it, lr=0.0001, step_loss=0.0148]\n",
      "Steps:  82%|████████▏ | 815/1000 [2:04:11<27:38,  8.96s/it, lr=0.0001, step_loss=0.0148]\n",
      "Steps:  82%|████████▏ | 815/1000 [2:04:11<27:38,  8.96s/it, lr=0.0001, step_loss=0.01]\n",
      "Steps:  82%|████████▏ | 816/1000 [2:04:20<27:32,  8.98s/it, lr=0.0001, step_loss=0.01]\n",
      "Steps:  82%|████████▏ | 816/1000 [2:04:20<27:32,  8.98s/it, lr=0.0001, step_loss=0.00433]\n",
      "Steps:  82%|████████▏ | 817/1000 [2:04:29<27:13,  8.92s/it, lr=0.0001, step_loss=0.00433]\n",
      "Steps:  82%|████████▏ | 817/1000 [2:04:29<27:13,  8.92s/it, lr=0.0001, step_loss=0.00473]\n",
      "Steps:  82%|████████▏ | 818/1000 [2:04:38<27:14,  8.98s/it, lr=0.0001, step_loss=0.00473]\n",
      "Steps:  82%|████████▏ | 818/1000 [2:04:38<27:14,  8.98s/it, lr=0.0001, step_loss=0.0108]\n",
      "Steps:  82%|████████▏ | 819/1000 [2:04:47<26:58,  8.94s/it, lr=0.0001, step_loss=0.0108]\n",
      "Steps:  82%|████████▏ | 819/1000 [2:04:47<26:58,  8.94s/it, lr=0.0001, step_loss=0.00551]\n",
      "Steps:  82%|████████▏ | 820/1000 [2:04:56<27:02,  9.01s/it, lr=0.0001, step_loss=0.00551]\n",
      "Steps:  82%|████████▏ | 820/1000 [2:04:56<27:02,  9.01s/it, lr=0.0001, step_loss=0.0187]\n",
      "Steps:  82%|████████▏ | 821/1000 [2:05:05<26:48,  8.98s/it, lr=0.0001, step_loss=0.0187]\n",
      "Steps:  82%|████████▏ | 821/1000 [2:05:05<26:48,  8.98s/it, lr=0.0001, step_loss=0.00435]\n",
      "Steps:  82%|████████▏ | 822/1000 [2:05:14<26:48,  9.03s/it, lr=0.0001, step_loss=0.00435]\n",
      "Steps:  82%|████████▏ | 822/1000 [2:05:14<26:48,  9.03s/it, lr=0.0001, step_loss=0.00135]\n",
      "Steps:  82%|████████▏ | 823/1000 [2:05:23<26:20,  8.93s/it, lr=0.0001, step_loss=0.00135]\n",
      "Steps:  82%|████████▏ | 823/1000 [2:05:23<26:20,  8.93s/it, lr=0.0001, step_loss=0.00447]\n",
      "Steps:  82%|████████▏ | 824/1000 [2:05:32<26:15,  8.95s/it, lr=0.0001, step_loss=0.00447]\n",
      "Steps:  82%|████████▏ | 824/1000 [2:05:32<26:15,  8.95s/it, lr=0.0001, step_loss=0.00576]\n",
      "Steps:  82%|████████▎ | 825/1000 [2:05:41<25:59,  8.91s/it, lr=0.0001, step_loss=0.00576]\n",
      "Steps:  82%|████████▎ | 825/1000 [2:05:41<25:59,  8.91s/it, lr=0.0001, step_loss=0.00318]\n",
      "Steps:  83%|████████▎ | 826/1000 [2:05:50<26:00,  8.97s/it, lr=0.0001, step_loss=0.00318]\n",
      "Steps:  83%|████████▎ | 826/1000 [2:05:50<26:00,  8.97s/it, lr=0.0001, step_loss=0.0036]\n",
      "Steps:  83%|████████▎ | 827/1000 [2:05:58<25:39,  8.90s/it, lr=0.0001, step_loss=0.0036]\n",
      "Steps:  83%|████████▎ | 827/1000 [2:05:58<25:39,  8.90s/it, lr=0.0001, step_loss=0.0029]\n",
      "Steps:  83%|████████▎ | 828/1000 [2:06:07<25:35,  8.93s/it, lr=0.0001, step_loss=0.0029]\n",
      "Steps:  83%|████████▎ | 828/1000 [2:06:07<25:35,  8.93s/it, lr=0.0001, step_loss=0.00403]\n",
      "Steps:  83%|████████▎ | 829/1000 [2:06:16<25:20,  8.89s/it, lr=0.0001, step_loss=0.00403]\n",
      "Steps:  83%|████████▎ | 829/1000 [2:06:16<25:20,  8.89s/it, lr=0.0001, step_loss=0.00333]\n",
      "Steps:  83%|████████▎ | 830/1000 [2:06:25<25:23,  8.96s/it, lr=0.0001, step_loss=0.00333]\n",
      "Steps:  83%|████████▎ | 830/1000 [2:06:25<25:23,  8.96s/it, lr=0.0001, step_loss=0.00843]\n",
      "Steps:  83%|████████▎ | 831/1000 [2:06:34<25:01,  8.88s/it, lr=0.0001, step_loss=0.00843]\n",
      "Steps:  83%|████████▎ | 831/1000 [2:06:34<25:01,  8.88s/it, lr=0.0001, step_loss=0.0065]\n",
      "Steps:  83%|████████▎ | 832/1000 [2:06:43<24:59,  8.93s/it, lr=0.0001, step_loss=0.0065]\n",
      "Steps:  83%|████████▎ | 832/1000 [2:06:43<24:59,  8.93s/it, lr=0.0001, step_loss=0.0123]\n",
      "Steps:  83%|████████▎ | 833/1000 [2:06:52<24:44,  8.89s/it, lr=0.0001, step_loss=0.0123]\n",
      "Steps:  83%|████████▎ | 833/1000 [2:06:52<24:44,  8.89s/it, lr=0.0001, step_loss=0.00342]\n",
      "Steps:  83%|████████▎ | 834/1000 [2:07:01<24:46,  8.96s/it, lr=0.0001, step_loss=0.00342]\n",
      "Steps:  83%|████████▎ | 834/1000 [2:07:01<24:46,  8.96s/it, lr=0.0001, step_loss=0.0112]\n",
      "Steps:  84%|████████▎ | 835/1000 [2:07:10<24:27,  8.89s/it, lr=0.0001, step_loss=0.0112]\n",
      "Steps:  84%|████████▎ | 835/1000 [2:07:10<24:27,  8.89s/it, lr=0.0001, step_loss=0.00523]\n",
      "Steps:  84%|████████▎ | 836/1000 [2:07:19<24:24,  8.93s/it, lr=0.0001, step_loss=0.00523]\n",
      "Steps:  84%|████████▎ | 836/1000 [2:07:19<24:24,  8.93s/it, lr=0.0001, step_loss=0.0184]\n",
      "Steps:  84%|████████▎ | 837/1000 [2:07:28<24:11,  8.90s/it, lr=0.0001, step_loss=0.0184]\n",
      "Steps:  84%|████████▎ | 837/1000 [2:07:28<24:11,  8.90s/it, lr=0.0001, step_loss=0.0318]\n",
      "Steps:  84%|████████▍ | 838/1000 [2:07:37<24:12,  8.97s/it, lr=0.0001, step_loss=0.0318]\n",
      "Steps:  84%|████████▍ | 838/1000 [2:07:37<24:12,  8.97s/it, lr=0.0001, step_loss=0.00778]\n",
      "Steps:  84%|████████▍ | 839/1000 [2:07:45<23:50,  8.89s/it, lr=0.0001, step_loss=0.00778]\n",
      "Steps:  84%|████████▍ | 839/1000 [2:07:45<23:50,  8.89s/it, lr=0.0001, step_loss=0.00181]\n",
      "Steps:  84%|████████▍ | 840/1000 [2:07:54<23:47,  8.92s/it, lr=0.0001, step_loss=0.00181]\n",
      "Steps:  84%|████████▍ | 840/1000 [2:07:54<23:47,  8.92s/it, lr=0.0001, step_loss=0.00411]\n",
      "Steps:  84%|████████▍ | 841/1000 [2:08:03<23:33,  8.89s/it, lr=0.0001, step_loss=0.00411]\n",
      "Steps:  84%|████████▍ | 841/1000 [2:08:03<23:33,  8.89s/it, lr=0.0001, step_loss=0.00786]\n",
      "Steps:  84%|████████▍ | 842/1000 [2:08:12<23:36,  8.97s/it, lr=0.0001, step_loss=0.00786]\n",
      "Steps:  84%|████████▍ | 842/1000 [2:08:12<23:36,  8.97s/it, lr=0.0001, step_loss=0.0183]\n",
      "Steps:  84%|████████▍ | 843/1000 [2:08:21<23:14,  8.88s/it, lr=0.0001, step_loss=0.0183]\n",
      "Steps:  84%|████████▍ | 843/1000 [2:08:21<23:14,  8.88s/it, lr=0.0001, step_loss=0.0242]\n",
      "Steps:  84%|████████▍ | 844/1000 [2:08:30<23:11,  8.92s/it, lr=0.0001, step_loss=0.0242]\n",
      "Steps:  84%|████████▍ | 844/1000 [2:08:30<23:11,  8.92s/it, lr=0.0001, step_loss=0.00792]\n",
      "Steps:  84%|████████▍ | 845/1000 [2:08:39<22:57,  8.89s/it, lr=0.0001, step_loss=0.00792]\n",
      "Steps:  84%|████████▍ | 845/1000 [2:08:39<22:57,  8.89s/it, lr=0.0001, step_loss=0.0268]\n",
      "Steps:  85%|████████▍ | 846/1000 [2:08:48<22:59,  8.96s/it, lr=0.0001, step_loss=0.0268]\n",
      "Steps:  85%|████████▍ | 846/1000 [2:08:48<22:59,  8.96s/it, lr=0.0001, step_loss=0.00409]\n",
      "Steps:  85%|████████▍ | 847/1000 [2:08:57<22:38,  8.88s/it, lr=0.0001, step_loss=0.00409]\n",
      "Steps:  85%|████████▍ | 847/1000 [2:08:57<22:38,  8.88s/it, lr=0.0001, step_loss=0.00463]\n",
      "Steps:  85%|████████▍ | 848/1000 [2:09:06<22:35,  8.92s/it, lr=0.0001, step_loss=0.00463]\n",
      "Steps:  85%|████████▍ | 848/1000 [2:09:06<22:35,  8.92s/it, lr=0.0001, step_loss=0.0053]\n",
      "Steps:  85%|████████▍ | 849/1000 [2:09:15<22:22,  8.89s/it, lr=0.0001, step_loss=0.0053]\n",
      "Steps:  85%|████████▍ | 849/1000 [2:09:15<22:22,  8.89s/it, lr=0.0001, step_loss=0.00411]\n",
      "Steps:  85%|████████▌ | 850/1000 [2:09:24<22:23,  8.95s/it, lr=0.0001, step_loss=0.00411]\n",
      "Steps:  85%|████████▌ | 850/1000 [2:09:24<22:23,  8.95s/it, lr=0.0001, step_loss=0.0293]\n",
      "Steps:  85%|████████▌ | 851/1000 [2:09:32<22:03,  8.88s/it, lr=0.0001, step_loss=0.0293]\n",
      "Steps:  85%|████████▌ | 851/1000 [2:09:32<22:03,  8.88s/it, lr=0.0001, step_loss=0.00331]\n",
      "Steps:  85%|████████▌ | 852/1000 [2:09:41<22:00,  8.92s/it, lr=0.0001, step_loss=0.00331]\n",
      "Steps:  85%|████████▌ | 852/1000 [2:09:41<22:00,  8.92s/it, lr=0.0001, step_loss=0.00549]\n",
      "Steps:  85%|████████▌ | 853/1000 [2:09:50<21:46,  8.89s/it, lr=0.0001, step_loss=0.00549]\n",
      "Steps:  85%|████████▌ | 853/1000 [2:09:50<21:46,  8.89s/it, lr=0.0001, step_loss=0.00122]\n",
      "Steps:  85%|████████▌ | 854/1000 [2:09:59<21:47,  8.95s/it, lr=0.0001, step_loss=0.00122]\n",
      "Steps:  85%|████████▌ | 854/1000 [2:09:59<21:47,  8.95s/it, lr=0.0001, step_loss=0.0318]\n",
      "Steps:  86%|████████▌ | 855/1000 [2:10:08<21:27,  8.88s/it, lr=0.0001, step_loss=0.0318]\n",
      "Steps:  86%|████████▌ | 855/1000 [2:10:08<21:27,  8.88s/it, lr=0.0001, step_loss=0.0319]\n",
      "Steps:  86%|████████▌ | 856/1000 [2:10:17<21:24,  8.92s/it, lr=0.0001, step_loss=0.0319]\n",
      "Steps:  86%|████████▌ | 856/1000 [2:10:17<21:24,  8.92s/it, lr=0.0001, step_loss=0.00825]\n",
      "Steps:  86%|████████▌ | 857/1000 [2:10:26<21:12,  8.90s/it, lr=0.0001, step_loss=0.00825]\n",
      "Steps:  86%|████████▌ | 857/1000 [2:10:26<21:12,  8.90s/it, lr=0.0001, step_loss=0.0216]\n",
      "Steps:  86%|████████▌ | 858/1000 [2:10:35<21:12,  8.96s/it, lr=0.0001, step_loss=0.0216]\n",
      "Steps:  86%|████████▌ | 858/1000 [2:10:35<21:12,  8.96s/it, lr=0.0001, step_loss=0.0251]\n",
      "Steps:  86%|████████▌ | 859/1000 [2:10:44<20:52,  8.88s/it, lr=0.0001, step_loss=0.0251]\n",
      "Steps:  86%|████████▌ | 859/1000 [2:10:44<20:52,  8.88s/it, lr=0.0001, step_loss=0.00415]\n",
      "Steps:  86%|████████▌ | 860/1000 [2:10:53<20:49,  8.92s/it, lr=0.0001, step_loss=0.00415]\n",
      "Steps:  86%|████████▌ | 860/1000 [2:10:53<20:49,  8.92s/it, lr=0.0001, step_loss=0.00763]\n",
      "Steps:  86%|████████▌ | 861/1000 [2:11:02<20:35,  8.89s/it, lr=0.0001, step_loss=0.00763]\n",
      "Steps:  86%|████████▌ | 861/1000 [2:11:02<20:35,  8.89s/it, lr=0.0001, step_loss=0.00342]\n",
      "Steps:  86%|████████▌ | 862/1000 [2:11:11<20:36,  8.96s/it, lr=0.0001, step_loss=0.00342]\n",
      "Steps:  86%|████████▌ | 862/1000 [2:11:11<20:36,  8.96s/it, lr=0.0001, step_loss=0.0104]\n",
      "Steps:  86%|████████▋ | 863/1000 [2:11:19<20:17,  8.88s/it, lr=0.0001, step_loss=0.0104]\n",
      "Steps:  86%|████████▋ | 863/1000 [2:11:19<20:17,  8.88s/it, lr=0.0001, step_loss=0.00483]\n",
      "Steps:  86%|████████▋ | 864/1000 [2:11:28<20:13,  8.93s/it, lr=0.0001, step_loss=0.00483]\n",
      "Steps:  86%|████████▋ | 864/1000 [2:11:28<20:13,  8.93s/it, lr=0.0001, step_loss=0.0084]\n",
      "Steps:  86%|████████▋ | 865/1000 [2:11:37<20:00,  8.89s/it, lr=0.0001, step_loss=0.0084]\n",
      "Steps:  86%|████████▋ | 865/1000 [2:11:37<20:00,  8.89s/it, lr=0.0001, step_loss=0.00552]\n",
      "Steps:  87%|████████▋ | 866/1000 [2:11:46<20:01,  8.97s/it, lr=0.0001, step_loss=0.00552]\n",
      "Steps:  87%|████████▋ | 866/1000 [2:11:46<20:01,  8.97s/it, lr=0.0001, step_loss=0.0145]\n",
      "Steps:  87%|████████▋ | 867/1000 [2:11:55<19:41,  8.89s/it, lr=0.0001, step_loss=0.0145]\n",
      "Steps:  87%|████████▋ | 867/1000 [2:11:55<19:41,  8.89s/it, lr=0.0001, step_loss=0.00208]\n",
      "Steps:  87%|████████▋ | 868/1000 [2:12:04<19:37,  8.92s/it, lr=0.0001, step_loss=0.00208]\n",
      "Steps:  87%|████████▋ | 868/1000 [2:12:04<19:37,  8.92s/it, lr=0.0001, step_loss=0.00277]\n",
      "Steps:  87%|████████▋ | 869/1000 [2:12:13<19:25,  8.90s/it, lr=0.0001, step_loss=0.00277]\n",
      "Steps:  87%|████████▋ | 869/1000 [2:12:13<19:25,  8.90s/it, lr=0.0001, step_loss=0.00808]\n",
      "Steps:  87%|████████▋ | 870/1000 [2:12:22<19:24,  8.96s/it, lr=0.0001, step_loss=0.00808]\n",
      "Steps:  87%|████████▋ | 870/1000 [2:12:22<19:24,  8.96s/it, lr=0.0001, step_loss=0.00391]\n",
      "Steps:  87%|████████▋ | 871/1000 [2:12:31<19:06,  8.88s/it, lr=0.0001, step_loss=0.00391]\n",
      "Steps:  87%|████████▋ | 871/1000 [2:12:31<19:06,  8.88s/it, lr=0.0001, step_loss=0.00391]\n",
      "Steps:  87%|████████▋ | 872/1000 [2:12:40<19:02,  8.92s/it, lr=0.0001, step_loss=0.00391]\n",
      "Steps:  87%|████████▋ | 872/1000 [2:12:40<19:02,  8.92s/it, lr=0.0001, step_loss=0.00427]\n",
      "Steps:  87%|████████▋ | 873/1000 [2:12:49<18:48,  8.89s/it, lr=0.0001, step_loss=0.00427]\n",
      "Steps:  87%|████████▋ | 873/1000 [2:12:49<18:48,  8.89s/it, lr=0.0001, step_loss=0.0071]\n",
      "Steps:  87%|████████▋ | 874/1000 [2:12:58<18:49,  8.96s/it, lr=0.0001, step_loss=0.0071]\n",
      "Steps:  87%|████████▋ | 874/1000 [2:12:58<18:49,  8.96s/it, lr=0.0001, step_loss=0.00952]\n",
      "Steps:  88%|████████▊ | 875/1000 [2:13:06<18:30,  8.89s/it, lr=0.0001, step_loss=0.00952]\n",
      "Steps:  88%|████████▊ | 875/1000 [2:13:06<18:30,  8.89s/it, lr=0.0001, step_loss=0.0184]\n",
      "Steps:  88%|████████▊ | 876/1000 [2:13:15<18:26,  8.92s/it, lr=0.0001, step_loss=0.0184]\n",
      "Steps:  88%|████████▊ | 876/1000 [2:13:15<18:26,  8.92s/it, lr=0.0001, step_loss=0.000918]\n",
      "Steps:  88%|████████▊ | 877/1000 [2:13:24<18:14,  8.90s/it, lr=0.0001, step_loss=0.000918]\n",
      "Steps:  88%|████████▊ | 877/1000 [2:13:24<18:14,  8.90s/it, lr=0.0001, step_loss=0.0102]\n",
      "Steps:  88%|████████▊ | 878/1000 [2:13:33<18:13,  8.96s/it, lr=0.0001, step_loss=0.0102]\n",
      "Steps:  88%|████████▊ | 878/1000 [2:13:33<18:13,  8.96s/it, lr=0.0001, step_loss=0.00795]\n",
      "Steps:  88%|████████▊ | 879/1000 [2:13:42<17:55,  8.88s/it, lr=0.0001, step_loss=0.00795]\n",
      "Steps:  88%|████████▊ | 879/1000 [2:13:42<17:55,  8.88s/it, lr=0.0001, step_loss=0.0139]\n",
      "Steps:  88%|████████▊ | 880/1000 [2:13:51<17:50,  8.92s/it, lr=0.0001, step_loss=0.0139]\n",
      "Steps:  88%|████████▊ | 880/1000 [2:13:51<17:50,  8.92s/it, lr=0.0001, step_loss=0.00752]\n",
      "Steps:  88%|████████▊ | 881/1000 [2:14:00<17:39,  8.90s/it, lr=0.0001, step_loss=0.00752]\n",
      "Steps:  88%|████████▊ | 881/1000 [2:14:00<17:39,  8.90s/it, lr=0.0001, step_loss=0.00298]\n",
      "Steps:  88%|████████▊ | 882/1000 [2:14:09<17:36,  8.96s/it, lr=0.0001, step_loss=0.00298]\n",
      "Steps:  88%|████████▊ | 882/1000 [2:14:09<17:36,  8.96s/it, lr=0.0001, step_loss=0.00352]\n",
      "Steps:  88%|████████▊ | 883/1000 [2:14:18<17:19,  8.89s/it, lr=0.0001, step_loss=0.00352]\n",
      "Steps:  88%|████████▊ | 883/1000 [2:14:18<17:19,  8.89s/it, lr=0.0001, step_loss=0.0286]\n",
      "Steps:  88%|████████▊ | 884/1000 [2:14:27<17:14,  8.92s/it, lr=0.0001, step_loss=0.0286]\n",
      "Steps:  88%|████████▊ | 884/1000 [2:14:27<17:14,  8.92s/it, lr=0.0001, step_loss=0.00859]\n",
      "Steps:  88%|████████▊ | 885/1000 [2:14:36<17:02,  8.89s/it, lr=0.0001, step_loss=0.00859]\n",
      "Steps:  88%|████████▊ | 885/1000 [2:14:36<17:02,  8.89s/it, lr=0.0001, step_loss=0.00976]\n",
      "Steps:  89%|████████▊ | 886/1000 [2:14:45<17:01,  8.96s/it, lr=0.0001, step_loss=0.00976]\n",
      "Steps:  89%|████████▊ | 886/1000 [2:14:45<17:01,  8.96s/it, lr=0.0001, step_loss=0.00329]\n",
      "Steps:  89%|████████▊ | 887/1000 [2:14:53<16:43,  8.88s/it, lr=0.0001, step_loss=0.00329]\n",
      "Steps:  89%|████████▊ | 887/1000 [2:14:53<16:43,  8.88s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  89%|████████▉ | 888/1000 [2:15:02<16:39,  8.92s/it, lr=0.0001, step_loss=0.0129]\n",
      "Steps:  89%|████████▉ | 888/1000 [2:15:02<16:39,  8.92s/it, lr=0.0001, step_loss=0.00266]\n",
      "Steps:  89%|████████▉ | 889/1000 [2:15:11<16:27,  8.89s/it, lr=0.0001, step_loss=0.00266]\n",
      "Steps:  89%|████████▉ | 889/1000 [2:15:11<16:27,  8.89s/it, lr=0.0001, step_loss=0.00111]\n",
      "Steps:  89%|████████▉ | 890/1000 [2:15:20<16:25,  8.96s/it, lr=0.0001, step_loss=0.00111]\n",
      "Steps:  89%|████████▉ | 890/1000 [2:15:20<16:25,  8.96s/it, lr=0.0001, step_loss=0.00793]\n",
      "Steps:  89%|████████▉ | 891/1000 [2:15:29<16:08,  8.89s/it, lr=0.0001, step_loss=0.00793]\n",
      "Steps:  89%|████████▉ | 891/1000 [2:15:29<16:08,  8.89s/it, lr=0.0001, step_loss=0.0165]\n",
      "Steps:  89%|████████▉ | 892/1000 [2:15:38<16:03,  8.92s/it, lr=0.0001, step_loss=0.0165]\n",
      "Steps:  89%|████████▉ | 892/1000 [2:15:38<16:03,  8.92s/it, lr=0.0001, step_loss=0.00714]\n",
      "Steps:  89%|████████▉ | 893/1000 [2:15:47<15:50,  8.89s/it, lr=0.0001, step_loss=0.00714]\n",
      "Steps:  89%|████████▉ | 893/1000 [2:15:47<15:50,  8.89s/it, lr=0.0001, step_loss=0.00404]\n",
      "Steps:  89%|████████▉ | 894/1000 [2:15:56<15:49,  8.96s/it, lr=0.0001, step_loss=0.00404]\n",
      "Steps:  89%|████████▉ | 894/1000 [2:15:56<15:49,  8.96s/it, lr=0.0001, step_loss=0.0117]\n",
      "Steps:  90%|████████▉ | 895/1000 [2:16:05<15:33,  8.89s/it, lr=0.0001, step_loss=0.0117]\n",
      "Steps:  90%|████████▉ | 895/1000 [2:16:05<15:33,  8.89s/it, lr=0.0001, step_loss=0.00463]\n",
      "Steps:  90%|████████▉ | 896/1000 [2:16:14<15:28,  8.93s/it, lr=0.0001, step_loss=0.00463]\n",
      "Steps:  90%|████████▉ | 896/1000 [2:16:14<15:28,  8.93s/it, lr=0.0001, step_loss=0.00608]\n",
      "Steps:  90%|████████▉ | 897/1000 [2:16:23<15:16,  8.90s/it, lr=0.0001, step_loss=0.00608]\n",
      "Steps:  90%|████████▉ | 897/1000 [2:16:23<15:16,  8.90s/it, lr=0.0001, step_loss=0.00417]\n",
      "Steps:  90%|████████▉ | 898/1000 [2:16:32<15:14,  8.96s/it, lr=0.0001, step_loss=0.00417]\n",
      "Steps:  90%|████████▉ | 898/1000 [2:16:32<15:14,  8.96s/it, lr=0.0001, step_loss=0.0036]\n",
      "Steps:  90%|████████▉ | 899/1000 [2:16:40<14:58,  8.89s/it, lr=0.0001, step_loss=0.0036]\n",
      "Steps:  90%|████████▉ | 899/1000 [2:16:40<14:58,  8.89s/it, lr=0.0001, step_loss=0.0405]\n",
      "Steps:  90%|█████████ | 900/1000 [2:16:49<14:52,  8.93s/it, lr=0.0001, step_loss=0.0405]04/17/2024 06:11:55 - INFO - accelerate.accelerator - Saving current state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-900\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-900\\unet_ema\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-900/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-900\\unet\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-900/unet/diffusion_pytorch_model.safetensors\n",
      "04/17/2024 06:12:32 - INFO - accelerate.checkpointing - Optimizer state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-900\\optimizer.bin\n",
      "04/17/2024 06:12:32 - INFO - accelerate.checkpointing - Scheduler state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-900\\scheduler.bin\n",
      "04/17/2024 06:12:32 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-900\\sampler.bin\n",
      "04/17/2024 06:12:32 - INFO - accelerate.checkpointing - Random states saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-900\\random_states_0.pkl\n",
      "04/17/2024 06:12:32 - INFO - __main__ - Saved state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-900\n",
      "\n",
      "Steps:  90%|█████████ | 900/1000 [2:17:26<14:52,  8.93s/it, lr=0.0001, step_loss=0.0125]\n",
      "Steps:  90%|█████████ | 901/1000 [2:17:35<33:05, 20.05s/it, lr=0.0001, step_loss=0.0125]\n",
      "Steps:  90%|█████████ | 901/1000 [2:17:35<33:05, 20.05s/it, lr=0.0001, step_loss=0.0197]\n",
      "Steps:  90%|█████████ | 902/1000 [2:17:45<27:23, 16.77s/it, lr=0.0001, step_loss=0.0197]\n",
      "Steps:  90%|█████████ | 902/1000 [2:17:45<27:23, 16.77s/it, lr=0.0001, step_loss=0.0119]\n",
      "Steps:  90%|█████████ | 903/1000 [2:17:53<23:12, 14.36s/it, lr=0.0001, step_loss=0.0119]\n",
      "Steps:  90%|█████████ | 903/1000 [2:17:53<23:12, 14.36s/it, lr=0.0001, step_loss=0.00418]\n",
      "Steps:  90%|█████████ | 904/1000 [2:18:02<20:24, 12.75s/it, lr=0.0001, step_loss=0.00418]\n",
      "Steps:  90%|█████████ | 904/1000 [2:18:02<20:24, 12.75s/it, lr=0.0001, step_loss=0.0214]\n",
      "Steps:  90%|█████████ | 905/1000 [2:18:11<18:19, 11.57s/it, lr=0.0001, step_loss=0.0214]\n",
      "Steps:  90%|█████████ | 905/1000 [2:18:11<18:19, 11.57s/it, lr=0.0001, step_loss=0.00194]\n",
      "Steps:  91%|█████████ | 906/1000 [2:18:20<16:59, 10.84s/it, lr=0.0001, step_loss=0.00194]\n",
      "Steps:  91%|█████████ | 906/1000 [2:18:20<16:59, 10.84s/it, lr=0.0001, step_loss=0.00837]\n",
      "Steps:  91%|█████████ | 907/1000 [2:18:29<15:48, 10.20s/it, lr=0.0001, step_loss=0.00837]\n",
      "Steps:  91%|█████████ | 907/1000 [2:18:29<15:48, 10.20s/it, lr=0.0001, step_loss=0.00902]\n",
      "Steps:  91%|█████████ | 908/1000 [2:18:38<15:05,  9.84s/it, lr=0.0001, step_loss=0.00902]\n",
      "Steps:  91%|█████████ | 908/1000 [2:18:38<15:05,  9.84s/it, lr=0.0001, step_loss=0.0165]\n",
      "Steps:  91%|█████████ | 909/1000 [2:18:47<14:26,  9.52s/it, lr=0.0001, step_loss=0.0165]\n",
      "Steps:  91%|█████████ | 909/1000 [2:18:47<14:26,  9.52s/it, lr=0.0001, step_loss=0.011]\n",
      "Steps:  91%|█████████ | 910/1000 [2:18:56<14:06,  9.41s/it, lr=0.0001, step_loss=0.011]\n",
      "Steps:  91%|█████████ | 910/1000 [2:18:56<14:06,  9.41s/it, lr=0.0001, step_loss=0.0034]\n",
      "Steps:  91%|█████████ | 911/1000 [2:19:05<13:39,  9.20s/it, lr=0.0001, step_loss=0.0034]\n",
      "Steps:  91%|█████████ | 911/1000 [2:19:05<13:39,  9.20s/it, lr=0.0001, step_loss=0.00284]\n",
      "Steps:  91%|█████████ | 912/1000 [2:19:14<13:24,  9.14s/it, lr=0.0001, step_loss=0.00284]\n",
      "Steps:  91%|█████████ | 912/1000 [2:19:14<13:24,  9.14s/it, lr=0.0001, step_loss=0.00672]\n",
      "Steps:  91%|█████████▏| 913/1000 [2:19:22<13:06,  9.04s/it, lr=0.0001, step_loss=0.00672]\n",
      "Steps:  91%|█████████▏| 913/1000 [2:19:22<13:06,  9.04s/it, lr=0.0001, step_loss=0.00219]\n",
      "Steps:  91%|█████████▏| 914/1000 [2:19:31<12:59,  9.06s/it, lr=0.0001, step_loss=0.00219]\n",
      "Steps:  91%|█████████▏| 914/1000 [2:19:31<12:59,  9.06s/it, lr=0.0001, step_loss=0.0147]\n",
      "Steps:  92%|█████████▏| 915/1000 [2:19:40<12:41,  8.96s/it, lr=0.0001, step_loss=0.0147]\n",
      "Steps:  92%|█████████▏| 915/1000 [2:19:40<12:41,  8.96s/it, lr=0.0001, step_loss=0.0196]\n",
      "Steps:  92%|█████████▏| 916/1000 [2:19:49<12:33,  8.97s/it, lr=0.0001, step_loss=0.0196]\n",
      "Steps:  92%|█████████▏| 916/1000 [2:19:49<12:33,  8.97s/it, lr=0.0001, step_loss=0.0066]\n",
      "Steps:  92%|█████████▏| 917/1000 [2:19:58<12:21,  8.93s/it, lr=0.0001, step_loss=0.0066]\n",
      "Steps:  92%|█████████▏| 917/1000 [2:19:58<12:21,  8.93s/it, lr=0.0001, step_loss=0.00555]\n",
      "Steps:  92%|█████████▏| 918/1000 [2:20:07<12:16,  8.98s/it, lr=0.0001, step_loss=0.00555]\n",
      "Steps:  92%|█████████▏| 918/1000 [2:20:07<12:16,  8.98s/it, lr=0.0001, step_loss=0.00277]\n",
      "Steps:  92%|█████████▏| 919/1000 [2:20:16<12:01,  8.91s/it, lr=0.0001, step_loss=0.00277]\n",
      "Steps:  92%|█████████▏| 919/1000 [2:20:16<12:01,  8.91s/it, lr=0.0001, step_loss=0.00208]\n",
      "Steps:  92%|█████████▏| 920/1000 [2:20:25<11:54,  8.94s/it, lr=0.0001, step_loss=0.00208]\n",
      "Steps:  92%|█████████▏| 920/1000 [2:20:25<11:54,  8.94s/it, lr=0.0001, step_loss=0.00364]\n",
      "Steps:  92%|█████████▏| 921/1000 [2:20:34<11:42,  8.89s/it, lr=0.0001, step_loss=0.00364]\n",
      "Steps:  92%|█████████▏| 921/1000 [2:20:34<11:42,  8.89s/it, lr=0.0001, step_loss=0.0108]\n",
      "Steps:  92%|█████████▏| 922/1000 [2:20:43<11:39,  8.97s/it, lr=0.0001, step_loss=0.0108]\n",
      "Steps:  92%|█████████▏| 922/1000 [2:20:43<11:39,  8.97s/it, lr=0.0001, step_loss=0.0103]\n",
      "Steps:  92%|█████████▏| 923/1000 [2:20:52<11:24,  8.89s/it, lr=0.0001, step_loss=0.0103]\n",
      "Steps:  92%|█████████▏| 923/1000 [2:20:52<11:24,  8.89s/it, lr=0.0001, step_loss=0.00293]\n",
      "Steps:  92%|█████████▏| 924/1000 [2:21:01<11:18,  8.93s/it, lr=0.0001, step_loss=0.00293]\n",
      "Steps:  92%|█████████▏| 924/1000 [2:21:01<11:18,  8.93s/it, lr=0.0001, step_loss=0.00232]\n",
      "Steps:  92%|█████████▎| 925/1000 [2:21:09<11:06,  8.89s/it, lr=0.0001, step_loss=0.00232]\n",
      "Steps:  92%|█████████▎| 925/1000 [2:21:09<11:06,  8.89s/it, lr=0.0001, step_loss=0.00881]\n",
      "Steps:  93%|█████████▎| 926/1000 [2:21:18<11:03,  8.96s/it, lr=0.0001, step_loss=0.00881]\n",
      "Steps:  93%|█████████▎| 926/1000 [2:21:18<11:03,  8.96s/it, lr=0.0001, step_loss=0.00925]\n",
      "Steps:  93%|█████████▎| 927/1000 [2:21:27<10:48,  8.89s/it, lr=0.0001, step_loss=0.00925]\n",
      "Steps:  93%|█████████▎| 927/1000 [2:21:27<10:48,  8.89s/it, lr=0.0001, step_loss=0.00521]\n",
      "Steps:  93%|█████████▎| 928/1000 [2:21:36<10:42,  8.92s/it, lr=0.0001, step_loss=0.00521]\n",
      "Steps:  93%|█████████▎| 928/1000 [2:21:36<10:42,  8.92s/it, lr=0.0001, step_loss=0.00204]\n",
      "Steps:  93%|█████████▎| 929/1000 [2:21:45<10:31,  8.89s/it, lr=0.0001, step_loss=0.00204]\n",
      "Steps:  93%|█████████▎| 929/1000 [2:21:45<10:31,  8.89s/it, lr=0.0001, step_loss=0.00548]\n",
      "Steps:  93%|█████████▎| 930/1000 [2:21:54<10:26,  8.95s/it, lr=0.0001, step_loss=0.00548]\n",
      "Steps:  93%|█████████▎| 930/1000 [2:21:54<10:26,  8.95s/it, lr=0.0001, step_loss=0.0249]\n",
      "Steps:  93%|█████████▎| 931/1000 [2:22:03<10:13,  8.88s/it, lr=0.0001, step_loss=0.0249]\n",
      "Steps:  93%|█████████▎| 931/1000 [2:22:03<10:13,  8.88s/it, lr=0.0001, step_loss=0.0192]\n",
      "Steps:  93%|█████████▎| 932/1000 [2:22:12<10:06,  8.92s/it, lr=0.0001, step_loss=0.0192]\n",
      "Steps:  93%|█████████▎| 932/1000 [2:22:12<10:06,  8.92s/it, lr=0.0001, step_loss=0.00101]\n",
      "Steps:  93%|█████████▎| 933/1000 [2:22:21<09:55,  8.89s/it, lr=0.0001, step_loss=0.00101]\n",
      "Steps:  93%|█████████▎| 933/1000 [2:22:21<09:55,  8.89s/it, lr=0.0001, step_loss=0.0106]\n",
      "Steps:  93%|█████████▎| 934/1000 [2:22:30<09:51,  8.96s/it, lr=0.0001, step_loss=0.0106]\n",
      "Steps:  93%|█████████▎| 934/1000 [2:22:30<09:51,  8.96s/it, lr=0.0001, step_loss=0.00836]\n",
      "Steps:  94%|█████████▎| 935/1000 [2:22:38<09:37,  8.89s/it, lr=0.0001, step_loss=0.00836]\n",
      "Steps:  94%|█████████▎| 935/1000 [2:22:38<09:37,  8.89s/it, lr=0.0001, step_loss=0.00621]\n",
      "Steps:  94%|█████████▎| 936/1000 [2:22:47<09:30,  8.92s/it, lr=0.0001, step_loss=0.00621]\n",
      "Steps:  94%|█████████▎| 936/1000 [2:22:47<09:30,  8.92s/it, lr=0.0001, step_loss=0.00947]\n",
      "Steps:  94%|█████████▎| 937/1000 [2:22:56<09:20,  8.90s/it, lr=0.0001, step_loss=0.00947]\n",
      "Steps:  94%|█████████▎| 937/1000 [2:22:56<09:20,  8.90s/it, lr=0.0001, step_loss=0.00363]\n",
      "Steps:  94%|█████████▍| 938/1000 [2:23:05<09:15,  8.96s/it, lr=0.0001, step_loss=0.00363]\n",
      "Steps:  94%|█████████▍| 938/1000 [2:23:05<09:15,  8.96s/it, lr=0.0001, step_loss=0.0109]\n",
      "Steps:  94%|█████████▍| 939/1000 [2:23:14<09:02,  8.89s/it, lr=0.0001, step_loss=0.0109]\n",
      "Steps:  94%|█████████▍| 939/1000 [2:23:14<09:02,  8.89s/it, lr=0.0001, step_loss=0.00403]\n",
      "Steps:  94%|█████████▍| 940/1000 [2:23:23<08:55,  8.92s/it, lr=0.0001, step_loss=0.00403]\n",
      "Steps:  94%|█████████▍| 940/1000 [2:23:23<08:55,  8.92s/it, lr=0.0001, step_loss=0.00587]\n",
      "Steps:  94%|█████████▍| 941/1000 [2:23:32<08:44,  8.88s/it, lr=0.0001, step_loss=0.00587]\n",
      "Steps:  94%|█████████▍| 941/1000 [2:23:32<08:44,  8.88s/it, lr=0.0001, step_loss=0.00927]\n",
      "Steps:  94%|█████████▍| 942/1000 [2:23:41<08:39,  8.96s/it, lr=0.0001, step_loss=0.00927]\n",
      "Steps:  94%|█████████▍| 942/1000 [2:23:41<08:39,  8.96s/it, lr=0.0001, step_loss=0.023]\n",
      "Steps:  94%|█████████▍| 943/1000 [2:23:50<08:26,  8.88s/it, lr=0.0001, step_loss=0.023]\n",
      "Steps:  94%|█████████▍| 943/1000 [2:23:50<08:26,  8.88s/it, lr=0.0001, step_loss=0.00408]\n",
      "Steps:  94%|█████████▍| 944/1000 [2:23:59<08:19,  8.93s/it, lr=0.0001, step_loss=0.00408]\n",
      "Steps:  94%|█████████▍| 944/1000 [2:23:59<08:19,  8.93s/it, lr=0.0001, step_loss=0.0036]\n",
      "Steps:  94%|█████████▍| 945/1000 [2:24:08<08:09,  8.89s/it, lr=0.0001, step_loss=0.0036]\n",
      "Steps:  94%|█████████▍| 945/1000 [2:24:08<08:09,  8.89s/it, lr=0.0001, step_loss=0.00551]\n",
      "Steps:  95%|█████████▍| 946/1000 [2:24:17<08:03,  8.96s/it, lr=0.0001, step_loss=0.00551]\n",
      "Steps:  95%|█████████▍| 946/1000 [2:24:17<08:03,  8.96s/it, lr=0.0001, step_loss=0.00693]\n",
      "Steps:  95%|█████████▍| 947/1000 [2:24:25<07:51,  8.89s/it, lr=0.0001, step_loss=0.00693]\n",
      "Steps:  95%|█████████▍| 947/1000 [2:24:25<07:51,  8.89s/it, lr=0.0001, step_loss=0.0369]\n",
      "Steps:  95%|█████████▍| 948/1000 [2:24:34<07:44,  8.92s/it, lr=0.0001, step_loss=0.0369]\n",
      "Steps:  95%|█████████▍| 948/1000 [2:24:34<07:44,  8.92s/it, lr=0.0001, step_loss=0.00425]\n",
      "Steps:  95%|█████████▍| 949/1000 [2:24:43<07:33,  8.89s/it, lr=0.0001, step_loss=0.00425]\n",
      "Steps:  95%|█████████▍| 949/1000 [2:24:43<07:33,  8.89s/it, lr=0.0001, step_loss=0.0267]\n",
      "Steps:  95%|█████████▌| 950/1000 [2:24:52<07:27,  8.96s/it, lr=0.0001, step_loss=0.0267]\n",
      "Steps:  95%|█████████▌| 950/1000 [2:24:52<07:27,  8.96s/it, lr=0.0001, step_loss=0.012]\n",
      "Steps:  95%|█████████▌| 951/1000 [2:25:01<07:15,  8.89s/it, lr=0.0001, step_loss=0.012]\n",
      "Steps:  95%|█████████▌| 951/1000 [2:25:01<07:15,  8.89s/it, lr=0.0001, step_loss=0.000863]\n",
      "Steps:  95%|█████████▌| 952/1000 [2:25:10<07:08,  8.92s/it, lr=0.0001, step_loss=0.000863]\n",
      "Steps:  95%|█████████▌| 952/1000 [2:25:10<07:08,  8.92s/it, lr=0.0001, step_loss=0.00321]\n",
      "Steps:  95%|█████████▌| 953/1000 [2:25:19<06:57,  8.89s/it, lr=0.0001, step_loss=0.00321]\n",
      "Steps:  95%|█████████▌| 953/1000 [2:25:19<06:57,  8.89s/it, lr=0.0001, step_loss=0.0028]\n",
      "Steps:  95%|█████████▌| 954/1000 [2:25:28<06:52,  8.96s/it, lr=0.0001, step_loss=0.0028]\n",
      "Steps:  95%|█████████▌| 954/1000 [2:25:28<06:52,  8.96s/it, lr=0.0001, step_loss=0.0112]\n",
      "Steps:  96%|█████████▌| 955/1000 [2:25:37<06:39,  8.88s/it, lr=0.0001, step_loss=0.0112]\n",
      "Steps:  96%|█████████▌| 955/1000 [2:25:37<06:39,  8.88s/it, lr=0.0001, step_loss=0.0213]\n",
      "Steps:  96%|█████████▌| 956/1000 [2:25:46<06:32,  8.92s/it, lr=0.0001, step_loss=0.0213]\n",
      "Steps:  96%|█████████▌| 956/1000 [2:25:46<06:32,  8.92s/it, lr=0.0001, step_loss=0.00193]\n",
      "Steps:  96%|█████████▌| 957/1000 [2:25:55<06:22,  8.89s/it, lr=0.0001, step_loss=0.00193]\n",
      "Steps:  96%|█████████▌| 957/1000 [2:25:55<06:22,  8.89s/it, lr=0.0001, step_loss=0.00458]\n",
      "Steps:  96%|█████████▌| 958/1000 [2:26:04<06:15,  8.95s/it, lr=0.0001, step_loss=0.00458]\n",
      "Steps:  96%|█████████▌| 958/1000 [2:26:04<06:15,  8.95s/it, lr=0.0001, step_loss=0.00387]\n",
      "Steps:  96%|█████████▌| 959/1000 [2:26:12<06:04,  8.89s/it, lr=0.0001, step_loss=0.00387]\n",
      "Steps:  96%|█████████▌| 959/1000 [2:26:12<06:04,  8.89s/it, lr=0.0001, step_loss=0.028]\n",
      "Steps:  96%|█████████▌| 960/1000 [2:26:21<05:56,  8.92s/it, lr=0.0001, step_loss=0.028]\n",
      "Steps:  96%|█████████▌| 960/1000 [2:26:21<05:56,  8.92s/it, lr=0.0001, step_loss=0.014]\n",
      "Steps:  96%|█████████▌| 961/1000 [2:26:30<05:46,  8.89s/it, lr=0.0001, step_loss=0.014]\n",
      "Steps:  96%|█████████▌| 961/1000 [2:26:30<05:46,  8.89s/it, lr=0.0001, step_loss=0.00294]\n",
      "Steps:  96%|█████████▌| 962/1000 [2:26:39<05:40,  8.96s/it, lr=0.0001, step_loss=0.00294]\n",
      "Steps:  96%|█████████▌| 962/1000 [2:26:39<05:40,  8.96s/it, lr=0.0001, step_loss=0.0115]\n",
      "Steps:  96%|█████████▋| 963/1000 [2:26:48<05:28,  8.89s/it, lr=0.0001, step_loss=0.0115]\n",
      "Steps:  96%|█████████▋| 963/1000 [2:26:48<05:28,  8.89s/it, lr=0.0001, step_loss=0.0143]\n",
      "Steps:  96%|█████████▋| 964/1000 [2:26:57<05:21,  8.92s/it, lr=0.0001, step_loss=0.0143]\n",
      "Steps:  96%|█████████▋| 964/1000 [2:26:57<05:21,  8.92s/it, lr=0.0001, step_loss=0.00188]\n",
      "Steps:  96%|█████████▋| 965/1000 [2:27:06<05:10,  8.88s/it, lr=0.0001, step_loss=0.00188]\n",
      "Steps:  96%|█████████▋| 965/1000 [2:27:06<05:10,  8.88s/it, lr=0.0001, step_loss=0.00738]\n",
      "Steps:  97%|█████████▋| 966/1000 [2:27:15<05:04,  8.96s/it, lr=0.0001, step_loss=0.00738]\n",
      "Steps:  97%|█████████▋| 966/1000 [2:27:15<05:04,  8.96s/it, lr=0.0001, step_loss=0.015]\n",
      "Steps:  97%|█████████▋| 967/1000 [2:27:24<04:53,  8.90s/it, lr=0.0001, step_loss=0.015]\n",
      "Steps:  97%|█████████▋| 967/1000 [2:27:24<04:53,  8.90s/it, lr=0.0001, step_loss=0.00386]\n",
      "Steps:  97%|█████████▋| 968/1000 [2:27:33<04:45,  8.93s/it, lr=0.0001, step_loss=0.00386]\n",
      "Steps:  97%|█████████▋| 968/1000 [2:27:33<04:45,  8.93s/it, lr=0.0001, step_loss=0.00107]\n",
      "Steps:  97%|█████████▋| 969/1000 [2:27:42<04:36,  8.91s/it, lr=0.0001, step_loss=0.00107]\n",
      "Steps:  97%|█████████▋| 969/1000 [2:27:42<04:36,  8.91s/it, lr=0.0001, step_loss=0.00197]\n",
      "Steps:  97%|█████████▋| 970/1000 [2:27:51<04:28,  8.96s/it, lr=0.0001, step_loss=0.00197]\n",
      "Steps:  97%|█████████▋| 970/1000 [2:27:51<04:28,  8.96s/it, lr=0.0001, step_loss=0.00471]\n",
      "Steps:  97%|█████████▋| 971/1000 [2:27:59<04:17,  8.89s/it, lr=0.0001, step_loss=0.00471]\n",
      "Steps:  97%|█████████▋| 971/1000 [2:27:59<04:17,  8.89s/it, lr=0.0001, step_loss=0.00296]\n",
      "Steps:  97%|█████████▋| 972/1000 [2:28:08<04:09,  8.92s/it, lr=0.0001, step_loss=0.00296]\n",
      "Steps:  97%|█████████▋| 972/1000 [2:28:08<04:09,  8.92s/it, lr=0.0001, step_loss=0.00719]\n",
      "Steps:  97%|█████████▋| 973/1000 [2:28:17<04:00,  8.90s/it, lr=0.0001, step_loss=0.00719]\n",
      "Steps:  97%|█████████▋| 973/1000 [2:28:17<04:00,  8.90s/it, lr=0.0001, step_loss=0.00443]\n",
      "Steps:  97%|█████████▋| 974/1000 [2:28:26<03:53,  8.96s/it, lr=0.0001, step_loss=0.00443]\n",
      "Steps:  97%|█████████▋| 974/1000 [2:28:26<03:53,  8.96s/it, lr=0.0001, step_loss=0.00163]\n",
      "Steps:  98%|█████████▊| 975/1000 [2:28:35<03:42,  8.88s/it, lr=0.0001, step_loss=0.00163]\n",
      "Steps:  98%|█████████▊| 975/1000 [2:28:35<03:42,  8.88s/it, lr=0.0001, step_loss=0.0033]\n",
      "Steps:  98%|█████████▊| 976/1000 [2:28:44<03:34,  8.92s/it, lr=0.0001, step_loss=0.0033]\n",
      "Steps:  98%|█████████▊| 976/1000 [2:28:44<03:34,  8.92s/it, lr=0.0001, step_loss=0.00557]\n",
      "Steps:  98%|█████████▊| 977/1000 [2:28:53<03:24,  8.89s/it, lr=0.0001, step_loss=0.00557]\n",
      "Steps:  98%|█████████▊| 977/1000 [2:28:53<03:24,  8.89s/it, lr=0.0001, step_loss=0.00162]\n",
      "Steps:  98%|█████████▊| 978/1000 [2:29:02<03:16,  8.95s/it, lr=0.0001, step_loss=0.00162]\n",
      "Steps:  98%|█████████▊| 978/1000 [2:29:02<03:16,  8.95s/it, lr=0.0001, step_loss=0.00442]\n",
      "Steps:  98%|█████████▊| 979/1000 [2:29:11<03:06,  8.89s/it, lr=0.0001, step_loss=0.00442]\n",
      "Steps:  98%|█████████▊| 979/1000 [2:29:11<03:06,  8.89s/it, lr=0.0001, step_loss=0.0144]\n",
      "Steps:  98%|█████████▊| 980/1000 [2:29:20<02:58,  8.93s/it, lr=0.0001, step_loss=0.0144]\n",
      "Steps:  98%|█████████▊| 980/1000 [2:29:20<02:58,  8.93s/it, lr=0.0001, step_loss=0.00797]\n",
      "Steps:  98%|█████████▊| 981/1000 [2:29:29<02:48,  8.89s/it, lr=0.0001, step_loss=0.00797]\n",
      "Steps:  98%|█████████▊| 981/1000 [2:29:29<02:48,  8.89s/it, lr=0.0001, step_loss=0.00246]\n",
      "Steps:  98%|█████████▊| 982/1000 [2:29:38<02:41,  8.96s/it, lr=0.0001, step_loss=0.00246]\n",
      "Steps:  98%|█████████▊| 982/1000 [2:29:38<02:41,  8.96s/it, lr=0.0001, step_loss=0.00499]\n",
      "Steps:  98%|█████████▊| 983/1000 [2:29:46<02:31,  8.89s/it, lr=0.0001, step_loss=0.00499]\n",
      "Steps:  98%|█████████▊| 983/1000 [2:29:46<02:31,  8.89s/it, lr=0.0001, step_loss=0.0103]\n",
      "Steps:  98%|█████████▊| 984/1000 [2:29:55<02:22,  8.93s/it, lr=0.0001, step_loss=0.0103]\n",
      "Steps:  98%|█████████▊| 984/1000 [2:29:55<02:22,  8.93s/it, lr=0.0001, step_loss=0.00875]\n",
      "Steps:  98%|█████████▊| 985/1000 [2:30:04<02:13,  8.90s/it, lr=0.0001, step_loss=0.00875]\n",
      "Steps:  98%|█████████▊| 985/1000 [2:30:04<02:13,  8.90s/it, lr=0.0001, step_loss=0.015]\n",
      "Steps:  99%|█████████▊| 986/1000 [2:30:13<02:05,  8.96s/it, lr=0.0001, step_loss=0.015]\n",
      "Steps:  99%|█████████▊| 986/1000 [2:30:13<02:05,  8.96s/it, lr=0.0001, step_loss=0.00565]\n",
      "Steps:  99%|█████████▊| 987/1000 [2:30:22<01:55,  8.89s/it, lr=0.0001, step_loss=0.00565]\n",
      "Steps:  99%|█████████▊| 987/1000 [2:30:22<01:55,  8.89s/it, lr=0.0001, step_loss=0.157]\n",
      "Steps:  99%|█████████▉| 988/1000 [2:30:31<01:47,  8.92s/it, lr=0.0001, step_loss=0.157]\n",
      "Steps:  99%|█████████▉| 988/1000 [2:30:31<01:47,  8.92s/it, lr=0.0001, step_loss=0.00342]\n",
      "Steps:  99%|█████████▉| 989/1000 [2:30:40<01:37,  8.89s/it, lr=0.0001, step_loss=0.00342]\n",
      "Steps:  99%|█████████▉| 989/1000 [2:30:40<01:37,  8.89s/it, lr=0.0001, step_loss=0.0197]\n",
      "Steps:  99%|█████████▉| 990/1000 [2:30:49<01:29,  8.96s/it, lr=0.0001, step_loss=0.0197]\n",
      "Steps:  99%|█████████▉| 990/1000 [2:30:49<01:29,  8.96s/it, lr=0.0001, step_loss=0.0125]\n",
      "Steps:  99%|█████████▉| 991/1000 [2:30:58<01:19,  8.88s/it, lr=0.0001, step_loss=0.0125]\n",
      "Steps:  99%|█████████▉| 991/1000 [2:30:58<01:19,  8.88s/it, lr=0.0001, step_loss=0.000662]\n",
      "Steps:  99%|█████████▉| 992/1000 [2:31:07<01:11,  8.92s/it, lr=0.0001, step_loss=0.000662]\n",
      "Steps:  99%|█████████▉| 992/1000 [2:31:07<01:11,  8.92s/it, lr=0.0001, step_loss=0.0141]\n",
      "Steps:  99%|█████████▉| 993/1000 [2:31:16<01:02,  8.89s/it, lr=0.0001, step_loss=0.0141]\n",
      "Steps:  99%|█████████▉| 993/1000 [2:31:16<01:02,  8.89s/it, lr=0.0001, step_loss=0.00228]\n",
      "Steps:  99%|█████████▉| 994/1000 [2:31:25<00:53,  8.96s/it, lr=0.0001, step_loss=0.00228]\n",
      "Steps:  99%|█████████▉| 994/1000 [2:31:25<00:53,  8.96s/it, lr=0.0001, step_loss=0.00259]\n",
      "Steps: 100%|█████████▉| 995/1000 [2:31:33<00:44,  8.88s/it, lr=0.0001, step_loss=0.00259]\n",
      "Steps: 100%|█████████▉| 995/1000 [2:31:33<00:44,  8.88s/it, lr=0.0001, step_loss=0.00126]\n",
      "Steps: 100%|█████████▉| 996/1000 [2:31:42<00:35,  8.92s/it, lr=0.0001, step_loss=0.00126]\n",
      "Steps: 100%|█████████▉| 996/1000 [2:31:42<00:35,  8.92s/it, lr=0.0001, step_loss=0.00388]\n",
      "Steps: 100%|█████████▉| 997/1000 [2:31:51<00:26,  8.88s/it, lr=0.0001, step_loss=0.00388]\n",
      "Steps: 100%|█████████▉| 997/1000 [2:31:51<00:26,  8.88s/it, lr=0.0001, step_loss=0.00697]\n",
      "Steps: 100%|█████████▉| 998/1000 [2:32:00<00:17,  8.96s/it, lr=0.0001, step_loss=0.00697]\n",
      "Steps: 100%|█████████▉| 998/1000 [2:32:00<00:17,  8.96s/it, lr=0.0001, step_loss=0.0186]\n",
      "Steps: 100%|█████████▉| 999/1000 [2:32:09<00:08,  8.88s/it, lr=0.0001, step_loss=0.0186]\n",
      "Steps: 100%|█████████▉| 999/1000 [2:32:09<00:08,  8.88s/it, lr=0.0001, step_loss=0.0172]\n",
      "Steps: 100%|██████████| 1000/1000 [2:32:18<00:00,  8.92s/it, lr=0.0001, step_loss=0.0172]04/17/2024 06:27:24 - INFO - accelerate.accelerator - Saving current state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-1000\n",
      "{'attention_type', 'dual_cross_attention', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'conv_out_kernel', 'class_embed_type', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'num_class_embeds', 'time_embedding_act_fn', 'time_embedding_dim', 'only_cross_attention', 'mid_block_type', 'conv_in_kernel', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'use_linear_projection', 'timestep_post_act', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-1000\\unet_ema\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-1000/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-1000\\unet\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/checkpoint-1000/unet/diffusion_pytorch_model.safetensors\n",
      "04/17/2024 06:28:01 - INFO - accelerate.checkpointing - Optimizer state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-1000\\optimizer.bin\n",
      "04/17/2024 06:28:01 - INFO - accelerate.checkpointing - Scheduler state saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-1000\\scheduler.bin\n",
      "04/17/2024 06:28:01 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-1000\\sampler.bin\n",
      "04/17/2024 06:28:01 - INFO - accelerate.checkpointing - Random states saved in X:\\Jupyter\\csc413_project\\Full_Parameter_Finetune\\checkpoint-1000\\random_states_0.pkl\n",
      "04/17/2024 06:28:01 - INFO - __main__ - Saved state to X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\checkpoint-1000\n",
      "\n",
      "Steps: 100%|██████████| 1000/1000 [2:32:56<00:00,  8.92s/it, lr=0.0001, step_loss=0.00678]{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 16.00it/s]\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\vae\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/vae/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\unet\\config.json\n",
      "Model weights saved in X:/Jupyter/csc413_project/Full_Parameter_Finetune/unet/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\scheduler\\scheduler_config.json\n",
      "Configuration saved in X:\\Jupyter\\csc413_project/Full_Parameter_Finetune\\model_index.json\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: - 0.002 MB of 0.042 MB uploaded (0.000 MB deduped)\n",
      "wandb: \\ 0.002 MB of 0.042 MB uploaded (0.000 MB deduped)\n",
      "wandb:\n",
      "wandb: Run summary:\n",
      "wandb: train_loss 0.00678\n",
      "wandb:\n",
      "wandb:  View run ruby-fog-40 at: https://wandb.ai/tonyxichen/text2image-fine-tune/runs/q9m70n6e\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: .\\wandb\\run-20240417_035505-q9m70n6e\\logs\n",
      "C:\\Users\\Tony\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2085: UserWarning: Run (q9m70n6e) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "\n",
      "Steps: 100%|██████████| 1000/1000 [2:33:12<00:00,  9.19s/it, lr=0.0001, step_loss=0.00678]\n"
     ]
    }
   ],
   "source": [
    "train_full_parameter_finetune()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
